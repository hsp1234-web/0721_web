# -*- coding: utf-8 -*-
"""
================================================================================
æ•´åˆå¾Œçš„æ¥­å‹™é‚è¼¯
================================================================================

æœ¬æª”æ¡ˆæ•´åˆäº†ã€æ™®ç¾…ç±³ä¿®æ–¯ä¹‹ç«ã€‘é‡‘èæ•¸æ“šæ¡†æ¶èˆ‡ã€é³³å‡°å°ˆæ¡ˆã€‘éŒ„éŸ³è½‰å¯«æœå‹™çš„æ ¸å¿ƒæ¥­å‹™é‚è¼¯ã€‚

--------------------------------------------------------------------------------
ç¬¬ä¸€éƒ¨åˆ†ï¼šã€æ™®ç¾…ç±³ä¿®æ–¯ä¹‹ç«ã€‘æ ¸å¿ƒé‚è¼¯
--------------------------------------------------------------------------------
é€™éƒ¨åˆ†åŒ…å«äº†é‡‘èæ•¸æ“šçš„ç²å–ã€è™•ç†ã€å› å­è¨ˆç®—èˆ‡æ¨¡æ“¬ã€‚
"""

# ... (å¾ 0709_wolf_88/src/prometheus/ ä¸‹çš„ç›¸é—œæª”æ¡ˆè¤‡è£½æ ¸å¿ƒé‚è¼¯) ...

"""
--------------------------------------------------------------------------------
ç¬¬äºŒéƒ¨åˆ†ï¼šã€é³³å‡°å°ˆæ¡ˆã€‘æ ¸å¿ƒé‚è¼¯
--------------------------------------------------------------------------------
é€™éƒ¨åˆ†åŒ…å«äº†éŒ„éŸ³è½‰å¯«æœå‹™çš„ APIã€å·¥äººé€²ç¨‹å’Œä»»å‹™ç®¡ç†ã€‚
"""

# ... (å¾ 0709_wolf_88/src/prometheus/transcriber/ ä¸‹çš„ç›¸é—œæª”æ¡ˆè¤‡è£½æ ¸å¿ƒé‚è¼¯) ...
import typer
from prometheus.entrypoints.ai_analyst_app import ai_analyst_job
from prometheus.entrypoints.query_gateway import run_dashboard_service
from prometheus.core.logging.log_manager import LogManager

app = typer.Typer()
# ç”±æ–¼ LogManager ä¸å†æ˜¯å–®ä¾‹ï¼Œæˆ‘å€‘ç‚º CLI çš„ä¸»é€²ç¨‹å‰µå»ºä¸€å€‹å¸¸è¦çš„ logger
log_manager = LogManager(log_file="prometheus_cli.log")
logger = log_manager.get_logger("Conductor")

@app.command(name="analyze")
def cli_analyze():
    """
    å•Ÿå‹• AI åˆ†æå¸«å ±å‘Šç”Ÿæˆå™¨ã€‚
    """
    logger.info("æ­£åœ¨å•Ÿå‹• AI åˆ†æå¸«...")
    ai_analyst_job()
    logger.info("AI åˆ†æå¸«å·¥ä½œå®Œæˆã€‚")


import subprocess
import sys
import os

@app.command(name="dashboard")
def cli_dashboard(
    host: str = typer.Option("127.0.0.1", help="ç¶å®šä¸»æ©Ÿ"),
    port: int = typer.Option(8000, help="ç¶å®šåŸ è™Ÿ"),
):
    """å•Ÿå‹•ç¶²é å„€è¡¨æ¿ã€‚"""
    logger.info(f"æº–å‚™åœ¨ http://{host}:{port} å•Ÿå‹•å„€è¡¨æ¿...")
    run_dashboard_service(None, host, port)

data_app = typer.Typer()
app.add_typer(data_app, name="data")

@data_app.command("create-dummy")
def create_dummy():
    """
    å»ºç«‹ä¸€å€‹ç”¨æ–¼æ¸¬è©¦çš„è™›æ§‹ OHLCV CSV æª”æ¡ˆã€‚
    """
    from pathlib import Path
    import numpy as np
    import pandas as pd

    DATA_DIR = Path("data")
    DATA_DIR.mkdir(exist_ok=True)
    file_path = DATA_DIR / "ohlcv_data.csv"

    date_range = pd.to_datetime(
        pd.date_range(start="2022-01-01", periods=1000, freq="D")
    )

    open_prices = np.random.uniform(90, 110, size=1000)

    data = {
        "Date": date_range,
        "Open": open_prices,
        "High": open_prices + np.random.uniform(0, 5, size=1000),
        "Low": open_prices - np.random.uniform(0, 5, size=1000),
        "Close": open_prices + np.random.uniform(-2, 2, size=1000),
        "Volume": np.random.randint(100000, 500000, size=1000),
    }

    df = pd.DataFrame(data)

    df.to_csv(file_path, index=False)
    logger.info(f"å·²æˆåŠŸå»ºç«‹è™›æ§‹æ•¸æ“šæª”æ¡ˆæ–¼: {file_path}")


results_app = typer.Typer()
app.add_typer(results_app, name="results")

@results_app.command("clear")
def clear_results():
    """
    æ¸…é™¤æ‰€æœ‰ç”Ÿæˆçš„çµæœã€ä½‡åˆ—ã€æ—¥èªŒå’Œæª¢æŸ¥é»ã€‚
    """
    import os
    import shutil

    logger.info("é–‹å§‹æ¸…é™¤æ‰€æœ‰åŸ·è¡Œæ•¸æ“š...")

    RESULTS_DB_PATH = "output/results.sqlite"
    QUEUE_DIR = "data/queues"
    LOG_DIR = "data/logs"
    CHECKPOINT_DIR = "data/checkpoints"
    REPORTS_DIR = "data/reports"

    def remove_path(path_str, is_dir=False):
        if is_dir:
            if os.path.isdir(path_str):
                shutil.rmtree(path_str)
                logger.info(f"å·²åˆªé™¤ä¸¦æ¸…ç©ºç›®éŒ„: {path_str}")
        else:
            if os.path.exists(path_str):
                os.remove(path_str)
                logger.info(f"å·²åˆªé™¤æª”æ¡ˆ: {path_str}")

    try:
        remove_path(RESULTS_DB_PATH, is_dir=False)
        remove_path(QUEUE_DIR, is_dir=True)
        remove_path(LOG_DIR, is_dir=True)
        remove_path(CHECKPOINT_DIR, is_dir=True)
        remove_path(REPORTS_DIR, is_dir=True)

        # é‡å»ºç©ºç›®éŒ„
        os.makedirs(QUEUE_DIR, exist_ok=True)
        os.makedirs(LOG_DIR, exist_ok=True)
        os.makedirs(CHECKPOINT_DIR, exist_ok=True)
        os.makedirs(REPORTS_DIR, exist_ok=True)

        logger.info("æ¸…é™¤ç¨‹åºå®Œæˆã€‚")
    except Exception as e:
        logger.error(f"æ¸…é™¤éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)


@results_app.command("show")
def show_results():
    """
    å¾ SQLite è³‡æ–™åº«æŸ¥è©¢ä¸¦é¡¯ç¤ºå›æ¸¬çµæœã€‚
    """
    import sqlite3
    import pandas as pd

    logger.info("æ­£åœ¨å¾ SQLite è³‡æ–™åº«æŸ¥è©¢çµæœ...")
    DB_PATH = "output/results.sqlite"
    TABLE_NAME = "backtest_results"
    try:
        conn = sqlite3.connect(DB_PATH)
        df = pd.read_sql_query(f"SELECT * FROM {TABLE_NAME}", conn)
        conn.close()

        if df.empty:
            logger.warning("è³‡æ–™åº«ä¸­å°šç„¡ä»»ä½•çµæœã€‚")
        else:
            logger.info("æŸ¥è©¢å®Œæˆã€‚")
            # ä½¿ç”¨ä¸€å€‹ logger å‘¼å«ä¾†é¡¯ç¤ºæ•´å€‹ DataFrame
            logger.info(f"\n--- å›æ¸¬çµæœ ---\n{df.to_string()}\n----------------")

    except Exception as e:
        logger.error(f"æŸ¥è©¢çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)


@results_app.command("generate-report")
def generate_report(
    xml_path: str = typer.Option("output/reports/report.xml", help="JUnit XML å ±å‘Šçš„è·¯å¾‘"),
    md_path: str = typer.Option("TEST_REPORT.md", help="è¦ç”Ÿæˆçš„ Markdown å ±å‘Šçš„è·¯å¾‘"),
):
    """
    å¾ JUnit XML æª”æ¡ˆç”¢ç”Ÿ Markdown å ±å‘Šã€‚
    """
    import xml.etree.ElementTree as ET
    from datetime import datetime

    logger.info(f"AI å ±å‘Šç”Ÿæˆå™¨å•Ÿå‹•ï¼Œæ­£åœ¨è®€å–åŸå§‹æ•¸æ“š: {xml_path}")
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
        suite = root.find("testsuite")

        # æå–æ ¸å¿ƒæ•¸æ“š
        total = int(suite.get("tests", 0))
        failures = int(suite.get("failures", 0))
        errors = int(suite.get("errors", 0))
        skipped = int(suite.get("skipped", 0))
        exec_time = float(suite.get("time", 0))
        passed = total - failures - errors - skipped

        # é–‹å§‹æ§‹å»º Markdown å ±å‘Š
        report_content = []
        report_content.append("# **ã€æ™®ç¾…ç±³ä¿®æ–¯ä¹‹ç«ã€‘ç³»çµ±æ¸¬è©¦ä½œæˆ°å ±å‘Š**")
        report_content.append(
            f"> å ±å‘Šç”Ÿæˆæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        )

        # ç¸½çµå€å¡Š
        report_content.append("## **ä¸€ã€ æˆ°æ³ç¸½è¦½**")
        if failures == 0 and errors == 0:
            report_content.append(
                "> **çµè«–ï¼š<font color='green'>ä»»å‹™æˆåŠŸ (SUCCESS)</font>** - æ‰€æœ‰å“è³ªé–˜é–€å‡å·²é€šéã€‚ç³»çµ±æˆ°å‚™ç‹€æ…‹è‰¯å¥½ã€‚"
            )
        else:
            report_content.append(
                "> **çµè«–ï¼š<font color='red'>ä»»å‹™å¤±æ•— (FAILURE)</font>** - ç™¼ç¾é—œéµæ€§éŒ¯èª¤ã€‚ç³»çµ±å­˜åœ¨é¢¨éšªï¼Œéœ€ç«‹å³å¯©æŸ¥ã€‚"
            )

        summary_table = [
            "| æŒ‡æ¨™ (Metric) | æ•¸é‡ (Count) |",
            "|:---|:---:|",
            f"| âœ… **æ¸¬è©¦é€šé (Passed)** | {passed} |",
            f"| âŒ **æ¸¬è©¦å¤±æ•— (Failed)** | {failures} |",
            f"| ğŸ”¥ **åŸ·è¡ŒéŒ¯èª¤ (Errors)** | {errors} |",
            f"| ğŸš§ **æ¸¬è©¦è·³é (Skipped)** | {skipped} |",
            f"| â±ï¸ **ç¸½åŸ·è¡Œæ™‚é–“ (Time)** | {exec_time:.2f} ç§’ |",
            f"| ğŸ§® **ç¸½åŸ·è¡Œæ•¸é‡ (Total)** | {total} |",
        ]
        report_content.append("\n".join(summary_table))

        # å¤±æ•—èˆ‡éŒ¯èª¤è©³æƒ…
        if failures > 0 or errors > 0:
            report_content.append("\n## **äºŒã€ å¤±æ•—èˆ‡éŒ¯èª¤è©³æƒ…**")
            count = 1
            for testcase in suite.findall("testcase"):
                failure = testcase.find("failure")
                error = testcase.find("error")

                detail = failure if failure is not None else error

                if detail is not None:
                    test_name = testcase.get("name", "æœªçŸ¥æ¸¬è©¦")
                    class_name = testcase.get("classname", "æœªçŸ¥é¡åˆ¥")
                    error_type = detail.tag.capitalize()  # "failure" -> "Failure"
                    message = detail.get("message", "ç„¡è¨Šæ¯").splitlines()[0]

                    report_content.append(f"\n### {count}. {error_type}: {message}")
                    report_content.append(
                        f"- **æ¸¬è©¦ä½ç½®:** `{class_name}.{test_name}`"
                    )
                    report_content.append("- **è©³ç´°å †ç–Šè¿½è¹¤:**")
                    # æª¢æŸ¥ detail.text æ˜¯å¦ç‚º None
                    stack_trace = (
                        detail.text.strip() if detail.text else "ç„¡å †ç–Šè¿½è¹¤è³‡è¨Šã€‚"
                    )
                    report_content.append(f"```\n{stack_trace}\n```")
                    count += 1

        # å¯«å…¥æª”æ¡ˆ
        with open(md_path, "w", encoding="utf-8") as f:
            f.write("\n".join(report_content))

        logger.info(f"ä½œæˆ°å ±å‘Šå·²æˆåŠŸç”Ÿæˆè‡³: {md_path}")

    except FileNotFoundError:
        logger.error(f"æ‰¾ä¸åˆ°åŸå§‹æ•¸æ“šæª”æ¡ˆ: {xml_path}")
    except ET.ParseError:
        logger.error(f"åŸå§‹æ•¸æ“šæª”æ¡ˆæ ¼å¼éŒ¯èª¤: {xml_path}")
    except Exception as e:
        logger.error(f"ç”Ÿæˆå ±å‘Šæ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")


@results_app.command("add-tasks")
def add_tasks(
    num_tasks: int = typer.Option(10, help="è¦æ·»åŠ çš„ä»»å‹™æ•¸é‡"),
):
    """
    å‘ä»»å‹™ä½‡åˆ—ä¸­æ·»åŠ æŒ‡å®šæ•¸é‡çš„éš¨æ©Ÿå›æ¸¬ä»»å‹™ã€‚
    """
    import random
    import uuid
    from prometheus.core.context import AppContext

    with AppContext() as ctx:
        logger.info(f"æ­£åœ¨ç”Ÿæˆ {num_tasks} å€‹å›æ¸¬ä»»å‹™...")
        batch_id = str(uuid.uuid4())
        for i in range(num_tasks):
            # ä»»å‹™ç¾åœ¨æ˜¯ä¸€å€‹å­—å…¸
            task = {
                "task_id": str(uuid.uuid4()),
                "type": "backtest",
                "strategy": "SMA_Crossover",
                "symbol": random.choice(["BTC/USDT", "ETH/USDT", "XRP/USDT"]),
                "params": {"fast": random.randint(5, 15), "slow": random.randint(20, 40)},
                "batch_id": batch_id,
            }
            ctx.queue.put(task)
            logger.debug(f"å·²å°‡ä»»å‹™ {i+1}/{num_tasks} ({task['strategy']}) æ·»åŠ åˆ°ä½‡åˆ—ã€‚")
        logger.info(f"æˆåŠŸå°‡ {num_tasks} å€‹ä»»å‹™æ·»åŠ åˆ°ä½‡åˆ—ã€‚")


pipelines_app = typer.Typer()
app.add_typer(pipelines_app, name="pipelines")

@pipelines_app.command("run-downloader")
def run_downloader(
    start_date: str = typer.Option(..., help="ä¸‹è¼‰é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)"),
    end_date: str = typer.Option(..., help="ä¸‹è¼‰çµæŸæ—¥æœŸ (YYYY-MM-DD)"),
    output_dir: str = typer.Option("data/downloads", help="æª”æ¡ˆå„²å­˜ç›®éŒ„"),
    max_workers: int = typer.Option(16, help="æœ€å¤§åŒæ™‚ä¸‹è¼‰ä»»å‹™æ•¸"),
):
    """
    TAIFEX è‡ªå‹•åŒ–æ•¸æ“šæ¡é›†å™¨ v1.0
    """
    import os
    import random
    import time
    from collections import Counter
    from concurrent.futures import ThreadPoolExecutor, as_completed
    from datetime import datetime, timedelta
    import requests
    from prometheus.core.config import config

    logger.info("--- å•Ÿå‹•æ•¸æ“šæ¡é›†ä»»å‹™ ---")
    logger.info(f"æ™‚é–“ç¯„åœ: {start_date} åˆ° {end_date}")
    logger.info(f"è¼¸å‡ºç›®éŒ„: {output_dir}")

    tasks = []
    start_dt = datetime.strptime(start_date, "%Y-%m-%d")
    end_dt = datetime.strptime(end_date, "%Y-%m-%d")
    date_range = [
        start_dt + timedelta(days=x) for x in range((end_dt - start_dt).days + 1)
    ]

    base_url = config.get("data_acquisition.taifex.base_url")
    for current_date in date_range:
        date_str = current_date.strftime("%Y_%m_%d")
        # ç¯„ä¾‹ï¼šåƒ…ä¸‹è¼‰æœŸè²¨é€ç­†è³‡æ–™
        tasks.append(
            {
                "url": f"{base_url}/file/taifex/Dailydownload/DailydownloadCSV/Daily_{date_str}.zip",
                "file_name": f"Daily_{date_str}.zip",
                "min_delay": 0.2,
                "max_delay": 1.0,
            }
        )

    results_counter = Counter()
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        with requests.Session() as session:
            future_to_task = {
                executor.submit(execute_download, session, task, output_dir): task
                for task in tasks
            }
            for future in as_completed(future_to_task):
                try:
                    status, message = future.result()
                    results_counter[status] += 1
                    logger.info(f"[{status.upper()}] {message}")
                except Exception as exc:
                    logger.info(f"[CRITICAL] ä»»å‹™åŸ·è¡Œç•°å¸¸: {exc}")

    logger.info("\n--- æ¡é›†ä»»å‹™ç¸½çµ ---")
    for status, count in results_counter.items():
        logger.info(f"  {status}: {count} å€‹")


def execute_download(session, task_info, output_dir):
    """åŸ·è¡Œå–®ä¸€æª”æ¡ˆä¸‹è¼‰ä»»å‹™ï¼ŒåŒ…å«é‡è©¦èˆ‡éŒ¯èª¤è™•ç†ã€‚"""
    import os
    import random
    import time
    import requests
    from prometheus.core.config import config

    file_path = os.path.join(output_dir, task_info["file_name"])
    if os.path.exists(file_path):
        return "exists", f"æª”æ¡ˆå·²å­˜åœ¨: {task_info['file_name']}"

    time.sleep(random.uniform(task_info["min_delay"], task_info["max_delay"]))

    user_agents = config.get("data_acquisition.taifex.user_agents")
    base_url = config.get("data_acquisition.taifex.base_url")

    for attempt in range(3):  # é‡è©¦3æ¬¡
        try:
            headers = {
                "User-Agent": random.choice(user_agents),
                "Referer": task_info.get("referer", base_url),
            }
            response = (
                session.post(
                    task_info["url"],
                    data=task_info.get("payload", {}),
                    headers=headers,
                    timeout=120,
                )
                if task_info.get("payload")
                else session.get(task_info["url"], headers=headers, timeout=120)
            )

            if (
                response.status_code == 200
                and len(response.content) > 100
                and "æŸ¥ç„¡è³‡æ–™" not in response.text
            ):
                os.makedirs(output_dir, exist_ok=True)
                with open(file_path, "wb") as f:
                    f.write(response.content)
                return "success", f"æˆåŠŸä¸‹è¼‰: {task_info['file_name']}"
            elif response.status_code == 404:
                return "not_found", f"404 Not Found: {task_info['file_name']}"
            else:
                return (
                    "error",
                    f"ä¼ºæœå™¨éŒ¯èª¤ {response.status_code}: {task_info['file_name']}",
                )

        except requests.exceptions.RequestException as e:
            if attempt == 2:
                return "error", f"ç¶²è·¯è«‹æ±‚å¤±æ•—: {e}"
            time.sleep(5 * (attempt + 1))

    return "error", f"é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸: {task_info['file_name']}"


@pipelines_app.command("run-explorer")
def run_explorer(
    input_dir: str = typer.Option("data/downloads", help="æƒæçš„åŸå§‹æª”æ¡ˆç›®éŒ„"),
    db_path: str = typer.Option("data/metadata/schema_registry.db", help="æ ¼å¼è¨»å†Šè¡¨è³‡æ–™åº«è·¯å¾‘"),
):
    """
    TAIFEX æ ¼å¼æ¢å‹˜èˆ‡è¨»å†Šå™¨ v1.0
    """
    import hashlib
    import os
    from prometheus.core.db.schema_registry import SchemaRegistry
    from prometheus.core.utils.helpers import (
        prospect_file_content,
        read_file_content,
    )

    registry = SchemaRegistry(db_path)
    logger.info(f"é–‹å§‹æƒæç›®éŒ„: {input_dir}")
    new_formats = 0
    updated_formats = 0

    for filename in os.listdir(input_dir):
        file_path = os.path.join(input_dir, filename)
        if not os.path.isfile(file_path):
            continue

        try:
            file_bytes_content = read_file_content(file_path)
            if file_bytes_content is None:
                continue

            result = prospect_file_content(file_bytes_content)

            if result["status"] == "success":
                fingerprint = get_header_fingerprint(result["header"])
                status = registry.add_or_update_schema(fingerprint, result["header"], result["encoding"], filename)
                if status == "new":
                    new_formats += 1
                else:
                    updated_formats += 1

        except Exception as e:
            logger.error(f"è™•ç†æª”æ¡ˆ {filename} å¤±æ•—: {e}", exc_info=True)

    registry.close()
    logger.info("--- æ ¼å¼æ¢å‹˜ç¸½çµ ---")
    logger.info(f"ç™¼ç¾æ–°æ ¼å¼: {new_formats} ç¨®")
    logger.info(f"æ›´æ–°ç¾æœ‰æ ¼å¼è¨ˆæ•¸: {updated_formats} æ¬¡")


def get_header_fingerprint(header_line: str) -> str:
    """å°æ¨™æº–åŒ–å¾Œçš„æ¨™é ­è¨ˆç®—æŒ‡ç´‹ã€‚"""
    import hashlib
    normalized_header = "".join(header_line.lower().split()).replace('"', "")
    return hashlib.sha256(normalized_header.encode("utf-8")).hexdigest()


@pipelines_app.command("run-elt")
def run_elt(
    input_dir: str = typer.Option("data/downloads", help="ä¸‹è¼‰æª”æ¡ˆçš„ä¾†æºç›®éŒ„ (ä¾› Loader ä½¿ç”¨)"),
    raw_db_path: str = typer.Option("data/raw_warehouse/raw_taifex.duckdb", help="åŸå§‹æ•¸æ“šè‰™è³‡æ–™åº«è·¯å¾‘"),
    schema_db_path: str = typer.Option("data/metadata/schema_registry.db", help="æ ¼å¼è¨»å†Šè¡¨è³‡æ–™åº«è·¯å¾‘"),
    analytics_db_path: str = typer.Option("data/analytics_warehouse/analytics_taifex.duckdb", help="åˆ†ææ•¸æ“šåº«è·¯å¾‘"),
):
    """
    TAIFEX ELT åŠ å·¥ç®¡ç·š v1.0
    """
    import os
    from prometheus.core.db.data_warehouse import AnalyticsDataWarehouse, RawDataWarehouse
    from prometheus.core.db.schema_registry import SchemaRegistry

    # Ensure parent directories for database files exist
    os.makedirs(os.path.dirname(raw_db_path), exist_ok=True)
    os.makedirs(os.path.dirname(schema_db_path), exist_ok=True)
    os.makedirs(os.path.dirname(analytics_db_path), exist_ok=True)

    run_loader(input_dir, raw_db_path, schema_db_path)
    run_transformer(raw_db_path, schema_db_path, analytics_db_path)


def run_loader(input_dir, raw_db_path, schema_db_path):
    import os
    from prometheus.core.db.data_warehouse import RawDataWarehouse
    from prometheus.core.db.schema_registry import SchemaRegistry
    from prometheus.core.utils.helpers import (
        prospect_file_content,
        read_file_content,
    )

    logger.info("--- [éšæ®µ 2] åŸ·è¡Œ Loader ---")
    raw_wh = RawDataWarehouse(raw_db_path)
    schema_registry = SchemaRegistry(schema_db_path)

    known_fingerprints = schema_registry.get_known_fingerprints()
    if not known_fingerprints:
        logger.info("Loader: No known fingerprints loaded from schema registry. Only files matching these will be processed.")

    files_loaded = 0
    if not os.path.exists(input_dir):
        logger.warning(f"Loader input directory {input_dir} does not exist. Skipping loading.")
        raw_wh.close()
        schema_registry.close()
        return

    for filename in os.listdir(input_dir):
        file_path = os.path.join(input_dir, filename)

        if not os.path.isfile(file_path):
            continue

        if raw_wh.is_file_processed(file_path):
            logger.debug(f"Loader: File {filename} already in raw_import_log. Skipping.")
            continue

        try:
            file_bytes_content = read_file_content(file_path)
            if file_bytes_content is None:
                continue

            result = prospect_file_content(file_bytes_content)
            if result["status"] == "success":
                fingerprint = get_header_fingerprint(result["header"])
                if fingerprint in known_fingerprints:
                    raw_wh.log_processed_file(file_path, file_bytes_content, fingerprint)
                    files_loaded += 1
                    logger.info(f"Loader: Loaded {filename} (fingerprint: {fingerprint[:8]}...) as it's a known schema.")
                else:
                    logger.info(f"Loader: Skipped {filename} (fingerprint: {fingerprint[:8]}...) as its schema is not in the registry.")
            else:
                logger.warning(f"Loader: Skipped file {filename} due to content prospecting failure: {result.get('error', 'Unknown error')}")

        except Exception as e:
            logger.error(f"Loader è™•ç† {filename} å¤±æ•—: {e}", exc_info=True)

    raw_wh.close()
    schema_registry.close()
    logger.info(f"Loader å®Œæˆï¼Œæ–°è¼‰å…¥ {files_loaded} å€‹æª”æ¡ˆã€‚")


def run_transformer(raw_db_path, schema_db_path, analytics_db_path):
    import io
    import pandas as pd
    from prometheus.core.db.data_warehouse import AnalyticsDataWarehouse, RawDataWarehouse
    from prometheus.core.db.schema_registry import SchemaRegistry

    logger.info("--- [éšæ®µ 3] åŸ·è¡Œ Transformer ---")
    schema_registry = SchemaRegistry(schema_db_path)
    raw_wh = RawDataWarehouse(raw_db_path)
    analytics_wh = AnalyticsDataWarehouse(analytics_db_path)

    schema_map = schema_registry.get_all_schemas()
    if not schema_map:
        logger.warning("Transformer: æ ¼å¼è¨»å†Šè¡¨ç‚ºç©ºæˆ–è®€å–å¤±æ•—ï¼ŒTransformer ç„¡æ³•åŸ·è¡Œæœ‰æ•ˆè½‰æ›ã€‚")
        schema_registry.close()
        raw_wh.close()
        analytics_wh.close()
        return

    daily_futures_header_str = "äº¤æ˜“æ—¥æœŸ,å¥‘ç´„ä»£ç¢¼,åˆ°æœŸæœˆä»½(é€±åˆ¥),é–‹ç›¤åƒ¹,æœ€é«˜åƒ¹,æœ€ä½åƒ¹,æ”¶ç›¤åƒ¹,æˆäº¤é‡"
    target_daily_futures_fingerprint = get_header_fingerprint(daily_futures_header_str)

    if target_daily_futures_fingerprint not in schema_map:
        logger.warning(f"Transformer: Did not find fingerprint for daily_futures_header '{daily_futures_header_str}' in schema_map. Cannot process daily_futures.")
    else:
        logger.info(f"Transformer: Target fingerprint for daily_futures is {target_daily_futures_fingerprint[:8]}...")

    analytics_wh.create_daily_futures_table()

    records = raw_wh.execute_query("SELECT content_blob, format_fingerprint FROM raw_import_log").fetchall()
    transformed_count = 0
    for blob, fingerprint in records:
        if fingerprint != target_daily_futures_fingerprint:
            continue

        if fingerprint not in schema_map:
            continue

        header_str_from_registry, encoding = schema_map[fingerprint]
        try:
            df = pd.read_csv(io.BytesIO(blob), encoding=encoding, thousands=",", header=0, on_bad_lines="skip")
            df.columns = [str(col).strip().replace('"', "") for col in df.columns]

            target_columns_canonical = [
                "äº¤æ˜“æ—¥æœŸ", "å¥‘ç´„ä»£ç¢¼", "åˆ°æœŸæœˆä»½(é€±åˆ¥)", "é–‹ç›¤åƒ¹",
                "æœ€é«˜åƒ¹", "æœ€ä½åƒ¹", "æ”¶ç›¤åƒ¹", "æˆäº¤é‡",
            ]

            df_to_load = pd.DataFrame()
            for canonical_col_name in target_columns_canonical:
                if canonical_col_name in df.columns:
                    df_to_load[canonical_col_name] = df[canonical_col_name]
                else:
                    df_to_load[canonical_col_name] = None

            if not df_to_load.empty:
                analytics_wh.insert_daily_futures(df_to_load)
                transformed_count += 1

        except pd.errors.EmptyDataError:
            logger.warning(f"Transformer: No data or columns found in CSV for fingerprint {fingerprint[:8]}...")
        except Exception as e:
            logger.error(f"Transformer è™•ç†æŒ‡ç´‹ {fingerprint[:8]}... çš„è³‡æ–™æ™‚å¤±æ•—: {e}", exc_info=True)

    raw_wh.close()
    analytics_wh.close()
    schema_registry.close()
    logger.info(f"Transformer å®Œæˆï¼ŒæˆåŠŸè½‰æ› {transformed_count} ç­†è¨˜éŒ„ã€‚")


@pipelines_app.command("run-stock-factors")
def run_stock_factors():
    """
    åŸ·è¡Œç¬¬å››è™Ÿç”Ÿç”¢ç·šï¼šè‚¡ç¥¨å› å­ç”Ÿæˆã€‚
    """
    from prometheus.pipelines.p4_stock_factor_generation import main as p4_main
    logger.info("--- å•Ÿå‹• P4ï¼šè‚¡ç¥¨å› å­ç”Ÿæˆç®¡ç·š ---")
    p4_main()
    logger.info("--- P4ï¼šè‚¡ç¥¨å› å­ç”Ÿæˆç®¡ç·šåŸ·è¡Œå®Œç•¢ ---")


@pipelines_app.command("run-crypto-factors")
def run_crypto_factors():
    """
    åŸ·è¡Œç¬¬äº”è™Ÿç”Ÿç”¢ç·šï¼šåŠ å¯†è²¨å¹£å› å­ç”Ÿæˆã€‚
    """
    from prometheus.pipelines.p5_crypto_factor_generation import main as p5_main
    logger.info("--- å•Ÿå‹• P5ï¼šåŠ å¯†è²¨å¹£å› å­ç”Ÿæˆç®¡ç·š ---")
    p5_main()
    logger.info("--- P5ï¼šåŠ å¯†è²¨å¹£å› å­ç”Ÿæˆç®¡ç·šåŸ·è¡Œå®Œç•¢ ---")


@app.command(name="build-feature-store")
def build_feature_store():
    """
    ã€ä½œæˆ°æŒ‡ä»¤ã€‘çµ±ä¸€æ•¸æ“šå€‰å„²é‡æ§‹ï¼šå»ºé€ ç‰¹å¾µå€‰å„²ã€‚
    """
    import asyncio
    from prometheus.core.db.db_manager import DBManager
    # from prometheus.pipelines.p1_factor_generation import p1_factor_generation_pipeline
    # from prometheus.pipelines.p2_index_factor_generation import p2_index_factor_pipeline
    # from prometheus.pipelines.p3_bond_factor_generation import p3_bond_factor_pipeline
    from prometheus.pipelines.p4_stock_factor_generation import main as p4_main
    from prometheus.pipelines.p5_crypto_factor_generation import main as p5_main

    logger.info("--- å•Ÿå‹•çµ±ä¸€æ•¸æ“šå€‰å„²å»ºæ§‹æµç¨‹ ---")
    db_manager = DBManager()

    # --- P1, P2, P3 (å·²åœç”¨) ---
    # æ ¹æ“šç›®å‰çš„æª”æ¡ˆçµæ§‹ï¼Œé€™äº›ç®¡ç·šä¸å­˜åœ¨ï¼Œæš«æ™‚è¨»è§£ä»¥ç¢ºä¿å‘½ä»¤å¯åŸ·è¡Œ
    # logger.info("åŸ·è¡Œ P1 é€šç”¨å› å­ç”Ÿæˆ...")
    # p1_df = asyncio.run(p1_factor_generation_pipeline.run())
    # db_manager.save_data(p1_df, 'factors')
    # logger.info("P1 é€šç”¨å› å­æ•¸æ“šå·²åˆä½µã€‚")

    # logger.info("åŸ·è¡Œ P2 æŒ‡æ•¸å› å­ç”Ÿæˆ...")
    # p2_df = asyncio.run(p2_index_factor_pipeline.run())
    # db_manager.save_data(p2_df, 'factors')
    # logger.info("P2 æŒ‡æ•¸å› å­æ•¸æ“šå·²åˆä½µã€‚")

    # logger.info("åŸ·è¡Œ P3 å‚µåˆ¸å› å­ç”Ÿæˆ...")
    # p3_df = asyncio.run(p3_bond_factor_pipeline.run())
    # db_manager.save_data(p3_df, 'factors')
    # logger.info("P3 å‚µåˆ¸å› å­æ•¸æ“šå·²åˆä½µã€‚")

    # --- P4, P5 ---
    # é€™äº›ç®¡ç·šçš„ main å‡½æ•¸å…§éƒ¨ç›´æ¥èª¿ç”¨ DBManagerï¼Œæˆ‘å€‘æš«æ™‚ä¿æŒé€™ç¨®æ–¹å¼
    # æœªä¾†å¯ä»¥é€²ä¸€æ­¥é‡æ§‹ï¼Œä½†ç›®å‰è¶³ä»¥æ»¿è¶³ä½œæˆ°ç›®æ¨™
    logger.info("åŸ·è¡Œ P4 è‚¡ç¥¨å› å­ç”Ÿæˆ...")
    p4_main()
    logger.info("P4 è‚¡ç¥¨å› å­æ•¸æ“šå·²åˆä½µã€‚")

    logger.info("åŸ·è¡Œ P5 åŠ å¯†è²¨å¹£å› å­ç”Ÿæˆ...")
    p5_main()
    logger.info("P5 åŠ å¯†è²¨å¹£å› å­æ•¸æ“šå·²åˆä½µã€‚")

    logger.info("--- çµ±ä¸€æ•¸æ“šå€‰å„²å»ºæ§‹æµç¨‹å®Œç•¢ ---")


@pipelines_app.command("run-simulation-training")
def run_simulation_training(
    target_factor: str = typer.Option(..., help="è¦æ¨¡æ“¬çš„ç›®æ¨™å› å­åç¨±"),
):
    """
    åŸ·è¡Œç¬¬å…­è™Ÿç”Ÿç”¢ç·šï¼šå› å­ä»£ç†æ¨¡æ“¬æ¨¡å‹è¨“ç·´ã€‚
    """
    from prometheus.pipelines.p6_simulation_training import run_main as p6_run_main
    logger.info(f"--- å•Ÿå‹• P6ï¼šå› å­ä»£ç†æ¨¡æ“¬æ¨¡å‹è¨“ç·´ç®¡ç·šï¼Œç›®æ¨™ç‚º {target_factor} ---")
    p6_run_main(target_factor=target_factor)
    logger.info(f"--- P6ï¼šå› å­ä»£ç†æ¨¡æ“¬æ¨¡å‹è¨“ç·´ç®¡ç·šåŸ·è¡Œå®Œç•¢ ---")


@pipelines_app.command("run")
def run_pipeline(
    name: str = typer.Option(..., help="è¦åŸ·è¡Œçš„ç®¡ç·šåç¨±"),
    ticker: str = typer.Option(None, "--ticker", "-t", help="è¦è™•ç†çš„è³‡ç”¢ä»£è™Ÿ")
):
    """
    åŸ·è¡ŒæŒ‡å®šçš„æ•¸æ“šç®¡ç·šã€‚
    """
    import asyncio
    pipeline_context = {"ticker": ticker} if ticker else {}
    logger.info(f"--- å•Ÿå‹• {name} ç®¡ç·šï¼Œä¸Šä¸‹æ–‡: {pipeline_context} ---")

    if name == "p1_factor_generation":
        from prometheus.pipelines.p1_factor_generation import p1_factor_generation_pipeline
        asyncio.run(p1_factor_generation_pipeline.run(context=pipeline_context))
    elif name == "p2_index_factor_generation":
        from prometheus.pipelines.p2_index_factor_generation import p2_index_factor_pipeline
        asyncio.run(p2_index_factor_pipeline.run(context=pipeline_context))
    elif name == "p3_bond_factor_generation":
        from prometheus.pipelines.p3_bond_factor_generation import p3_bond_factor_pipeline
        asyncio.run(p3_bond_factor_pipeline.run(context=pipeline_context))
    else:
        logger.error(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°åç‚º '{name}' çš„ç®¡ç·šã€‚")
        raise typer.Exit(code=1)

    logger.info(f"--- {name} ç®¡ç·šåŸ·è¡Œå®Œç•¢ ---")


@pipelines_app.command("run-backfill")
def run_backfill_cli(
    start_date: str = typer.Option(..., help="å›å¡«é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)"),
    end_date: str = typer.Option(..., help="å›å¡«çµæŸæ—¥æœŸ (YYYY-MM-DD)"),
):
    """
    åŸ·è¡Œæ­·å²æ•¸æ“šå›å¡«ç®¡ç·šã€‚
    """
    import pandas as pd
    from prometheus.core.analysis.data_engine import DataEngine
    from prometheus.core.clients.client_factory import ClientFactory

    logger.info(f"--- é–‹å§‹åŸ·è¡Œæ•¸æ“šå›å¡«ä½œæ¥­ï¼šå¾ {start_date} åˆ° {end_date} ---")

    data_engine = DataEngine()
    hourly_timestamps = pd.date_range(start=start_date, end=end_date, freq="H")
    total_tasks = len(hourly_timestamps)

    for i, ts in enumerate(hourly_timestamps):
        logger.debug(f"--- æ­£åœ¨è™•ç† ({i + 1}/{total_tasks}): {ts} ---")
        try:
            data_engine.generate_snapshot(ts)
        except Exception as e:
            logger.error(f"âŒ è™•ç† {ts} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)

    data_engine.close()
    ClientFactory.close_all()
    logger.info("--- æ•¸æ“šå›å¡«ä½œæ¥­å®Œæˆ ---")


from prometheus.models.strategy_models import Strategy
from prometheus.services.backtesting_service import BacktestingService
from prometheus.services.evolution_chamber import EvolutionChamber
from prometheus.services.strategy_reporter import StrategyReporter
from prometheus.core.db.db_manager import DBManager

@app.command()
def run_evolution_cycle():
    """
    ğŸš€ [ç«¯åˆ°ç«¯] åŸ·è¡Œä¸€æ¬¡å®Œæ•´çš„æ¼”åŒ–é€±æœŸï¼šæ¼”åŒ– -> å›æ¸¬ -> å ±å‘Šã€‚
    """
    print("--- å•Ÿå‹•ã€æ¼”åŒ–å®¤è¡Œå‹•ã€‘å®Œæ•´ä½œæˆ°é€±æœŸ ---")

    # 1. åˆå§‹åŒ–æ ¸å¿ƒæœå‹™
    db_manager = DBManager()
    backtester = BacktestingService(db_manager)

    # 2. æº–å‚™æ¼”åŒ–æ‰€éœ€æ•¸æ“š
    # å‡è¨­å› å­æ•¸æ“šå·²å­˜åœ¨
    all_factors_df = db_manager.fetch_table('factors')
    # æ’é™¤éå› å­æ¬„ä½
    available_factors = [col for col in all_factors_df.columns if col not in ['date', 'symbol', 'close']]

    if not available_factors:
        print("éŒ¯èª¤ï¼šæ•¸æ“šåº«ä¸­æ‰¾ä¸åˆ°å¯ç”¨çš„å› å­ã€‚è«‹å…ˆåŸ·è¡Œ build-feature-storeã€‚")
        return

    target_asset_for_evolution = 'AAPL' # é¸æ“‡ä¸€å€‹æ•¸æ“šåº«ä¸­å­˜åœ¨çš„è³‡ç”¢
    print(f"INFO: å°‡ä½¿ç”¨ '{target_asset_for_evolution}' ä½œç‚ºæœ¬æ¬¡æ¼”åŒ–çš„ç›®æ¨™è³‡ç”¢ã€‚")
    chamber = EvolutionChamber(backtester, available_factors, target_asset=target_asset_for_evolution)

    # 3. åŸ·è¡Œæ¼”åŒ–
    # ç‚ºäº†å¿«é€Ÿæ¼”ç¤ºï¼Œä½¿ç”¨è¼ƒå°çš„åƒæ•¸
    hof = chamber.run_evolution(n_pop=20, n_gen=5)

    if not hof:
        print("éŒ¯èª¤ï¼šæ¼”åŒ–æœªèƒ½ç”¢ç”Ÿæœ‰æ•ˆçµæœã€‚")
        return

    # 4. å°æœ€å„ªç­–ç•¥é€²è¡Œæœ€çµ‚å›æ¸¬ä»¥ç²å¾—å®Œæ•´å ±å‘Š
    best_individual = hof[0]
    best_factors = [available_factors[i] for i in best_individual]
    final_strategy = Strategy(
        factors=best_factors,
        weights={factor: 1.0 / len(best_factors) for factor in best_factors},
        target_asset='AAPL' # ä¿®æ­£ï¼šæ˜ç¢ºæŒ‡å®šä¸€å€‹å­˜åœ¨çš„è³‡ç”¢
    )
    final_report = backtester.run(final_strategy)

    # 5. ç”Ÿæˆå ±å‘Š
    reporter = StrategyReporter()
    reporter.generate_report(hof, final_report, available_factors)

    print("--- ã€æ¼”åŒ–å®¤è¡Œå‹•ã€‘ä½œæˆ°é€±æœŸçµæŸ ---")

# --- Transcriber CLI ---
transcriber_cli = typer.Typer()
app.add_typer(transcriber_cli, name="transcriber")

@transcriber_cli.command(name="run-server")
def run_transcriber_server(
    host: str = typer.Option("127.0.0.1", help="ç¶å®šä¸»æ©Ÿ"),
    port: int = typer.Option(8001, help="ç¶å®šåŸ è™Ÿ"),
    num_workers: int = typer.Option(1, help="è¦å•Ÿå‹•çš„è½‰å¯«å·¥äººæ•¸é‡"),
):
    """
    å•Ÿå‹•è½‰å¯«æœå‹™ API ä¼ºæœå™¨ã€‚
    """
    import multiprocessing as mp
    import sys
    import time
    from prometheus.transcriber.main import app as transcriber_app
    from prometheus.transcriber.transcriber_worker import transcriber_worker_process
    from prometheus.transcriber.mock_worker import mock_worker_process
    import uvicorn

    logger.info("--- å•Ÿå‹•ã€é³³å‡°å°ˆæ¡ˆã€‘è½‰å¯«æœå‹™ ---")

    # æ ¹æ“šå¹³å°è¨­å®šå¤šé€²ç¨‹å•Ÿå‹•æ–¹æ³•
    if sys.platform == "darwin":
        mp.set_start_method("spawn", force=True)
    else:
        mp.set_start_method("spawn")

    # We need to get the queue from the app state, but the app is not running yet.
    # For now, we will create a new queue and pass it to both the app and the workers.
    # This is not ideal, but it's a temporary solution.

    task_queue = mp.Queue()
    transcriber_app.state.task_queue = task_queue # Inject the queue into the app state

    processes = []

    # 1. å•Ÿå‹• API ä¼ºæœå™¨
    api_process = mp.Process(
        target=uvicorn.run,
        kwargs={"app": transcriber_app, "host": host, "port": port, "log_level": "info"},
        name="TranscriberAPIServer"
    )
    processes.append(api_process)

    # 2. é¸æ“‡ä¸¦å•Ÿå‹•å·¥äºº
    # é€™è£¡æˆ‘å€‘æš«æ™‚ç¡¬ç·¨ç¢¼ä½¿ç”¨æ¨¡æ“¬å·¥äººï¼Œä»¥ä¾¿æ¸¬è©¦
    worker_target = mock_worker_process
    worker_name = "MockWorkerProcess"
    logger.info(f"å°‡å•Ÿå‹• {num_workers} å€‹æ¨¡æ“¬è½‰å¯«å·¥äººã€‚")

    for i in range(num_workers):
        worker_process_instance = mp.Process(
            target=worker_target,
            # ç°¡åŒ–åƒæ•¸ï¼Œæš«æ™‚ä¸å‚³å…¥ config å’Œ result_queue
            args=(None, task_queue, None, None),
            name=f"{worker_name}-{i+1}"
        )
        processes.append(worker_process_instance)

    try:
        for p in processes:
            p.start()
            logger.info(f"å·²å•Ÿå‹•è¡Œç¨‹: {p.name} (PID: {p.pid})")

        # ä¸»è¡Œç¨‹ç­‰å¾…æ‰€æœ‰å­è¡Œç¨‹çµæŸ
        for p in processes:
            p.join()

    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨é—œé–‰æ‰€æœ‰è½‰å¯«æœå‹™è¡Œç¨‹...")
    finally:
        for p in processes:
            if p.is_alive():
                p.terminate()
                p.join(timeout=5)
        logger.info("æ‰€æœ‰è½‰å¯«æœå‹™å·²é—œé–‰ã€‚")


if __name__ == "__main__":
    app()
\n\n# ==================== Transcriber Logic ====================\n\n
"""è½‰å¯«æœå‹™ API ä¸»æ‡‰ç”¨ç¨‹å¼"""
import uuid
from contextlib import asynccontextmanager
from pathlib import Path
from typing import Any, AsyncGenerator
import logging

import aiofiles
import aiosqlite
from fastapi import FastAPI, File, HTTPException, UploadFile, Request
from fastapi.staticfiles import StaticFiles

from prometheus.core.logging.log_manager import LogManager
from prometheus.transcriber.core import DATABASE_FILE, UPLOAD_DIR, initialize_database

# --- éœæ…‹æª”æ¡ˆç›®éŒ„è¨­å®š ---
static_dir = Path("static")
static_dir.mkdir(exist_ok=True)

# --- æ—¥èªŒè¨˜éŒ„å™¨è¨­å®š ---
log_manager = LogManager(log_file="transcriber_api.log")
logger = log_manager.get_logger(__name__)


# --- æ‡‰ç”¨ç¨‹å¼ç”Ÿå‘½é€±æœŸç®¡ç† ---
import multiprocessing as mp

@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    """è™•ç†æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•èˆ‡é—œé–‰äº‹ä»¶"""
    logger.info("è½‰å¯«æœå‹™ API å•Ÿå‹•ä¸­...")
    await initialize_database()
    # å¦‚æœæ²’æœ‰å¾ CLI æ³¨å…¥ä½‡åˆ—ï¼Œå‰‡å»ºç«‹ä¸€å€‹æ–°çš„
    if not hasattr(app.state, 'task_queue'):
        logger.warning("æœªåœ¨ app.state ä¸­æ‰¾åˆ°ä»»å‹™ä½‡åˆ—ï¼Œæ­£åœ¨å»ºç«‹æ–°çš„ä½‡åˆ—ã€‚")
        app.state.task_queue = mp.Queue()
    yield
    logger.info("è½‰å¯«æœå‹™ API å·²é—œé–‰ã€‚")
    # ä½‡åˆ—çš„ç”Ÿå‘½é€±æœŸç”± CLI ç®¡ç†ï¼Œæ­¤è™•ä¸é—œé–‰


# --- FastAPI æ‡‰ç”¨ç¨‹å¼å¯¦ä¾‹ ---
app = FastAPI(lifespan=lifespan)


# --- API ç«¯é» ---
@app.get("/health", status_code=200, summary="å¥åº·æª¢æŸ¥")
async def health_check() -> dict[str, str]:
    """æä¾›ä¸€å€‹ç°¡å–®çš„å¥åº·æª¢æŸ¥ç«¯é»ï¼Œç¢ºèªæœå‹™æ˜¯å¦æ­£å¸¸é‹è¡Œã€‚"""
    return {"status": "ok"}


@app.post("/upload", status_code=202, summary="ä¸Šå‚³éŸ³æª”ä¸¦å»ºç«‹è½‰å¯«ä»»å‹™")
async def upload_file(
    request: Request,
    file: UploadFile = File(..., description="è¦ä¸Šå‚³çš„éŸ³è¨Šæª”æ¡ˆ"),
) -> dict[str, str]:
    """
    æ¥æ”¶å®¢æˆ¶ç«¯ä¸Šå‚³çš„éŸ³è¨Šæª”æ¡ˆï¼Œå°‡å…¶å„²å­˜è‡³ä¼ºæœå™¨ï¼Œä¸¦ç‚ºå…¶å»ºç«‹ä¸€å€‹æ–°çš„è½‰å¯«ä»»å‹™ã€‚

    - **å„²å­˜æª”æ¡ˆ**: æª”æ¡ˆæœƒä»¥ UUID é‡æ–°å‘½åï¼Œä»¥é¿å…æª”åè¡çªã€‚
    - **å»ºç«‹ä»»å‹™**: åœ¨è³‡æ–™åº«ä¸­æ–°å¢ä¸€ç­†ä»»å‹™è¨˜éŒ„ã€‚
    - **åŠ å…¥ä½‡åˆ—**: å°‡ä»»å‹™ ID åŠ å…¥èƒŒæ™¯è™•ç†ä½‡åˆ—ï¼Œäº¤ç”±å·¥äººé€²ç¨‹è™•ç†ã€‚
    """
    task_id = str(uuid.uuid4())
    # ç¢ºä¿ä¸Šå‚³ç›®éŒ„å­˜åœ¨
    UPLOAD_DIR.mkdir(exist_ok=True)
    filepath = UPLOAD_DIR / f"{task_id}_{file.filename}"

    try:
        # ä»¥ 1MB çš„å€å¡Šè®€å¯«æª”æ¡ˆï¼Œé¿å…ä¸€æ¬¡æ€§è¼‰å…¥éå¤§æª”æ¡ˆä½”ç”¨è¨˜æ†¶é«”
        async with aiofiles.open(filepath, "wb") as out_file:
            while content := await file.read(1024 * 1024):
                await out_file.write(content)
        logger.info("æª”æ¡ˆ '%s' å·²æˆåŠŸä¸Šå‚³è‡³ '%s'", file.filename, filepath)

        # åœ¨è³‡æ–™åº«ä¸­è¨˜éŒ„æ­¤ä»»å‹™
        async with aiosqlite.connect(DATABASE_FILE) as db:
            await db.execute(
                "INSERT INTO transcription_tasks (id, original_filepath) VALUES (?, ?)",
                (task_id, str(filepath)),
            )
            await db.commit()

        # å°‡ä»»å‹™ ID æ¨å…¥ä½‡åˆ—
        task_queue = request.app.state.task_queue
        task_queue.put(task_id)
        logger.info("ä»»å‹™ %s å·²æˆåŠŸåŠ å…¥è½‰å¯«ä½‡åˆ—ã€‚", task_id)

    except IOError as e:
        logger.exception("æª”æ¡ˆæ“ä½œæ™‚ç™¼ç”ŸéŒ¯èª¤: %s", e)
        raise HTTPException(status_code=500, detail="æª”æ¡ˆå„²å­˜å¤±æ•—ï¼Œè«‹æª¢æŸ¥ä¼ºæœå™¨æ¬Šé™æˆ–ç£ç¢Ÿç©ºé–“ã€‚") from e
    except aiosqlite.Error as e:
        logger.exception("è³‡æ–™åº«æ“ä½œæ™‚ç™¼ç”ŸéŒ¯èª¤: %s", e)
        raise HTTPException(status_code=500, detail="ç„¡æ³•å»ºç«‹è½‰å¯«ä»»å‹™ï¼Œè«‹æª¢æŸ¥è³‡æ–™åº«ç‹€æ…‹ã€‚") from e

    return {"task_id": task_id}


@app.get("/status/{task_id}", summary="æŸ¥è©¢è½‰å¯«ä»»å‹™ç‹€æ…‹")
async def get_task_status(
    task_id: str,
) -> dict[str, Any]:
    """æ ¹æ“šä»»å‹™ IDï¼ŒæŸ¥è©¢ä¸¦è¿”å›è©²ä»»å‹™çš„ç•¶å‰ç‹€æ…‹ã€é€²åº¦åŠè½‰å¯«çµæœã€‚"""
    try:
        async with aiosqlite.connect(DATABASE_FILE) as db:
            db.row_factory = aiosqlite.Row  # è®“æŸ¥è©¢çµæœå¯ä»¥åƒå­—å…¸ä¸€æ¨£å–å€¼
            async with db.execute(
                "SELECT * FROM transcription_tasks WHERE id = ?", (task_id,)
            ) as cursor:
                task = await cursor.fetchone()

        if task is None:
            raise HTTPException(status_code=404, detail=f"æ‰¾ä¸åˆ°ä»»å‹™ ID: {task_id}")

        return dict(task)

    except aiosqlite.Error as e:
        logger.exception("æŸ¥è©¢ä»»å‹™ %s ç‹€æ…‹æ™‚ç™¼ç”Ÿè³‡æ–™åº«éŒ¯èª¤: %s", task_id, e)
        raise HTTPException(status_code=500, detail="æŸ¥è©¢ä»»å‹™ç‹€æ…‹æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚") from e


# --- æ›è¼‰éœæ…‹æª”æ¡ˆ ---
# å°‡ static ç›®éŒ„ä¸‹çš„æª”æ¡ˆä½œç‚ºæ ¹è·¯å¾‘ (/) çš„éœæ…‹è³‡æºï¼Œä¸¦æä¾› index.html ä½œç‚ºé è¨­é é¢
app.mount("/", StaticFiles(directory=str(static_dir), html=True), name="static")

# --- ç›´æ¥åŸ·è¡Œæ™‚çš„å‚™ç”¨å•Ÿå‹•æ–¹å¼ ---
if __name__ == "__main__":
    import uvicorn
    logger.info("ä½¿ç”¨ uvicorn ç›´æ¥å•Ÿå‹• API ä¼ºæœå™¨ (åƒ…ä¾›é–‹ç™¼æ¸¬è©¦)...")
    uvicorn.run(app, host="127.0.0.1", port=8000)
"""çœŸå¯¦è½‰å¯«å·¥äººæ¨¡çµ„"""
import asyncio
import multiprocessing as mp
import time
from typing import Any, Optional

from faster_whisper import WhisperModel
import aiosqlite

from prometheus.core.logging.log_manager import LogManager
from prometheus.transcriber.core import DATABASE_FILE
from prometheus.transcriber.core.hardware import get_best_hardware_config

# --- æ—¥èªŒè¨˜éŒ„å™¨è¨­å®š ---
log_manager = LogManager(log_file="transcriber_worker.log")
logger = log_manager.get_logger(__name__)


async def process_single_task(task_id: str) -> None:
    """
    è™•ç†ä¸€å€‹æŒ‡å®šçš„è½‰å¯«ä»»å‹™ã€‚

    Args:
        task_id (str): è¦è™•ç†çš„ä»»å‹™ IDã€‚
    """
    logger.info("é–‹å§‹è™•ç†ä»»å‹™: %s", task_id)
    audio_path = None

    try:
        # 1. æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚º 'processing'
        await update_task_status_in_db(task_id, "processing")

        # 2. å¾è³‡æ–™åº«ç²å–éŸ³æª”è·¯å¾‘
        async with aiosqlite.connect(DATABASE_FILE) as db:
            async with db.execute(
                "SELECT original_filepath FROM transcription_tasks WHERE id = ?",
                (task_id,),
            ) as cursor:
                row = await cursor.fetchone()
                if not row:
                    logger.error("åœ¨è³‡æ–™åº«ä¸­æ‰¾ä¸åˆ°ä»»å‹™ %s çš„æª”æ¡ˆè·¯å¾‘ã€‚", task_id)
                    await update_task_status_in_db(task_id, "failed", error_message="æ‰¾ä¸åˆ°æª”æ¡ˆè·¯å¾‘")
                    return
                audio_path = row[0]

        # 3. åŸ·è¡Œè½‰å¯«
        hardware_config = get_best_hardware_config()
        logger.info("ä½¿ç”¨ç¡¬é«”è¨­å®š: %s", hardware_config)
        model = WhisperModel(
            "tiny",  # æš«æ™‚ä½¿ç”¨ tiny æ¨¡å‹ä»¥åˆ©æ¸¬è©¦
            device=hardware_config["device"],
            compute_type=hardware_config["compute_type"],
        )
        segments, _info = model.transcribe(audio_path, beam_size=5)
        full_transcript = "".join(segment.text for segment in segments)
        logger.info("ä»»å‹™ %s: è½‰å¯«å®Œæˆã€‚", task_id)

        # 4. æ›´æ–°æœ€çµ‚çµæœ
        await update_task_status_in_db(
            task_id, "completed", result_text=full_transcript.strip()
        )
        logger.info("ä»»å‹™ %s ç‹€æ…‹æ›´æ–°ç‚º: completed", task_id)

    except Exception as e:
        logger.exception("è™•ç†ä»»å‹™ %s éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤", task_id)
        await update_task_status_in_db(task_id, "failed", error_message=str(e))
        logger.info("ä»»å‹™ %s ç‹€æ…‹æ›´æ–°ç‚º: failed", task_id)


async def update_task_status_in_db(
    task_id: str,
    status: str,
    result_text: Optional[str] = None,
    error_message: Optional[str] = None,
) -> None:
    """åœ¨è³‡æ–™åº«ä¸­æ›´æ–°ä»»å‹™çš„ç‹€æ…‹ã€çµæœæˆ–éŒ¯èª¤è¨Šæ¯ã€‚"""
    try:
        async with aiosqlite.connect(DATABASE_FILE) as db:
            await db.execute(
                """
                UPDATE transcription_tasks
                SET status = ?, result_text = ?, error_message = ?
                WHERE id = ?
                """,
                (status, result_text, error_message, task_id),
            )
            await db.commit()
    except aiosqlite.Error as e:
        logger.exception("æ›´æ–°ä»»å‹™ %s ç‹€æ…‹æ™‚ç™¼ç”Ÿè³‡æ–™åº«éŒ¯èª¤", task_id)


def transcriber_worker_process(
    log_queue: Optional[mp.Queue],
    task_queue: mp.Queue,
    result_queue: Optional[mp.Queue],
    config: Optional[dict[str, Any]],
) -> None:
    """
    è½‰å¯«å·¥äººè¡Œç¨‹çš„ä¸»å‡½æ•¸ã€‚

    å®ƒæœƒæŒçºŒå¾ä»»å‹™ä½‡åˆ—ä¸­ç²å–ä»»å‹™ IDï¼Œä¸¦å‘¼å« `process_single_task` é€²è¡Œè™•ç†ã€‚
    """
    # æ­¤è™•çš„ log_queue, result_queue, config æš«æœªä½¿ç”¨ï¼Œä½†ä¿ç•™ç°½åä»¥ç¬¦åˆæ¶æ§‹
    logger.info("çœŸå¯¦è½‰å¯«å·¥äººè¡Œç¨‹å·²å•Ÿå‹• (PID: %s)", mp.current_process().pid)

    async def main_loop() -> None:
        while True:
            try:
                # å¾ä½‡åˆ—ä¸­ç²å–ä»»å‹™ï¼Œå¦‚æœä½‡åˆ—ç‚ºç©ºï¼Œæœƒé˜»å¡ç­‰å¾…
                task_id = task_queue.get()
                if task_id is None:
                    logger.info("æ”¶åˆ°çµæŸä¿¡è™Ÿï¼Œå·¥äººè¡Œç¨‹å³å°‡é—œé–‰ã€‚")
                    break
                await process_single_task(task_id)
            except (KeyboardInterrupt, SystemExit):
                logger.info("å·¥äººè¡Œç¨‹æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨é—œé–‰ã€‚")
                break
            except Exception:
                logger.exception("å·¥äººåœ¨ä¸»å¾ªç’°ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼Œå°‡åœ¨10ç§’å¾Œé‡è©¦ã€‚")
                await asyncio.sleep(10)

    # ç‚ºäº†èƒ½åœ¨åŒæ­¥å‡½æ•¸ä¸­é‹è¡Œç•°æ­¥å¾ªç’°
    asyncio.run(main_loop())


if __name__ == "__main__":
    # é€™éƒ¨åˆ†ä¿ç•™ç”¨æ–¼ç¨ç«‹æ¸¬è©¦
    # ç‚ºäº†ç›´æ¥é‹è¡Œ, éœ€è¦ä¸€å€‹æ¨¡æ“¬çš„ä½‡åˆ—
    class MockQueue:
        """æ¨¡æ“¬ä½‡åˆ—."""

        def put(self, *args: Any, **kwargs: Any) -> None:
            """æ¨¡æ“¬ put."""
            pass

    transcriber_worker_process(MockQueue(), mp.Queue(), mp.Queue(), {})
"""æ¨¡æ“¬è½‰å¯«å·¥äººæ¨¡çµ„"""
import asyncio
import multiprocessing as mp
import time
from typing import Any, Optional

import aiosqlite

from prometheus.core.logging.log_manager import LogManager
from prometheus.transcriber.core import DATABASE_FILE

# --- æ—¥èªŒè¨˜éŒ„å™¨è¨­å®š ---
log_manager = LogManager(log_file="transcriber_mock_worker.log")
logger = log_manager.get_logger(__name__)


async def simulate_task_processing(task_id: str) -> None:
    """
    æ¨¡æ“¬è™•ç†ä¸€å€‹æŒ‡å®šçš„è½‰å¯«ä»»å‹™ã€‚

    Args:
        task_id (str): è¦æ¨¡æ“¬è™•ç†çš„ä»»å‹™ IDã€‚
    """
    logger.info("æ¨¡æ“¬è™•ç†ä»»å‹™: %s", task_id)

    try:
        # 1. æ¨¡æ“¬æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚º 'processing'
        await update_task_status_in_db(task_id, "processing")
        logger.info("ä»»å‹™ %s: ç‹€æ…‹æ›´æ–°ç‚º -> processing", task_id)
        await asyncio.sleep(0.5)  # æ¨¡æ“¬è™•ç†å»¶é²

        # 2. æ¨¡æ“¬è½‰å¯«å®Œæˆ
        mock_transcript = f"é€™æ˜¯ä»»å‹™ {task_id} çš„æ¨¡æ“¬è½‰å¯«çµæœã€‚"
        logger.info("ä»»å‹™ %s: æ¨¡æ“¬è½‰å¯«å®Œæˆã€‚", task_id)
        await asyncio.sleep(0.5)

        # 3. æ›´æ–°æœ€çµ‚çµæœ
        await update_task_status_in_db(
            task_id, "completed", result_text=mock_transcript
        )
        logger.info("ä»»å‹™ %s: ç‹€æ…‹æ›´æ–°ç‚º -> completed", task_id)

    except Exception as e:
        logger.exception("æ¨¡æ“¬è™•ç†ä»»å‹™ %s éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤", task_id)
        await update_task_status_in_db(task_id, "failed", error_message=str(e))
        logger.info("ä»»å‹™ %s: ç‹€æ…‹æ›´æ–°ç‚º -> failed", task_id)


async def update_task_status_in_db(
    task_id: str,
    status: str,
    result_text: Optional[str] = None,
    error_message: Optional[str] = None,
) -> None:
    """åœ¨è³‡æ–™åº«ä¸­æ›´æ–°ä»»å‹™çš„ç‹€æ…‹ã€çµæœæˆ–éŒ¯èª¤è¨Šæ¯ã€‚"""
    try:
        async with aiosqlite.connect(DATABASE_FILE) as db:
            await db.execute(
                """
                UPDATE transcription_tasks
                SET status = ?, result_text = ?, error_message = ?
                WHERE id = ?
                """,
                (status, result_text, error_message, task_id),
            )
            await db.commit()
    except aiosqlite.Error as e:
        logger.exception("æ›´æ–°ä»»å‹™ %s ç‹€æ…‹æ™‚ç™¼ç”Ÿè³‡æ–™åº«éŒ¯èª¤", task_id)


def mock_worker_process(
    log_queue: Optional[mp.Queue],
    task_queue: mp.Queue,
    result_queue: Optional[mp.Queue],
    config: Optional[dict[str, Any]],
) -> None:
    """
    æ¨¡æ“¬è½‰å¯«å·¥äººè¡Œç¨‹çš„ä¸»å‡½æ•¸ã€‚

    å®ƒæœƒæŒçºŒå¾ä»»å‹™ä½‡åˆ—ä¸­ç²å–ä»»å‹™ IDï¼Œä¸¦å‘¼å« `simulate_task_processing` é€²è¡Œæ¨¡æ“¬è™•ç†ã€‚
    """
    logger.info("æ¨¡æ“¬è½‰å¯«å·¥äººè¡Œç¨‹å·²å•Ÿå‹• (PID: %s)", mp.current_process().pid)

    async def main_loop() -> None:
        while True:
            try:
                task_id = task_queue.get()
                if task_id is None:
                    logger.info("æ”¶åˆ°çµæŸä¿¡è™Ÿï¼Œæ¨¡æ“¬å·¥äººè¡Œç¨‹å³å°‡é—œé–‰ã€‚")
                    break
                await simulate_task_processing(task_id)
            except (KeyboardInterrupt, SystemExit):
                logger.info("æ¨¡æ“¬å·¥äººè¡Œç¨‹æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨é—œé–‰ã€‚")
                break
            except Exception:
                logger.exception("æ¨¡æ“¬å·¥äººåœ¨ä¸»å¾ªç’°ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼Œå°‡åœ¨10ç§’å¾Œé‡è©¦ã€‚")
                await asyncio.sleep(10)

    asyncio.run(main_loop())
