# -*- coding: utf-8 -*-
"""
================================================================================
æ•´åˆå¾Œçš„æ¥­å‹™é‚è¼¯
================================================================================

æœ¬æª”æ¡ˆæ•´åˆäº†ã€æ™®ç¾…ç±³ä¿®æ–¯ä¹‹ç«ã€‘é‡‘èæ•¸æ“šæ¡†æ¶èˆ‡ã€é³³å‡°å°ˆæ¡ˆã€‘éŒ„éŸ³è½‰å¯«æœå‹™çš„æ ¸å¿ƒæ¥­å‹™é‚è¼¯ã€‚

--------------------------------------------------------------------------------
ç¬¬ä¸€éƒ¨åˆ†ï¼šã€æ™®ç¾…ç±³ä¿®æ–¯ä¹‹ç«ã€‘æ ¸å¿ƒé‚è¼¯
--------------------------------------------------------------------------------
é€™éƒ¨åˆ†åŒ…å«äº†é‡‘èæ•¸æ“šçš„ç²å–ã€è™•ç†ã€å› å­è¨ˆç®—èˆ‡æ¨¡æ“¬ã€‚
"""

# ... (å¾ 0709_wolf_88/src/prometheus/ ä¸‹çš„ç›¸é—œæª”æ¡ˆè¤‡è£½æ ¸å¿ƒé‚è¼¯) ...

"""
--------------------------------------------------------------------------------
ç¬¬äºŒéƒ¨åˆ†ï¼šã€é³³å‡°å°ˆæ¡ˆã€‘æ ¸å¿ƒé‚è¼¯
--------------------------------------------------------------------------------
é€™éƒ¨åˆ†åŒ…å«äº†éŒ„éŸ³è½‰å¯«æœå‹™çš„ APIã€å·¥äººé€²ç¨‹å’Œä»»å‹™ç®¡ç†ã€‚
"""

# ... (å¾ 0709_wolf_88/src/prometheus/transcriber/ ä¸‹çš„ç›¸é—œæª”æ¡ˆè¤‡è£½æ ¸å¿ƒé‚è¼¯) ...
import typer
from prometheus.entrypoints.ai_analyst_app import ai_analyst_job
from prometheus.entrypoints.query_gateway import run_dashboard_service
from prometheus.core.logging.log_manager import LogManager

app = typer.Typer()
# ç”±æ–¼ LogManager ä¸å†æ˜¯å–®ä¾‹ï¼Œæˆ‘å€‘ç‚º CLI çš„ä¸»é€²ç¨‹å‰µå»ºä¸€å€‹å¸¸è¦çš„ logger
log_manager = LogManager(log_file="prometheus_cli.log")
logger = log_manager.get_logger("Conductor")

@app.command(name="analyze")
def cli_analyze():
    """
    å•Ÿå‹• AI åˆ†æå¸«å ±å‘Šç”Ÿæˆå™¨ã€‚
    """
    logger.info("æ­£åœ¨å•Ÿå‹• AI åˆ†æå¸«...")
    ai_analyst_job()
    logger.info("AI åˆ†æå¸«å·¥ä½œå®Œæˆã€‚")


import subprocess
import sys
import os

@app.command(name="dashboard")
def cli_dashboard(
    host: str = typer.Option("127.0.0.1", help="ç¶å®šä¸»æ©Ÿ"),
    port: int = typer.Option(8000, help="ç¶å®šåŸ è™Ÿ"),
):
    """å•Ÿå‹•ç¶²é å„€è¡¨æ¿ã€‚"""
    logger.info(f"æº–å‚™åœ¨ http://{host}:{port} å•Ÿå‹•å„€è¡¨æ¿...")
    run_dashboard_service(None, host, port)

data_app = typer.Typer()
app.add_typer(data_app, name="data")

@data_app.command("create-dummy")
def create_dummy():
    """
    å»ºç«‹ä¸€å€‹ç”¨æ–¼æ¸¬è©¦çš„è™›æ§‹ OHLCV CSV æª”æ¡ˆã€‚
    """
    from pathlib import Path
    import numpy as np
    import pandas as pd

    DATA_DIR = Path("data")
    DATA_DIR.mkdir(exist_ok=True)
    file_path = DATA_DIR / "ohlcv_data.csv"

    date_range = pd.to_datetime(
        pd.date_range(start="2022-01-01", periods=1000, freq="D")
    )

    open_prices = np.random.uniform(90, 110, size=1000)

    data = {
        "Date": date_range,
        "Open": open_prices,
        "High": open_prices + np.random.uniform(0, 5, size=1000),
        "Low": open_prices - np.random.uniform(0, 5, size=1000),
        "Close": open_prices + np.random.uniform(-2, 2, size=1000),
        "Volume": np.random.randint(100000, 500000, size=1000),
    }

    df = pd.DataFrame(data)

    df.to_csv(file_path, index=False)
    logger.info(f"å·²æˆåŠŸå»ºç«‹è™›æ§‹æ•¸æ“šæª”æ¡ˆæ–¼: {file_path}")


results_app = typer.Typer()
app.add_typer(results_app, name="results")

@results_app.command("clear")
def clear_results():
    """
    æ¸…é™¤æ‰€æœ‰ç”Ÿæˆçš„çµæœã€ä½‡åˆ—ã€æ—¥èªŒå’Œæª¢æŸ¥é»ã€‚
    """
    import os
    import shutil

    logger.info("é–‹å§‹æ¸…é™¤æ‰€æœ‰åŸ·è¡Œæ•¸æ“š...")

    RESULTS_DB_PATH = "output/results.sqlite"
    QUEUE_DIR = "data/queues"
    LOG_DIR = "data/logs"
    CHECKPOINT_DIR = "data/checkpoints"
    REPORTS_DIR = "data/reports"

    def remove_path(path_str, is_dir=False):
        if is_dir:
            if os.path.isdir(path_str):
                shutil.rmtree(path_str)
                logger.info(f"å·²åˆªé™¤ä¸¦æ¸…ç©ºç›®éŒ„: {path_str}")
        else:
            if os.path.exists(path_str):
                os.remove(path_str)
                logger.info(f"å·²åˆªé™¤æª”æ¡ˆ: {path_str}")

    try:
        remove_path(RESULTS_DB_PATH, is_dir=False)
        remove_path(QUEUE_DIR, is_dir=True)
        remove_path(LOG_DIR, is_dir=True)
        remove_path(CHECKPOINT_DIR, is_dir=True)
        remove_path(REPORTS_DIR, is_dir=True)

        # é‡å»ºç©ºç›®éŒ„
        os.makedirs(QUEUE_DIR, exist_ok=True)
        os.makedirs(LOG_DIR, exist_ok=True)
        os.makedirs(CHECKPOINT_DIR, exist_ok=True)
        os.makedirs(REPORTS_DIR, exist_ok=True)

        logger.info("æ¸…é™¤ç¨‹åºå®Œæˆã€‚")
    except Exception as e:
        logger.error(f"æ¸…é™¤éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)


@results_app.command("show")
def show_results():
    """
    å¾ SQLite è³‡æ–™åº«æŸ¥è©¢ä¸¦é¡¯ç¤ºå›æ¸¬çµæœã€‚
    """
    import sqlite3
    import pandas as pd

    logger.info("æ­£åœ¨å¾ SQLite è³‡æ–™åº«æŸ¥è©¢çµæœ...")
    DB_PATH = "output/results.sqlite"
    TABLE_NAME = "backtest_results"
    try:
        conn = sqlite3.connect(DB_PATH)
        df = pd.read_sql_query(f"SELECT * FROM {TABLE_NAME}", conn)
        conn.close()

        if df.empty:
            logger.warning("è³‡æ–™åº«ä¸­å°šç„¡ä»»ä½•çµæœã€‚")
        else:
            logger.info("æŸ¥è©¢å®Œæˆã€‚")
            # ä½¿ç”¨ä¸€å€‹ logger å‘¼å«ä¾†é¡¯ç¤ºæ•´å€‹ DataFrame
            logger.info(f"\n--- å›æ¸¬çµæœ ---\n{df.to_string()}\n----------------")

    except Exception as e:
        logger.error(f"æŸ¥è©¢çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)


@results_app.command("generate-report")
def generate_report(
    xml_path: str = typer.Option("output/reports/report.xml", help="JUnit XML å ±å‘Šçš„è·¯å¾‘"),
    md_path: str = typer.Option("TEST_REPORT.md", help="è¦ç”Ÿæˆçš„ Markdown å ±å‘Šçš„è·¯å¾‘"),
):
    """
    å¾ JUnit XML æª”æ¡ˆç”¢ç”Ÿ Markdown å ±å‘Šã€‚
    """
    import xml.etree.ElementTree as ET
    from datetime import datetime

    logger.info(f"AI å ±å‘Šç”Ÿæˆå™¨å•Ÿå‹•ï¼Œæ­£åœ¨è®€å–åŸå§‹æ•¸æ“š: {xml_path}")
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
        suite = root.find("testsuite")

        # æå–æ ¸å¿ƒæ•¸æ“š
        total = int(suite.get("tests", 0))
        failures = int(suite.get("failures", 0))
        errors = int(suite.get("errors", 0))
        skipped = int(suite.get("skipped", 0))
        exec_time = float(suite.get("time", 0))
        passed = total - failures - errors - skipped

        # é–‹å§‹æ§‹å»º Markdown å ±å‘Š
        report_content = []
        report_content.append("# **ã€æ™®ç¾…ç±³ä¿®æ–¯ä¹‹ç«ã€‘ç³»çµ±æ¸¬è©¦ä½œæˆ°å ±å‘Š**")
        report_content.append(
            f"> å ±å‘Šç”Ÿæˆæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        )

        # ç¸½çµå€å¡Š
        report_content.append("## **ä¸€ã€ æˆ°æ³ç¸½è¦½**")
        if failures == 0 and errors == 0:
            report_content.append(
                "> **çµè«–ï¼š<font color='green'>ä»»å‹™æˆåŠŸ (SUCCESS)</font>** - æ‰€æœ‰å“è³ªé–˜é–€å‡å·²é€šéã€‚ç³»çµ±æˆ°å‚™ç‹€æ…‹è‰¯å¥½ã€‚"
            )
        else:
            report_content.append(
                "> **çµè«–ï¼š<font color='red'>ä»»å‹™å¤±æ•— (FAILURE)</font>** - ç™¼ç¾é—œéµæ€§éŒ¯èª¤ã€‚ç³»çµ±å­˜åœ¨é¢¨éšªï¼Œéœ€ç«‹å³å¯©æŸ¥ã€‚"
            )

        summary_table = [
            "| æŒ‡æ¨™ (Metric) | æ•¸é‡ (Count) |",
            "|:---|:---:|",
            f"| âœ… **æ¸¬è©¦é€šé (Passed)** | {passed} |",
            f"| âŒ **æ¸¬è©¦å¤±æ•— (Failed)** | {failures} |",
            f"| ğŸ”¥ **åŸ·è¡ŒéŒ¯èª¤ (Errors)** | {errors} |",
            f"| ğŸš§ **æ¸¬è©¦è·³é (Skipped)** | {skipped} |",
            f"| â±ï¸ **ç¸½åŸ·è¡Œæ™‚é–“ (Time)** | {exec_time:.2f} ç§’ |",
            f"| ğŸ§® **ç¸½åŸ·è¡Œæ•¸é‡ (Total)** | {total} |",
        ]
        report_content.append("\n".join(summary_table))

        # å¤±æ•—èˆ‡éŒ¯èª¤è©³æƒ…
        if failures > 0 or errors > 0:
            report_content.append("\n## **äºŒã€ å¤±æ•—èˆ‡éŒ¯èª¤è©³æƒ…**")
            count = 1
            for testcase in suite.findall("testcase"):
                failure = testcase.find("failure")
                error = testcase.find("error")

                detail = failure if failure is not None else error

                if detail is not None:
                    test_name = testcase.get("name", "æœªçŸ¥æ¸¬è©¦")
                    class_name = testcase.get("classname", "æœªçŸ¥é¡åˆ¥")
                    error_type = detail.tag.capitalize()  # "failure" -> "Failure"
                    message = detail.get("message", "ç„¡è¨Šæ¯").splitlines()[0]

                    report_content.append(f"\n### {count}. {error_type}: {message}")
                    report_content.append(
                        f"- **æ¸¬è©¦ä½ç½®:** `{class_name}.{test_name}`"
                    )
                    report_content.append("- **è©³ç´°å †ç–Šè¿½è¹¤:**")
                    # æª¢æŸ¥ detail.text æ˜¯å¦ç‚º None
                    stack_trace = (
                        detail.text.strip() if detail.text else "ç„¡å †ç–Šè¿½è¹¤è³‡è¨Šã€‚"
                    )
                    report_content.append(f"```\n{stack_trace}\n```")
                    count += 1

        # å¯«å…¥æª”æ¡ˆ
        with open(md_path, "w", encoding="utf-8") as f:
            f.write("\n".join(report_content))

        logger.info(f"ä½œæˆ°å ±å‘Šå·²æˆåŠŸç”Ÿæˆè‡³: {md_path}")

    except FileNotFoundError:
        logger.error(f"æ‰¾ä¸åˆ°åŸå§‹æ•¸æ“šæª”æ¡ˆ: {xml_path}")
    except ET.ParseError:
        logger.error(f"åŸå§‹æ•¸æ“šæª”æ¡ˆæ ¼å¼éŒ¯èª¤: {xml_path}")
    except Exception as e:
        logger.error(f"ç”Ÿæˆå ±å‘Šæ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")


@results_app.command("add-tasks")
def add_tasks(
    num_tasks: int = typer.Option(10, help="è¦æ·»åŠ çš„ä»»å‹™æ•¸é‡"),
):
    """
    å‘ä»»å‹™ä½‡åˆ—ä¸­æ·»åŠ æŒ‡å®šæ•¸é‡çš„éš¨æ©Ÿå›æ¸¬ä»»å‹™ã€‚
    """
    import random
    import uuid
    from prometheus.core.context import AppContext

    with AppContext() as ctx:
        logger.info(f"æ­£åœ¨ç”Ÿæˆ {num_tasks} å€‹å›æ¸¬ä»»å‹™...")
        batch_id = str(uuid.uuid4())
        for i in range(num_tasks):
            # ä»»å‹™ç¾åœ¨æ˜¯ä¸€å€‹å­—å…¸
            task = {
                "task_id": str(uuid.uuid4()),
                "type": "backtest",
                "strategy": "SMA_Crossover",
                "symbol": random.choice(["BTC/USDT", "ETH/USDT", "XRP/USDT"]),
                "params": {"fast": random.randint(5, 15), "slow": random.randint(20, 40)},
                "batch_id": batch_id,
            }
            ctx.queue.put(task)
            logger.debug(f"å·²å°‡ä»»å‹™ {i+1}/{num_tasks} ({task['strategy']}) æ·»åŠ åˆ°ä½‡åˆ—ã€‚")
        logger.info(f"æˆåŠŸå°‡ {num_tasks} å€‹ä»»å‹™æ·»åŠ åˆ°ä½‡åˆ—ã€‚")


pipelines_app = typer.Typer()
app.add_typer(pipelines_app, name="pipelines")

@pipelines_app.command("run-downloader")
def run_downloader(
    start_date: str = typer.Option(..., help="ä¸‹è¼‰é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)"),
    end_date: str = typer.Option(..., help="ä¸‹è¼‰çµæŸæ—¥æœŸ (YYYY-MM-DD)"),
    output_dir: str = typer.Option("data/downloads", help="æª”æ¡ˆå„²å­˜ç›®éŒ„"),
    max_workers: int = typer.Option(16, help="æœ€å¤§åŒæ™‚ä¸‹è¼‰ä»»å‹™æ•¸"),
):
    """
    TAIFEX è‡ªå‹•åŒ–æ•¸æ“šæ¡é›†å™¨ v1.0
    """
    import os
    import random
    import time
    from collections import Counter
    from concurrent.futures import ThreadPoolExecutor, as_completed
    from datetime import datetime, timedelta
    import requests
    from prometheus.core.config import config

    logger.info("--- å•Ÿå‹•æ•¸æ“šæ¡é›†ä»»å‹™ ---")
    logger.info(f"æ™‚é–“ç¯„åœ: {start_date} åˆ° {end_date}")
    logger.info(f"è¼¸å‡ºç›®éŒ„: {output_dir}")

    tasks = []
    start_dt = datetime.strptime(start_date, "%Y-%m-%d")
    end_dt = datetime.strptime(end_date, "%Y-%m-%d")
    date_range = [
        start_dt + timedelta(days=x) for x in range((end_dt - start_dt).days + 1)
    ]

    base_url = config.get("data_acquisition.taifex.base_url")
    for current_date in date_range:
        date_str = current_date.strftime("%Y_%m_%d")
        # ç¯„ä¾‹ï¼šåƒ…ä¸‹è¼‰æœŸè²¨é€ç­†è³‡æ–™
        tasks.append(
            {
                "url": f"{base_url}/file/taifex/Dailydownload/DailydownloadCSV/Daily_{date_str}.zip",
                "file_name": f"Daily_{date_str}.zip",
                "min_delay": 0.2,
                "max_delay": 1.0,
            }
        )

    results_counter = Counter()
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        with requests.Session() as session:
            future_to_task = {
                executor.submit(execute_download, session, task, output_dir): task
                for task in tasks
            }
            for future in as_completed(future_to_task):
                try:
                    status, message = future.result()
                    results_counter[status] += 1
                    logger.info(f"[{status.upper()}] {message}")
                except Exception as exc:
                    logger.info(f"[CRITICAL] ä»»å‹™åŸ·è¡Œç•°å¸¸: {exc}")

    logger.info("\n--- æ¡é›†ä»»å‹™ç¸½çµ ---")
    for status, count in results_counter.items():
        logger.info(f"  {status}: {count} å€‹")


def execute_download(session, task_info, output_dir):
    """åŸ·è¡Œå–®ä¸€æª”æ¡ˆä¸‹è¼‰ä»»å‹™ï¼ŒåŒ…å«é‡è©¦èˆ‡éŒ¯èª¤è™•ç†ã€‚"""
    import os
    import random
    import time
    import requests
    from prometheus.core.config import config

    file_path = os.path.join(output_dir, task_info["file_name"])
    if os.path.exists(file_path):
        return "exists", f"æª”æ¡ˆå·²å­˜åœ¨: {task_info['file_name']}"

    time.sleep(random.uniform(task_info["min_delay"], task_info["max_delay"]))

    user_agents = config.get("data_acquisition.taifex.user_agents")
    base_url = config.get("data_acquisition.taifex.base_url")

    for attempt in range(3):  # é‡è©¦3æ¬¡
        try:
            headers = {
                "User-Agent": random.choice(user_agents),
                "Referer": task_info.get("referer", base_url),
            }
            response = (
                session.post(
                    task_info["url"],
                    data=task_info.get("payload", {}),
                    headers=headers,
                    timeout=120,
                )
                if task_info.get("payload")
                else session.get(task_info["url"], headers=headers, timeout=120)
            )

            if (
                response.status_code == 200
                and len(response.content) > 100
                and "æŸ¥ç„¡è³‡æ–™" not in response.text
            ):
                os.makedirs(output_dir, exist_ok=True)
                with open(file_path, "wb") as f:
                    f.write(response.content)
                return "success", f"æˆåŠŸä¸‹è¼‰: {task_info['file_name']}"
            elif response.status_code == 404:
                return "not_found", f"404 Not Found: {task_info['file_name']}"
            else:
                return (
                    "error",
                    f"ä¼ºæœå™¨éŒ¯èª¤ {response.status_code}: {task_info['file_name']}",
                )

        except requests.exceptions.RequestException as e:
            if attempt == 2:
                return "error", f"ç¶²è·¯è«‹æ±‚å¤±æ•—: {e}"
            time.sleep(5 * (attempt + 1))

    return "error", f"é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸: {task_info['file_name']}"


@pipelines_app.command("run-explorer")
def run_explorer(
    input_dir: str = typer.Option("data/downloads", help="æƒæçš„åŸå§‹æª”æ¡ˆç›®éŒ„"),
    db_path: str = typer.Option("data/metadata/schema_registry.db", help="æ ¼å¼è¨»å†Šè¡¨è³‡æ–™åº«è·¯å¾‘"),
):
    """
    TAIFEX æ ¼å¼æ¢å‹˜èˆ‡è¨»å†Šå™¨ v1.0
    """
    import hashlib
    import os
    from prometheus.core.db.schema_registry import SchemaRegistry
    from prometheus.core.utils.helpers import (
        prospect_file_content,
        read_file_content,
    )

    registry = SchemaRegistry(db_path)
    logger.info(f"é–‹å§‹æƒæç›®éŒ„: {input_dir}")
    new_formats = 0
    updated_formats = 0

    for filename in os.listdir(input_dir):
        file_path = os.path.join(input_dir, filename)
        if not os.path.isfile(file_path):
            continue

        try:
            file_bytes_content = read_file_content(file_path)
            if file_bytes_content is None:
                continue

            result = prospect_file_content(file_bytes_content)

            if result["status"] == "success":
                fingerprint = get_header_fingerprint(result["header"])
                status = registry.add_or_update_schema(fingerprint, result["header"], result["encoding"], filename)
                if status == "new":
                    new_formats += 1
                else:
                    updated_formats += 1

        except Exception as e:
            logger.error(f"è™•ç†æª”æ¡ˆ {filename} å¤±æ•—: {e}", exc_info=True)

    registry.close()
    logger.info("--- æ ¼å¼æ¢å‹˜ç¸½çµ ---")
    logger.info(f"ç™¼ç¾æ–°æ ¼å¼: {new_formats} ç¨®")
    logger.info(f"æ›´æ–°ç¾æœ‰æ ¼å¼è¨ˆæ•¸: {updated_formats} æ¬¡")


def get_header_fingerprint(header_line: str) -> str:
    """å°æ¨™æº–åŒ–å¾Œçš„æ¨™é ­è¨ˆç®—æŒ‡ç´‹ã€‚"""
    import hashlib
    normalized_header = "".join(header_line.lower().split()).replace('"', "")
    return hashlib.sha256(normalized_header.encode("utf-8")).hexdigest()


@pipelines_app.command("run-elt")
def run_elt(
    input_dir: str = typer.Option("data/downloads", help="ä¸‹è¼‰æª”æ¡ˆçš„ä¾†æºç›®éŒ„ (ä¾› Loader ä½¿ç”¨)"),
    raw_db_path: str = typer.Option("data/raw_warehouse/raw_taifex.duckdb", help="åŸå§‹æ•¸æ“šè‰™è³‡æ–™åº«è·¯å¾‘"),
    schema_db_path: str = typer.Option("data/metadata/schema_registry.db", help="æ ¼å¼è¨»å†Šè¡¨è³‡æ–™åº«è·¯å¾‘"),
    analytics_db_path: str = typer.Option("data/analytics_warehouse/analytics_taifex.duckdb", help="åˆ†ææ•¸æ“šåº«è·¯å¾‘"),
):
    """
    TAIFEX ELT åŠ å·¥ç®¡ç·š v1.0
    """
    import os
    from prometheus.core.db.data_warehouse import AnalyticsDataWarehouse, RawDataWarehouse
    from prometheus.core.db.schema_registry import SchemaRegistry

    # Ensure parent directories for database files exist
    os.makedirs(os.path.dirname(raw_db_path), exist_ok=True)
    os.makedirs(os.path.dirname(schema_db_path), exist_ok=True)
    os.makedirs(os.path.dirname(analytics_db_path), exist_ok=True)

    run_loader(input_dir, raw_db_path, schema_db_path)
    run_transformer(raw_db_path, schema_db_path, analytics_db_path)


def run_loader(input_dir, raw_db_path, schema_db_path):
    import os
    from prometheus.core.db.data_warehouse import RawDataWarehouse
    from prometheus.core.db.schema_registry import SchemaRegistry
    from prometheus.core.utils.helpers import (
        prospect_file_content,
        read_file_content,
    )

    logger.info("--- [éšæ®µ 2] åŸ·è¡Œ Loader ---")
    raw_wh = RawDataWarehouse(raw_db_path)
    schema_registry = SchemaRegistry(schema_db_path)

    known_fingerprints = schema_registry.get_known_fingerprints()
    if not known_fingerprints:
        logger.info("Loader: No known fingerprints loaded from schema registry. Only files matching these will be processed.")

    files_loaded = 0
    if not os.path.exists(input_dir):
        logger.warning(f"Loader input directory {input_dir} does not exist. Skipping loading.")
        raw_wh.close()
        schema_registry.close()
        return

    for filename in os.listdir(input_dir):
        file_path = os.path.join(input_dir, filename)

        if not os.path.isfile(file_path):
            continue

        if raw_wh.is_file_processed(file_path):
            logger.debug(f"Loader: File {filename} already in raw_import_log. Skipping.")
            continue

        try:
            file_bytes_content = read_file_content(file_path)
            if file_bytes_content is None:
                continue

            result = prospect_file_content(file_bytes_content)
            if result["status"] == "success":
                fingerprint = get_header_fingerprint(result["header"])
                if fingerprint in known_fingerprints:
                    raw_wh.log_processed_file(file_path, file_bytes_content, fingerprint)
                    files_loaded += 1
                    logger.info(f"Loader: Loaded {filename} (fingerprint: {fingerprint[:8]}...) as it's a known schema.")
                else:
                    logger.info(f"Loader: Skipped {filename} (fingerprint: {fingerprint[:8]}...) as its schema is not in the registry.")
            else:
                logger.warning(f"Loader: Skipped file {filename} due to content prospecting failure: {result.get('error', 'Unknown error')}")

        except Exception as e:
            logger.error(f"Loader è™•ç† {filename} å¤±æ•—: {e}", exc_info=True)

    raw_wh.close()
    schema_registry.close()
    logger.info(f"Loader å®Œæˆï¼Œæ–°è¼‰å…¥ {files_loaded} å€‹æª”æ¡ˆã€‚")


def run_transformer(raw_db_path, schema_db_path, analytics_db_path):
    import io
    import pandas as pd
    from prometheus.core.db.data_warehouse import AnalyticsDataWarehouse, RawDataWarehouse
    from prometheus.core.db.schema_registry import SchemaRegistry

    logger.info("--- [éšæ®µ 3] åŸ·è¡Œ Transformer ---")
    schema_registry = SchemaRegistry(schema_db_path)
    raw_wh = RawDataWarehouse(raw_db_path)
    analytics_wh = AnalyticsDataWarehouse(analytics_db_path)

    schema_map = schema_registry.get_all_schemas()
    if not schema_map:
        logger.warning("Transformer: æ ¼å¼è¨»å†Šè¡¨ç‚ºç©ºæˆ–è®€å–å¤±æ•—ï¼ŒTransformer ç„¡æ³•åŸ·è¡Œæœ‰æ•ˆè½‰æ›ã€‚")
        schema_registry.close()
        raw_wh.close()
        analytics_wh.close()
        return

    daily_futures_header_str = "äº¤æ˜“æ—¥æœŸ,å¥‘ç´„ä»£ç¢¼,åˆ°æœŸæœˆä»½(é€±åˆ¥),é–‹ç›¤åƒ¹,æœ€é«˜åƒ¹,æœ€ä½åƒ¹,æ”¶ç›¤åƒ¹,æˆäº¤é‡"
    target_daily_futures_fingerprint = get_header_fingerprint(daily_futures_header_str)

    if target_daily_futures_fingerprint not in schema_map:
        logger.warning(f"Transformer: Did not find fingerprint for daily_futures_header '{daily_futures_header_str}' in schema_map. Cannot process daily_futures.")
    else:
        logger.info(f"Transformer: Target fingerprint for daily_futures is {target_daily_futures_fingerprint[:8]}...")

    analytics_wh.create_daily_futures_table()

    records = raw_wh.execute_query("SELECT content_blob, format_fingerprint FROM raw_import_log").fetchall()
    transformed_count = 0
    for blob, fingerprint in records:
        if fingerprint != target_daily_futures_fingerprint:
            continue

        if fingerprint not in schema_map:
            continue

        header_str_from_registry, encoding = schema_map[fingerprint]
        try:
            df = pd.read_csv(io.BytesIO(blob), encoding=encoding, thousands=",", header=0, on_bad_lines="skip")
            df.columns = [str(col).strip().replace('"', "") for col in df.columns]

            target_columns_canonical = [
                "äº¤æ˜“æ—¥æœŸ", "å¥‘ç´„ä»£ç¢¼", "åˆ°æœŸæœˆä»½(é€±åˆ¥)", "é–‹ç›¤åƒ¹",
                "æœ€é«˜åƒ¹", "æœ€ä½åƒ¹", "æ”¶ç›¤åƒ¹", "æˆäº¤é‡",
            ]

            df_to_load = pd.DataFrame()
            for canonical_col_name in target_columns_canonical:
                if canonical_col_name in df.columns:
                    df_to_load[canonical_col_name] = df[canonical_col_name]
                else:
                    df_to_load[canonical_col_name] = None

            if not df_to_load.empty:
                analytics_wh.insert_daily_futures(df_to_load)
                transformed_count += 1

        except pd.errors.EmptyDataError:
            logger.warning(f"Transformer: No data or columns found in CSV for fingerprint {fingerprint[:8]}...")
        except Exception as e:
            logger.error(f"Transformer è™•ç†æŒ‡ç´‹ {fingerprint[:8]}... çš„è³‡æ–™æ™‚å¤±æ•—: {e}", exc_info=True)

    raw_wh.close()
    analytics_wh.close()
    schema_registry.close()
    logger.info(f"Transformer å®Œæˆï¼ŒæˆåŠŸè½‰æ› {transformed_count} ç­†è¨˜éŒ„ã€‚")


@pipelines_app.command("run-stock-factors")
def run_stock_factors():
    """
    åŸ·è¡Œç¬¬å››è™Ÿç”Ÿç”¢ç·šï¼šè‚¡ç¥¨å› å­ç”Ÿæˆã€‚
    """
    from prometheus.pipelines.p4_stock_factor_generation import main as p4_main
    logger.info("--- å•Ÿå‹• P4ï¼šè‚¡ç¥¨å› å­ç”Ÿæˆç®¡ç·š ---")
    p4_main()
    logger.info("--- P4ï¼šè‚¡ç¥¨å› å­ç”Ÿæˆç®¡ç·šåŸ·è¡Œå®Œç•¢ ---")


@pipelines_app.command("run-crypto-factors")
def run_crypto_factors():
    """
    åŸ·è¡Œç¬¬äº”è™Ÿç”Ÿç”¢ç·šï¼šåŠ å¯†è²¨å¹£å› å­ç”Ÿæˆã€‚
    """
    from prometheus.pipelines.p5_crypto_factor_generation import main as p5_main
    logger.info("--- å•Ÿå‹• P5ï¼šåŠ å¯†è²¨å¹£å› å­ç”Ÿæˆç®¡ç·š ---")
    p5_main()
    logger.info("--- P5ï¼šåŠ å¯†è²¨å¹£å› å­ç”Ÿæˆç®¡ç·šåŸ·è¡Œå®Œç•¢ ---")


@app.command(name="build-feature-store")
def build_feature_store():
    """
    ã€ä½œæˆ°æŒ‡ä»¤ã€‘çµ±ä¸€æ•¸æ“šå€‰å„²é‡æ§‹ï¼šå»ºé€ ç‰¹å¾µå€‰å„²ã€‚
    """
    import asyncio
    from prometheus.core.db.db_manager import DBManager
    # from prometheus.pipelines.p1_factor_generation import p1_factor_generation_pipeline
    # from prometheus.pipelines.p2_index_factor_generation import p2_index_factor_pipeline
    # from prometheus.pipelines.p3_bond_factor_generation import p3_bond_factor_pipeline
    from prometheus.pipelines.p4_stock_factor_generation import main as p4_main
    from prometheus.pipelines.p5_crypto_factor_generation import main as p5_main

    logger.info("--- å•Ÿå‹•çµ±ä¸€æ•¸æ“šå€‰å„²å»ºæ§‹æµç¨‹ ---")
    db_manager = DBManager()

    # --- P1, P2, P3 (å·²åœç”¨) ---
    # æ ¹æ“šç›®å‰çš„æª”æ¡ˆçµæ§‹ï¼Œé€™äº›ç®¡ç·šä¸å­˜åœ¨ï¼Œæš«æ™‚è¨»è§£ä»¥ç¢ºä¿å‘½ä»¤å¯åŸ·è¡Œ
    # logger.info("åŸ·è¡Œ P1 é€šç”¨å› å­ç”Ÿæˆ...")
    # p1_df = asyncio.run(p1_factor_generation_pipeline.run())
    # db_manager.save_data(p1_df, 'factors')
    # logger.info("P1 é€šç”¨å› å­æ•¸æ“šå·²åˆä½µã€‚")

    # logger.info("åŸ·è¡Œ P2 æŒ‡æ•¸å› å­ç”Ÿæˆ...")
    # p2_df = asyncio.run(p2_index_factor_pipeline.run())
    # db_manager.save_data(p2_df, 'factors')
    # logger.info("P2 æŒ‡æ•¸å› å­æ•¸æ“šå·²åˆä½µã€‚")

    # logger.info("åŸ·è¡Œ P3 å‚µåˆ¸å› å­ç”Ÿæˆ...")
    # p3_df = asyncio.run(p3_bond_factor_pipeline.run())
    # db_manager.save_data(p3_df, 'factors')
    # logger.info("P3 å‚µåˆ¸å› å­æ•¸æ“šå·²åˆä½µã€‚")

    # --- P4, P5 ---
    # é€™äº›ç®¡ç·šçš„ main å‡½æ•¸å…§éƒ¨ç›´æ¥èª¿ç”¨ DBManagerï¼Œæˆ‘å€‘æš«æ™‚ä¿æŒé€™ç¨®æ–¹å¼
    # æœªä¾†å¯ä»¥é€²ä¸€æ­¥é‡æ§‹ï¼Œä½†ç›®å‰è¶³ä»¥æ»¿è¶³ä½œæˆ°ç›®æ¨™
    logger.info("åŸ·è¡Œ P4 è‚¡ç¥¨å› å­ç”Ÿæˆ...")
    p4_main()
    logger.info("P4 è‚¡ç¥¨å› å­æ•¸æ“šå·²åˆä½µã€‚")

    logger.info("åŸ·è¡Œ P5 åŠ å¯†è²¨å¹£å› å­ç”Ÿæˆ...")
    p5_main()
    logger.info("P5 åŠ å¯†è²¨å¹£å› å­æ•¸æ“šå·²åˆä½µã€‚")

    logger.info("--- çµ±ä¸€æ•¸æ“šå€‰å„²å»ºæ§‹æµç¨‹å®Œç•¢ ---")


@pipelines_app.command("run-simulation-training")
def run_simulation_training(
    target_factor: str = typer.Option(..., help="è¦æ¨¡æ“¬çš„ç›®æ¨™å› å­åç¨±"),
):
    """
    åŸ·è¡Œç¬¬å…­è™Ÿç”Ÿç”¢ç·šï¼šå› å­ä»£ç†æ¨¡æ“¬æ¨¡å‹è¨“ç·´ã€‚
    """
    from prometheus.pipelines.p6_simulation_training import run_main as p6_run_main
    logger.info(f"--- å•Ÿå‹• P6ï¼šå› å­ä»£ç†æ¨¡æ“¬æ¨¡å‹è¨“ç·´ç®¡ç·šï¼Œç›®æ¨™ç‚º {target_factor} ---")
    p6_run_main(target_factor=target_factor)
    logger.info(f"--- P6ï¼šå› å­ä»£ç†æ¨¡æ“¬æ¨¡å‹è¨“ç·´ç®¡ç·šåŸ·è¡Œå®Œç•¢ ---")


@pipelines_app.command("run")
def run_pipeline(
    name: str = typer.Option(..., help="è¦åŸ·è¡Œçš„ç®¡ç·šåç¨±"),
    ticker: str = typer.Option(None, "--ticker", "-t", help="è¦è™•ç†çš„è³‡ç”¢ä»£è™Ÿ")
):
    """
    åŸ·è¡ŒæŒ‡å®šçš„æ•¸æ“šç®¡ç·šã€‚
    """
    import asyncio
    pipeline_context = {"ticker": ticker} if ticker else {}
    logger.info(f"--- å•Ÿå‹• {name} ç®¡ç·šï¼Œä¸Šä¸‹æ–‡: {pipeline_context} ---")

    if name == "p1_factor_generation":
        from prometheus.pipelines.p1_factor_generation import p1_factor_generation_pipeline
        asyncio.run(p1_factor_generation_pipeline.run(context=pipeline_context))
    elif name == "p2_index_factor_generation":
        from prometheus.pipelines.p2_index_factor_generation import p2_index_factor_pipeline
        asyncio.run(p2_index_factor_pipeline.run(context=pipeline_context))
    elif name == "p3_bond_factor_generation":
        from prometheus.pipelines.p3_bond_factor_generation import p3_bond_factor_pipeline
        asyncio.run(p3_bond_factor_pipeline.run(context=pipeline_context))
    else:
        logger.error(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°åç‚º '{name}' çš„ç®¡ç·šã€‚")
        raise typer.Exit(code=1)

    logger.info(f"--- {name} ç®¡ç·šåŸ·è¡Œå®Œç•¢ ---")


@pipelines_app.command("run-backfill")
def run_backfill_cli(
    start_date: str = typer.Option(..., help="å›å¡«é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)"),
    end_date: str = typer.Option(..., help="å›å¡«çµæŸæ—¥æœŸ (YYYY-MM-DD)"),
):
    """
    åŸ·è¡Œæ­·å²æ•¸æ“šå›å¡«ç®¡ç·šã€‚
    """
    import pandas as pd
    from prometheus.core.analysis.data_engine import DataEngine
    from prometheus.core.clients.client_factory import ClientFactory

    logger.info(f"--- é–‹å§‹åŸ·è¡Œæ•¸æ“šå›å¡«ä½œæ¥­ï¼šå¾ {start_date} åˆ° {end_date} ---")

    data_engine = DataEngine()
    hourly_timestamps = pd.date_range(start=start_date, end=end_date, freq="H")
    total_tasks = len(hourly_timestamps)

    for i, ts in enumerate(hourly_timestamps):
        logger.debug(f"--- æ­£åœ¨è™•ç† ({i + 1}/{total_tasks}): {ts} ---")
        try:
            data_engine.generate_snapshot(ts)
        except Exception as e:
            logger.error(f"âŒ è™•ç† {ts} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)

    data_engine.close()
    ClientFactory.close_all()
    logger.info("--- æ•¸æ“šå›å¡«ä½œæ¥­å®Œæˆ ---")


from prometheus.models.strategy_models import Strategy
from prometheus.services.backtesting_service import BacktestingService
from prometheus.services.evolution_chamber import EvolutionChamber
from prometheus.services.strategy_reporter import StrategyReporter
from prometheus.core.db.db_manager import DBManager

@app.command()
def run_evolution_cycle():
    """
    ğŸš€ [ç«¯åˆ°ç«¯] åŸ·è¡Œä¸€æ¬¡å®Œæ•´çš„æ¼”åŒ–é€±æœŸï¼šæ¼”åŒ– -> å›æ¸¬ -> å ±å‘Šã€‚
    """
    print("--- å•Ÿå‹•ã€æ¼”åŒ–å®¤è¡Œå‹•ã€‘å®Œæ•´ä½œæˆ°é€±æœŸ ---")

    # 1. åˆå§‹åŒ–æ ¸å¿ƒæœå‹™
    db_manager = DBManager()
    backtester = BacktestingService(db_manager)

    # 2. æº–å‚™æ¼”åŒ–æ‰€éœ€æ•¸æ“š
    # å‡è¨­å› å­æ•¸æ“šå·²å­˜åœ¨
    all_factors_df = db_manager.fetch_table('factors')
    # æ’é™¤éå› å­æ¬„ä½
    available_factors = [col for col in all_factors_df.columns if col not in ['date', 'symbol', 'close']]

    if not available_factors:
        print("éŒ¯èª¤ï¼šæ•¸æ“šåº«ä¸­æ‰¾ä¸åˆ°å¯ç”¨çš„å› å­ã€‚è«‹å…ˆåŸ·è¡Œ build-feature-storeã€‚")
        return

    target_asset_for_evolution = 'AAPL' # é¸æ“‡ä¸€å€‹æ•¸æ“šåº«ä¸­å­˜åœ¨çš„è³‡ç”¢
    print(f"INFO: å°‡ä½¿ç”¨ '{target_asset_for_evolution}' ä½œç‚ºæœ¬æ¬¡æ¼”åŒ–çš„ç›®æ¨™è³‡ç”¢ã€‚")
    chamber = EvolutionChamber(backtester, available_factors, target_asset=target_asset_for_evolution)

    # 3. åŸ·è¡Œæ¼”åŒ–
    # ç‚ºäº†å¿«é€Ÿæ¼”ç¤ºï¼Œä½¿ç”¨è¼ƒå°çš„åƒæ•¸
    hof = chamber.run_evolution(n_pop=20, n_gen=5)

    if not hof:
        print("éŒ¯èª¤ï¼šæ¼”åŒ–æœªèƒ½ç”¢ç”Ÿæœ‰æ•ˆçµæœã€‚")
        return

    # 4. å°æœ€å„ªç­–ç•¥é€²è¡Œæœ€çµ‚å›æ¸¬ä»¥ç²å¾—å®Œæ•´å ±å‘Š
    best_individual = hof[0]
    best_factors = [available_factors[i] for i in best_individual]
    final_strategy = Strategy(
        factors=best_factors,
        weights={factor: 1.0 / len(best_factors) for factor in best_factors},
        target_asset='AAPL' # ä¿®æ­£ï¼šæ˜ç¢ºæŒ‡å®šä¸€å€‹å­˜åœ¨çš„è³‡ç”¢
    )
    final_report = backtester.run(final_strategy)

    # 5. ç”Ÿæˆå ±å‘Š
    reporter = StrategyReporter()
    reporter.generate_report(hof, final_report, available_factors)

    print("--- ã€æ¼”åŒ–å®¤è¡Œå‹•ã€‘ä½œæˆ°é€±æœŸçµæŸ ---")

if __name__ == "__main__":
    app()
import logging
from datetime import datetime, timedelta

import numpy as np
from prometheus.core.logging.log_manager import LogManager
import pandas as pd
import plotly.graph_objects as go

from prometheus.core.clients.fred import FredClient
from prometheus.core.clients.nyfed import NYFedClient

logger = LogManager.get_instance().get_logger("StressIndexCalculator")


class StressIndexCalculator:
    def __init__(self, rolling_window=252):
        logger.info("æ­£åœ¨åˆå§‹åŒ–å£“åŠ›æŒ‡æ•¸è¨ˆç®—å¼•æ“...")
        self.fred_client = FredClient()
        self.nyfed_client = NYFedClient()
        self.rolling_window = rolling_window
        self.logger = logging.getLogger(self.__class__.__name__)

    def _fetch_all_data(self, force_refresh=False):
        data_frames = {}
        nyfed_df = self.nyfed_client.fetch_data(force_refresh=force_refresh)
        if not nyfed_df.empty and "Date" in nyfed_df.columns and "Total_Positions" in nyfed_df.columns:
            data_frames["NYFed_Positions"] = nyfed_df[["Date", "Total_Positions"]].set_index("Date")
        fred_symbols = {"VIX": "VIXCLS", "Yield_Spread": "T10Y2Y", "Reserves": "WTREGEN", "SOFR": "SOFR"}
        for name, symbol in fred_symbols.items():
            df_item = self.fred_client.fetch_data(symbol, force_refresh=force_refresh)
            if not df_item.empty:
                data_frames[name] = df_item
        return data_frames

    def _align_and_preprocess(self, data_frames):
        if not data_frames:
            return pd.DataFrame()
        combined_df = pd.concat(data_frames.values(), axis=1, join="outer")
        combined_df = combined_df.ffill().dropna()
        return combined_df

    def _normalize_to_zscore(self, df):
        zscore_df = pd.DataFrame(index=df.index)
        for col in df.columns:
            series = pd.to_numeric(df[col], errors="coerce")
            if series.isnull().all():
                continue
            rolling_mean = series.rolling(window=self.rolling_window, min_periods=1).mean()
            rolling_std = series.rolling(window=self.rolling_window, min_periods=1).std()
            rolling_std.loc[rolling_std.abs() < 1e-6] = 1.0
            zscore_values = (series - rolling_mean) / rolling_std
            zscore_df[f"{col}_zscore"] = zscore_values
        return zscore_df.dropna()

    def _invert_and_weight(self, zscore_df):
        if "VIX_zscore" in zscore_df.columns:
            zscore_df["VIX_zscore"] = -zscore_df["VIX_zscore"]
        if "Yield_Spread_zscore" in zscore_df.columns:
            zscore_df["Yield_Spread_zscore"] = -zscore_df["Yield_Spread_zscore"]
        return zscore_df

    def calculate_stress_index(self, force_refresh=False):
        data_frames = self._fetch_all_data(force_refresh=force_refresh)
        aligned_df = self._align_and_preprocess(data_frames)
        if aligned_df.empty:
            return pd.Series(dtype="float64")
        zscore_df = self._normalize_to_zscore(aligned_df)
        weighted_df = self._invert_and_weight(zscore_df)
        stress_index = weighted_df.mean(axis=1)
        return stress_index.dropna()

    def plot_stress_index(self, stress_index, zscore_components):
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=stress_index.index, y=stress_index, mode='lines', name='Stress Index', line=dict(color='red', width=2)))
        for col in zscore_components.columns:
            fig.add_trace(go.Scatter(x=zscore_components.index, y=zscore_components[col], mode='lines', name=col, visible='legendonly'))
        fig.update_layout(title="Financial Stress Index and Components", xaxis_title="Date", yaxis_title="Z-Score / Index Value")
        fig.show()

if __name__ == "__main__":
    calculator = StressIndexCalculator()
    stress_index = calculator.calculate_stress_index()
    if not stress_index.empty:
        print("Stress Index calculated successfully:")
        print(stress_index.tail())
        data_frames = calculator._fetch_all_data()
        aligned_df = calculator._align_and_preprocess(data_frames)
        zscore_df = calculator._normalize_to_zscore(aligned_df)
        calculator.plot_stress_index(stress_index, zscore_df)
    else:
        print("Failed to calculate Stress Index.")
# æª”æ¡ˆè·¯å¾‘: core/analysis/data_engine.py
from datetime import datetime
from pathlib import Path
from typing import Any, Dict

import duckdb
import pandas as pd

from prometheus.core.clients.client_factory import ClientFactory
from prometheus.core.logging.log_manager import LogManager

logger = LogManager.get_instance().get_logger("DataEngine")


class DataEngine:
    """
    æ•¸æ“šå¼•æ“æ ¸å¿ƒã€‚
    è² è²¬å”èª¿æ‰€æœ‰æ•¸æ“šå®¢æˆ¶ç«¯ï¼Œè¨ˆç®—å¤šç¶­åº¦æŒ‡æ¨™ï¼Œ
    ä¸¦ç”Ÿæˆä¸€ä»½ã€Œé«˜å¯†åº¦å¸‚å ´ç‹€æ…‹å¿«ç…§ã€ã€‚
    """

    def __init__(
        self,
        db_connection=None,
    ):
        """
        é€éä¾è³´æ³¨å…¥åˆå§‹åŒ–ï¼Œå‚³å…¥æ‰€æœ‰éœ€è¦çš„æ•¸æ“šå®¢æˆ¶ç«¯ã€‚
        """
        self.yf_client = ClientFactory.get_client("yfinance")
        self.fred_client = ClientFactory.get_client("fred")
        self.taifex_client = ClientFactory.get_client("taifex")

        # --- æ–°å¢ç¨‹å¼ç¢¼ ---
        if db_connection:
            self.db_con = db_connection
            logger.info("ä½¿ç”¨å‚³å…¥çš„ DuckDB é€£æ¥ã€‚")
        else:
            db_path = Path("prometheus_fire.duckdb")
            self.db_con = duckdb.connect(database=str(db_path), read_only=False)
            logger.info("DuckDB é€£æ¥å·²å»ºç«‹ã€‚")

        self._initialize_db()

    def _initialize_db(self):
        """å¦‚æœ hourly_time_series è¡¨ä¸å­˜åœ¨ï¼Œå‰‡å‰µå»ºå®ƒã€‚"""
        try:
            self.db_con.execute("SELECT 1 FROM hourly_time_series LIMIT 1")
            logger.debug("'hourly_time_series' è¡¨å·²å­˜åœ¨ã€‚")
        except duckdb.CatalogException:
            logger.info("'hourly_time_series' è¡¨ä¸å­˜åœ¨ï¼Œæ­£åœ¨å‰µå»º...")
            schema = {
                "timestamp": "TIMESTAMP",
                "spy_open": "DOUBLE",
                "spy_high": "DOUBLE",
                "spy_low": "DOUBLE",
                "spy_close": "DOUBLE",
                "spy_volume": "BIGINT",
                "qqq_close": "DOUBLE",
                "tlt_close": "DOUBLE",
                "btc_usd_close": "DOUBLE",
                "nq_f_close": "DOUBLE",
                "es_f_close": "DOUBLE",
                "ym_f_close": "DOUBLE",
                "cl_f_close": "DOUBLE",
                "gc_f_close": "DOUBLE",
                "si_f_close": "DOUBLE",
                "zb_f_close": "DOUBLE",
                "zn_f_close": "DOUBLE",
                "zt_f_close": "DOUBLE",
                "zf_f_close": "DOUBLE",
                "gld_close": "DOUBLE",
                "shy_close": "DOUBLE",
                "iei_close": "DOUBLE",
                "aapl_close": "DOUBLE",
                "msft_close": "DOUBLE",
                "nvda_close": "DOUBLE",
                "goog_close": "DOUBLE",
                "tsm_close": "DOUBLE",
                "601318_ss_close": "DOUBLE",
                "688981_ss_close": "DOUBLE",
                "0981_hk_close": "DOUBLE",
                "spy_rsi_14_1h": "DOUBLE",
                "spy_macd_signal_1h": "DOUBLE",
                "spy_bbands_width_pct_1h": "DOUBLE",
                "spy_vwap_1h": "DOUBLE",
                "spy_atr_14_1h": "DOUBLE",
                "spy_vwap_deviation_pct_1h": "DOUBLE",
                "spy_momentum_1h_100": "DOUBLE",
                "spy_bollinger_band_upper_1h": "DOUBLE",
                "spy_bollinger_band_lower_1h": "DOUBLE",
                "spy_bb_middle_band_20h": "DOUBLE",
                "spy_bb_upper_band_20h": "DOUBLE",
                "spy_bb_lower_band_20h": "DOUBLE",
                "spy_bb_band_width_pct_20h": "DOUBLE",
                "spy_bb_percent_b_20h": "DOUBLE",
                "spy_gex_total": "DOUBLE",
                "spy_gex_flip_level": "DOUBLE",
                "spy_max_pain": "DOUBLE",
                "spy_call_wall_strike": "DOUBLE",
                "spy_put_wall_strike": "DOUBLE",
                "spy_pc_ratio_volume": "DOUBLE",
                "spy_pc_ratio_oi": "DOUBLE",
                "spy_iv_atm_1m": "DOUBLE",
                "spy_skew_quantified": "DOUBLE",
                "spy_vanna_exposure": "DOUBLE",
                "spy_charm_exposure": "DOUBLE",
                "vvix_close": "DOUBLE",
            }
            columns_def = ", ".join(
                [f'"{col}" {dtype}' for col, dtype in schema.items()]
            )
            create_table_sql = f"CREATE TABLE hourly_time_series ({columns_def})"
            self.db_con.execute(create_table_sql)
            logger.info("'hourly_time_series' è¡¨å·²æˆåŠŸå‰µå»ºã€‚")

    def close(self):
        self.db_con.close()
        logger.info("DuckDB é€£æ¥å·²é—œé–‰ã€‚")

    def _query_cache(self, dt):
        """
        å¾ DuckDB å¿«å–ä¸­æŸ¥è©¢å–®ä¸€æ™‚é–“é»çš„æ•¸æ“šã€‚
        :param dt: (datetime) è¦æŸ¥è©¢çš„æ™‚é–“æˆ³ã€‚
        :return: (pandas.DataFrame) å¦‚æœæ‰¾åˆ°æ•¸æ“šå‰‡è¿”å›å–®è¡Œ DataFrameï¼Œå¦å‰‡è¿”å› Noneã€‚
        """
        query = "SELECT * FROM hourly_time_series WHERE timestamp = ?"
        result_df = self.db_con.execute(query, [dt]).fetch_df()

        if not result_df.empty:
            logger.debug(f"CACHE HIT: æ–¼ {dt} æ‰¾åˆ°æ•¸æ“šã€‚")
            return result_df
        else:
            logger.debug(f"CACHE MISS: æ–¼ {dt} æœªæ‰¾åˆ°æ•¸æ“šã€‚")
            return None

    def _write_cache(self, data_df):
        """
        å°‡æ–°çš„æ•¸æ“š DataFrame å¯«å…¥ DuckDB å¿«å–ã€‚
        :param data_df: (pandas.DataFrame) åŒ…å«å–®è¡Œå¾…å¯«å…¥æ•¸æ“šçš„ DataFrameã€‚
        """
        self.db_con.append("hourly_time_series", data_df)
        logger.debug(f"CACHE WRITE: å·²å°‡ {data_df['timestamp'].iloc[0]} çš„æ•¸æ“šå¯«å…¥å¿«å–ã€‚")

    def _calculate_technicals(self, ohlcv: pd.DataFrame) -> Dict[str, Any]:
        """
        ç§æœ‰æ–¹æ³•ï¼šè¨ˆç®—åŸºç¤æŠ€è¡“æŒ‡æ¨™ã€‚
        ã€Julesçš„ä»»å‹™ã€‘: åœ¨æ­¤å¯¦ç¾ RSI, MACD, BBands ç­‰è¨ˆç®—é‚è¼¯ã€‚
        """
        technicals = {}
        # ç¯„ä¾‹ï¼šè¨ˆç®—20æ—¥å‡ç·š
        if "close" in ohlcv.columns and len(ohlcv) >= 20:
            technicals["MA20"] = round(
                ohlcv["close"].rolling(window=20).mean().iloc[-1], 2
            )
        else:
            technicals["MA20"] = None

        # TODO: å¯¦ç¾ RSI, MACD, BBands ç­‰æŒ‡æ¨™è¨ˆç®—
        technicals["RSI_14D"] = 70  # æš«ç”¨å‡æ•¸æ“š
        technicals["RSI_status"] = "è¶…è²·"  # æš«ç”¨å‡æ•¸æ“š

        return technicals

    def _calculate_approx_credit_spread(self) -> float:
        """
        è¨ˆç®—è¿‘ä¼¼ä¿¡ç”¨åˆ©å·® (HYGåƒ¹æ ¼ / IEFåƒ¹æ ¼)ã€‚
        """
        import asyncio
        try:
            hyg_data = asyncio.run(self.yf_client.fetch_data("HYG", period="1d"))
            ief_data = asyncio.run(self.yf_client.fetch_data("IEF", period="1d"))

            if (
                hyg_data.empty
                or "close" not in hyg_data.columns
                or hyg_data["close"].iloc[-1] is None
            ):
                logger.warning("ç„¡æ³•ç²å– HYG çš„æœ€æ–°æ”¶ç›¤åƒ¹ã€‚")
                return float("nan")
            if (
                ief_data.empty
                or "close" not in ief_data.columns
                or ief_data["close"].iloc[-1] is None
            ):
                logger.warning("ç„¡æ³•ç²å– IEF çš„æœ€æ–°æ”¶ç›¤åƒ¹ã€‚")
                return float("nan")

            hyg_price = hyg_data["close"].iloc[-1]
            ief_price = ief_data["close"].iloc[-1]

            if ief_price == 0:
                logger.warning("IEF åƒ¹æ ¼ç‚ºé›¶ï¼Œç„¡æ³•è¨ˆç®—ä¿¡ç”¨åˆ©å·®ã€‚")
                return float("nan")

            return round(hyg_price / ief_price, 4)
        except Exception as e:
            logger.error(f"è¨ˆç®—è¿‘ä¼¼ä¿¡ç”¨åˆ©å·®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return float("nan")

    def _calculate_proxy_move(self) -> float:
        """
        è¨ˆç®—ä»£ç†å‚µå¸‚æ³¢å‹•ç‡ (TLT 60å¤©æ—¥ç·šæ•¸æ“šçš„20å¤©æ»¾å‹•æ¨™æº–å·®)ã€‚
        """
        import asyncio
        try:
            tlt_data = asyncio.run(self.yf_client.fetch_data("TLT", period="60d"))
            if (
                tlt_data.empty or "close" not in tlt_data.columns or len(tlt_data) < 21
            ):  # Need at least 20 periods + 1 for pct_change
                logger.warning("TLT æ•¸æ“šä¸è¶³ä»¥è¨ˆç®—ä»£ç†æ³¢å‹•ç‡ã€‚")
                return float("nan")

            daily_returns = tlt_data["close"].pct_change()
            proxy_move = daily_returns.rolling(window=20).std().iloc[-1]
            return round(proxy_move, 4)
        except Exception as e:
            logger.error(f"è¨ˆç®—ä»£ç†å‚µå¸‚æ³¢å‹•ç‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return float("nan")

    def _calculate_gold_copper_ratio(self) -> float:
        """
        è¨ˆç®—é‡‘éŠ…æ¯” (GLDåƒ¹æ ¼ / HG=Fåƒ¹æ ¼)ã€‚
        """
        import asyncio
        try:
            gld_data = asyncio.run(self.yf_client.fetch_data("GLD", period="1d"))
            copper_data = asyncio.run(self.yf_client.fetch_data("HG=F", period="1d"))

            if (
                gld_data.empty
                or "close" not in gld_data.columns
                or gld_data["close"].iloc[-1] is None
            ):
                logger.warning("ç„¡æ³•ç²å– GLD çš„æœ€æ–°æ”¶ç›¤åƒ¹ã€‚")
                return float("nan")
            if (
                copper_data.empty
                or "close" not in copper_data.columns
                or copper_data["close"].iloc[-1] is None
            ):
                logger.warning("ç„¡æ³•ç²å– HG=F çš„æœ€æ–°æ”¶ç›¤åƒ¹ã€‚")
                return float("nan")

            gld_price = gld_data["close"].iloc[-1]
            copper_price = copper_data["close"].iloc[-1]

            if copper_price == 0:
                logger.warning("éŠ…åƒ¹ç‚ºé›¶ï¼Œç„¡æ³•è¨ˆç®—é‡‘éŠ…æ¯”ã€‚")
                return float("nan")

            return round(gld_price / copper_price, 4)
        except Exception as e:
            logger.error(f"è¨ˆç®—é‡‘éŠ…æ¯”æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return float("nan")

    def generate_snapshot(self, dt: datetime):
        # 1. é¦–å…ˆï¼Œå˜—è©¦å¾å¿«å–è®€å–æ•¸æ“š
        cached_data = self._query_cache(dt)

        # 2. åˆ¤æ–·å¿«å–æ˜¯å¦å‘½ä¸­
        if cached_data is not None:
            # --- å¿«å–å‘½ä¸­ ---
            # ç›´æ¥è¿”å›å¾è³‡æ–™åº«è®€å–çš„æ•¸æ“š
            return cached_data
        else:
            # --- å¿«å–æœªå‘½ä¸­ ---
            # a. åŸ·è¡Œç¾æœ‰çš„ API å‘¼å«é‚è¼¯ï¼Œç²å–æ‰€æœ‰åŸå§‹å¸‚å ´æ•¸æ“š
            #    (ä¾‹å¦‚: yfinance_client.get_data(), fred_client.fetch_data() ...)

            # b. åŸ·è¡Œç¾æœ‰çš„æ‰€æœ‰è¨ˆç®—é‚è¼¯ (æŠ€è¡“æŒ‡æ¨™ã€é¸æ“‡æ¬Šæ•¸æ“šç­‰)
            #    ...

            # c. å°‡æ‰€æœ‰ç²å–å’Œè¨ˆç®—å‡ºçš„æ•¸æ“šçµ„è£æˆä¸€å€‹ç¬¦åˆè¡¨æ ¼çµæ§‹çš„å–®è¡Œ DataFrame
            # new_data_df = self._build_snapshot_df(...) # å‡è¨­æœ‰æ­¤æ–¹æ³•

            # ç‚ºäº†æ¼”ç¤ºï¼Œé€™è£¡æˆ‘å€‘å›å‚³ä¸€å€‹å‡è³‡æ–™
            data = {
                "timestamp": [dt],
                "spy_open": [None],
                "spy_high": [None],
                "spy_low": [None],
                "spy_close": [500.0],
                "spy_volume": [None],
                "qqq_close": [None],
                "tlt_close": [None],
                "btc_usd_close": [None],
                "nq_f_close": [None],
                "es_f_close": [None],
                "ym_f_close": [None],
                "cl_f_close": [None],
                "gc_f_close": [None],
                "si_f_close": [None],
                "zb_f_close": [None],
                "zn_f_close": [None],
                "zt_f_close": [None],
                "zf_f_close": [None],
                "gld_close": [None],
                "shy_close": [None],
                "iei_close": [None],
                "aapl_close": [None],
                "msft_close": [None],
                "nvda_close": [None],
                "goog_close": [None],
                "tsm_close": [None],
                "601318_ss_close": [None],
                "688981_ss_close": [None],
                "0981_hk_close": [None],
                "spy_rsi_14_1h": [None],
                "spy_macd_signal_1h": [None],
                "spy_bbands_width_pct_1h": [None],
                "spy_vwap_1h": [None],
                "spy_atr_14_1h": [None],
                "spy_vwap_deviation_pct_1h": [None],
                "spy_momentum_1h_100": [None],
                "spy_bollinger_band_upper_1h": [None],
                "spy_bollinger_band_lower_1h": [None],
                "spy_bb_middle_band_20h": [None],
                "spy_bb_upper_band_20h": [None],
                "spy_bb_lower_band_20h": [None],
                "spy_bb_band_width_pct_20h": [None],
                "spy_bb_percent_b_20h": [None],
                "spy_gex_total": [None],
                "spy_gex_flip_level": [None],
                "spy_max_pain": [None],
                "spy_call_wall_strike": [None],
                "spy_put_wall_strike": [None],
                "spy_pc_ratio_volume": [None],
                "spy_pc_ratio_oi": [None],
                "spy_iv_atm_1m": [None],
                "spy_skew_quantified": [None],
                "spy_vanna_exposure": [None],
                "spy_charm_exposure": [None],
                "vvix_close": [None],
            }
            new_data_df = pd.DataFrame(data)

            # d. å°‡é€™ç­†æ–°æ•¸æ“šå¯«å…¥å¿«å–ï¼Œä¾›æœªä¾†ä½¿ç”¨
            self._write_cache(new_data_df)

            # e. è¿”å›é€™ç­†å‰›å¾ API ç²å–çš„æ–°æ•¸æ“š
            return new_data_df

    def get_hourly_series(
        self, ticker: str, column: str, start_date: str, end_date: str
    ) -> "pd.Series":
        """
        å¾ DuckDB ç²å–æŒ‡å®šæ™‚é–“ç¯„åœå…§çš„å°æ™‚ç´šæ•¸æ“šã€‚
        """
        query = f"SELECT timestamp, {ticker}_{column} FROM hourly_time_series WHERE timestamp BETWEEN ? AND ? ORDER BY timestamp"
        result_df = self.db_con.execute(query, [start_date, end_date]).fetch_df()

        if result_df.empty:
            return pd.Series(dtype="float64")

        result_df = result_df.set_index("timestamp")
        return result_df[f"{ticker}_{column}"]
import json
import sqlite3
import time
import abc
from pathlib import Path
from typing import Any, Optional

import logging

# ç‚ºæ­¤æ¨¡çµ„å‰µå»ºä¸€å€‹æ¨™æº–çš„ loggerï¼Œè€Œä¸æ˜¯ä¾è³´ LogManager
# é€™ä½¿å¾—æ¨¡çµ„æ›´åŠ ç¨ç«‹å’Œå¯é‡ç”¨
logger = logging.getLogger(__name__)


class BaseQueue(abc.ABC):
    """
    ä»»å‹™ä½‡åˆ—æŠ½è±¡åŸºåº•é¡åˆ¥ã€‚
    å®šç¾©äº†æ‰€æœ‰ä½‡åˆ—å¯¦ç¾éƒ½å¿…é ˆæä¾›çš„æ¨™æº–ä»‹é¢ã€‚
    """

    @abc.abstractmethod
    def put(self, task_data: dict) -> None:
        """
        å°‡ä¸€å€‹æ–°ä»»å‹™æ”¾å…¥ä½‡åˆ—ã€‚

        Args:
            task_data (dict): è¦åŸ·è¡Œçš„ä»»å‹™å…§å®¹ï¼Œå¿…é ˆæ˜¯å¯åºåˆ—åŒ–ç‚º JSON çš„å­—å…¸ã€‚
        """
        raise NotImplementedError

    @abc.abstractmethod
    def get(self) -> dict | None:
        """
        å¾ä½‡åˆ—ä¸­å–å‡ºä¸€å€‹å¾…è™•ç†çš„ä»»å‹™ã€‚
        æ­¤æ“ä½œæ‡‰å…·å‚™åŸå­æ€§ï¼Œé˜²æ­¢å¤šå€‹å·¥ä½œè€…å–å¾—åŒä¸€å€‹ä»»å‹™ã€‚

        Returns:
            dict | None: å¦‚æœä½‡åˆ—ä¸­æœ‰ä»»å‹™ï¼Œå‰‡è¿”å›ä»»å‹™å…§å®¹ï¼›å¦å‰‡è¿”å› Noneã€‚
        """
        raise NotImplementedError

    @abc.abstractmethod
    def task_done(self, task_id: any) -> None:
        """
        æ¨™è¨˜ä¸€å€‹ä»»å‹™å·²å®Œæˆã€‚

        Args:
            task_id (any): å·²å®Œæˆä»»å‹™çš„å”¯ä¸€è­˜åˆ¥ç¢¼ã€‚
        """
        raise NotImplementedError

    @abc.abstractmethod
    def qsize(self) -> int:
        """
        è¿”å›ä½‡åˆ—ä¸­å¾…è™•ç†ä»»å‹™çš„æ•¸é‡ã€‚

        Returns:
            int: å¾…è™•ç†ä»»å‹™çš„æ•¸é‡ã€‚
        """
        raise NotImplementedError


class SQLiteQueue(BaseQueue):
    """
    ä¸€å€‹åŸºæ–¼ SQLite çš„ã€æ”¯æŒé˜»å¡å’Œæ¯’ä¸¸é—œé–‰çš„æŒä¹…åŒ–ä½‡åˆ—ã€‚
    """

    def __init__(self, db_path: str | Path, table_name: str = "queue"):
        self.db_path = Path(db_path)
        self.table_name = table_name
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        # å…è¨±å¤šåŸ·è¡Œç·’å…±äº«åŒä¸€å€‹é€£ç·šï¼Œä¸¦å¢åŠ è¶…æ™‚
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False, timeout=10)
        self._init_db()

    def _init_db(self):
        with self.conn:
            self.conn.execute(f"""
                CREATE TABLE IF NOT EXISTS {self.table_name} (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    item TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

    def put(self, item: Any):
        """å°‡ä¸€å€‹é …ç›®æ”¾å…¥ä½‡åˆ—ã€‚"""
        with self.conn:
            self.conn.execute(
                f"INSERT INTO {self.table_name} (item) VALUES (?)", (json.dumps(item),)
            )

    def get(self, block: bool = True, timeout: Optional[float] = None) -> Optional[Any]:
        """
        å¾ä½‡åˆ—ä¸­å–å‡ºä¸€å€‹é …ç›®ã€‚
        å¦‚æœ block=Trueï¼Œå‰‡æœƒç­‰å¾…ç›´åˆ°æœ‰é …ç›®å¯ç”¨ã€‚
        """
        start_time = time.time()
        while True:
            try:
                with self.conn:
                    cursor = self.conn.cursor()
                    cursor.execute(
                        f"SELECT id, item FROM {self.table_name} ORDER BY id LIMIT 1"
                    )
                    row = cursor.fetchone()

                    if row:
                        item_id, item_json = row
                        cursor.execute(
                            f"DELETE FROM {self.table_name} WHERE id = ?", (item_id,)
                        )
                        return json.loads(item_json)
            except sqlite3.Error as e:
                # å¦‚æœç™¼ç”Ÿè³‡æ–™åº«éŒ¯èª¤ï¼ŒçŸ­æš«ç­‰å¾…å¾Œé‡è©¦
                logger.error(f"å¾ä½‡åˆ—è®€å–æ™‚ç™¼ç”Ÿè³‡æ–™åº«éŒ¯èª¤: {e}", exc_info=True)
                time.sleep(0.1)

            if not block:
                return None

            if timeout and (time.time() - start_time) > timeout:
                return None

            time.sleep(0.1)  # é¿å…éæ–¼é »ç¹åœ°æŸ¥è©¢

    def qsize(self) -> int:
        """è¿”å›ä½‡åˆ—ä¸­çš„é …ç›®æ•¸é‡ã€‚"""
        with self.conn:
            cursor = self.conn.cursor()
            cursor.execute(f"SELECT COUNT(*) FROM {self.table_name}")
            return cursor.fetchone()[0]

    def task_done(self, task_id: any) -> None:
        """åœ¨é€™å€‹å¯¦ä½œä¸­ï¼Œget() å·²ç¶“æ˜¯åŸå­æ€§çš„ï¼Œæ‰€ä»¥é€™å€‹æ–¹æ³•å¯ä»¥ç•™ç©ºã€‚"""
        pass

    def close(self):
        """é—œé–‰è³‡æ–™åº«é€£ç·šã€‚"""
        if self.conn:
            self.conn.close()
            self.conn = None
from prometheus.core.clients.fred import FredClient
from prometheus.core.config import ConfigManager

import logging

logger = logging.getLogger(__name__)

class ClientFactory:
    def __init__(self):
        self.config = ConfigManager()._config
        self._clients = {}

    def get_client(self, client_name):
        logger.info(f"Attempting to get client for: {client_name}")
        if client_name not in self._clients:
            if client_name == "fred":
                logger.info("Getting fred client")
                api_key = self.config.get("api_keys", {}).get("fred")
                self._clients[client_name] = FredClient(api_key=api_key)
            else:
                raise ValueError(f"Unknown client name: {client_name}")
        return self._clients[client_name]
"""
å®šç¾©ç³»çµ±ä¸­æ‰€æœ‰é ˜åŸŸäº‹ä»¶çš„æ¨™æº–è³‡æ–™çµæ§‹ã€‚
ä½¿ç”¨ dataclasses ç¢ºä¿äº‹ä»¶çš„ä¸å¯è®Šæ€§èˆ‡çµæ§‹æ¸…æ™°ã€‚
"""

import dataclasses
from typing import Any, Dict


@dataclasses.dataclass(frozen=True)
class BaseEvent:
    """äº‹ä»¶åŸºé¡"""

    pass


@dataclasses.dataclass(frozen=True)
class GenomeGenerated(BaseEvent):
    """ç•¶ä¸€å€‹æ–°çš„ç­–ç•¥åŸºå› é«”è¢«å‰µé€ å‡ºä¾†æ™‚è§¸ç™¼"""

    genome_id: str
    genome: Dict[str, Any]
    generation: int


@dataclasses.dataclass(frozen=True)
class BacktestCompleted(BaseEvent):
    """ç•¶ä¸€å€‹åŸºå› é«”çš„å›æ¸¬å®Œæˆæ™‚è§¸ç™¼"""

    genome_id: str
    sharpe_ratio: float
    generation: int
    genome: Dict[str, Any]


@dataclasses.dataclass(frozen=True)
class SystemShutdown(BaseEvent):
    """ä¸€å€‹ç‰¹æ®Šçš„ä¿¡è™Ÿäº‹ä»¶ï¼Œé€šçŸ¥æ‰€æœ‰æ¶ˆè²»è€…å„ªé›…åœ°é—œé–‰ã€‚"""

    reason: str
"""
åŸºæ–¼ aiosqlite çš„æŒä¹…åŒ–äº‹ä»¶æµå¯¦ç¾ã€‚
é€™æ˜¯ç³»çµ±çš„ã€Œå”¯ä¸€äº‹å¯¦ä¾†æºã€ã€‚
"""

import asyncio
import json
from typing import List, Tuple


class PersistentEventStream:
    def __init__(self, conn):
        self._conn = conn
        # ä½¿ç”¨ä¸€å€‹éåŒæ­¥é–ä¾†è™•ç†æ½›åœ¨çš„ä¸¦ç™¼å¯«å…¥
        self._lock = asyncio.Lock()

    async def initialize(self):
        """åˆå§‹åŒ–äº‹ä»¶å„²å­˜èˆ‡æª¢æŸ¥é»å„²å­˜ã€‚"""
        async with self._lock:
            await self._conn.execute("""
            CREATE TABLE IF NOT EXISTS events (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                event_type TEXT NOT NULL,
                data TEXT NOT NULL,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
            """)
            await self._conn.execute("""
            CREATE TABLE IF NOT EXISTS consumer_checkpoints (
                consumer_id TEXT PRIMARY KEY,
                last_processed_id INTEGER NOT NULL
            )
            """)
            await self._conn.commit()

    async def append(self, event):
        """å°‡ä¸€å€‹äº‹ä»¶é™„åŠ åˆ°æµçš„æœ«å°¾ã€‚"""
        event_type = type(event).__name__
        # å°‡ dataclass åºåˆ—åŒ–ç‚º JSON å­—ä¸²
        data = json.dumps(event.__dict__)
        async with self._lock:
            await self._conn.execute(
                "INSERT INTO events (event_type, data) VALUES (?, ?)",
                (event_type, data),
            )
            await self._conn.commit()

    async def subscribe(self, last_seen_id: int, batch_size: int = 100) -> List[Tuple]:
        """å¾ä¸Šæ¬¡çœ‹åˆ°çš„ä½ç½®è®€å–æ–°äº‹ä»¶ã€‚"""
        cursor = await self._conn.execute(
            "SELECT id, event_type, data FROM events WHERE id > ? ORDER BY id ASC LIMIT ?",
            (last_seen_id, batch_size),
        )
        return await cursor.fetchall()

    async def get_checkpoint(self, consumer_id: str) -> int:
        """ç²å–æŒ‡å®šæ¶ˆè²»è€…çš„æœ€å¾Œè™•ç†äº‹ä»¶IDã€‚"""
        cursor = await self._conn.execute(
            "SELECT last_processed_id FROM consumer_checkpoints WHERE consumer_id = ?",
            (consumer_id,),
        )
        row = await cursor.fetchone()
        return row[0] if row else 0

    async def update_checkpoint(self, consumer_id: str, last_processed_id: int):
        """æ›´æ–°æŒ‡å®šæ¶ˆè²»è€…çš„æª¢æŸ¥é»ã€‚"""
        async with self._lock:
            await self._conn.execute(
                """
                INSERT INTO consumer_checkpoints (consumer_id, last_processed_id)
                VALUES (?, ?)
                ON CONFLICT(consumer_id) DO UPDATE SET last_processed_id = excluded.last_processed_id
                """,
                (consumer_id, last_processed_id),
            )
            await self._conn.commit()

    async def get_total_event_count(self) -> int:
        """ç²å–äº‹ä»¶æµä¸­çš„äº‹ä»¶ç¸½æ•¸ã€‚"""
        cursor = await self._conn.execute("SELECT MAX(id) FROM events")
        row = await cursor.fetchone()
        return row[0] if row and row[0] is not None else 0

    async def get_all_checkpoints(self) -> dict[str, int]:
        """ç²å–æ‰€æœ‰æ¶ˆè²»è€…çš„æª¢æŸ¥é»ã€‚"""
        cursor = await self._conn.execute(
            "SELECT consumer_id, last_processed_id FROM consumer_checkpoints"
        )
        return dict(await cursor.fetchall())
# -*- coding: utf-8 -*-
"""
æ ¸å¿ƒå·¥å…·æ¨¡çµ„ï¼šä¸­å¤®å¿«å–å¼•æ“ (v2.0 - æ°¸ä¹…ä¿å­˜ç‰ˆ)

åŠŸèƒ½ï¼š
- æä¾›ä¸€å€‹å…¨å°ˆæ¡ˆå…±ç”¨çš„ã€é…ç½®å¥½å¿«å–ç­–ç•¥çš„ requests Session ç‰©ä»¶ã€‚
- é è¨­æ°¸ä¹…ä¿å­˜æ‰€æœ‰æˆåŠŸç²å–çš„æ•¸æ“šã€‚
- æ”¯æ´é€éä¸Šä¸‹æ–‡ç®¡ç†å™¨æ‰‹å‹•ç¦ç”¨å¿«å–ï¼Œä»¥å¯¦ç¾å¼·åˆ¶åˆ·æ–°ã€‚
"""

from contextlib import contextmanager

try:
    import requests_cache
except ImportError:
    requests_cache = None
from prometheus.core.logging.log_manager import LogManager

logger = LogManager.get_instance().get_logger("Helpers")

# å®šç¾©å¿«å–æª”æ¡ˆçš„è·¯å¾‘èˆ‡åç¨±
CACHE_NAME = ".financial_data_cache"
# é—œéµè®Šæ›´ï¼šå°‡éæœŸæ™‚é–“è¨­å®šç‚º Noneï¼Œä»£è¡¨ã€Œæ°¸ä¸éæœŸã€
# æ•¸æ“šä¸€ç¶“å¯«å…¥ï¼Œé™¤éæ‰‹å‹•æ¸…ç†å¿«å–æª”æ¡ˆï¼Œå¦å‰‡å°‡æ°¸ä¹…ä¿å­˜ã€‚
CACHE_EXPIRE_AFTER = None


def get_cached_session() -> requests_cache.CachedSession:
    """
    ç²å–ä¸€å€‹é…ç½®å¥½çš„ã€å¸¶æœ‰æ°¸ä¹…å¿«å–çš„ Session ç‰©ä»¶ã€‚

    Returns:
        requests_cache.CachedSession: é…ç½®å®Œæˆçš„å¿«å– Sessionã€‚
    """
    return requests_cache.CachedSession(
        cache_name=CACHE_NAME,
        backend="sqlite",
        expire_after=CACHE_EXPIRE_AFTER,
        allowable_methods=["GET", "POST"],
    )


@contextmanager
def temporary_disabled_cache(session: requests_cache.CachedSession):
    """
    ä¸€å€‹ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨æ–¼æš«æ™‚ç¦ç”¨çµ¦å®š Session çš„å¿«å–åŠŸèƒ½ã€‚
    é€™å°æ–¼å¯¦ç¾ã€Œå¼·åˆ¶åˆ·æ–°ã€åŠŸèƒ½è‡³é—œé‡è¦ã€‚

    Args:
        session (requests_cache.CachedSession): éœ€è¦æš«æ™‚ç¦ç”¨å¿«å–çš„ Sessionã€‚
    """
    with session.cache_disabled():
        yield


if __name__ == "__main__":
    # (è‡ªæˆ‘æ¸¬è©¦ä»£ç¢¼ç¶­æŒä¸è®Šï¼Œç”¨æ–¼é©—è­‰æ–°ç­–ç•¥)
    logger.info("--- [è‡ªæˆ‘æ¸¬è©¦] æ­£åœ¨é©—è­‰ä¸­å¤®å¿«å–å¼•æ“ (v2.0 æ°¸ä¹…ä¿å­˜æ¨¡å¼) ---")
    test_url = "https://httpbin.org/delay/2"
    session = get_cached_session()
    logger.info("æ­£åœ¨é€²è¡Œç¬¬ä¸€æ¬¡è«‹æ±‚ (æ‡‰æœ‰ 2 ç§’å»¶é²)...")
    import time

    start_time = time.time()
    response1 = session.get(test_url)
    end_time = time.time()
    logger.info(
        f"ç¬¬ä¸€æ¬¡è«‹æ±‚å®Œæˆã€‚è€—æ™‚: {end_time - start_time:.2f} ç§’ã€‚From Cache: {response1.from_cache}"
    )

    logger.info("\næ­£åœ¨é€²è¡Œç¬¬äºŒæ¬¡è«‹æ±‚ (æ‡‰ç«‹å³å®Œæˆ)...")
    start_time = time.time()
    response2 = session.get(test_url)
    end_time = time.time()
    logger.info(
        f"ç¬¬äºŒæ¬¡è«‹æ±‚å®Œæˆã€‚è€—æ™‚: {end_time - start_time:.2f} ç§’ã€‚From Cache: {response2.from_cache}"
    )

    logger.info("\næ­£åœ¨é€²è¡Œå¼·åˆ¶åˆ·æ–°è«‹æ±‚ (æ‡‰å†æ¬¡æœ‰ 2 ç§’å»¶é²)...")
    start_time = time.time()
    with temporary_disabled_cache(session):
        response3 = session.get(test_url)
    end_time = time.time()
    logger.info(
        f"å¼·åˆ¶åˆ·æ–°è«‹æ±‚å®Œæˆã€‚è€—æ™‚: {end_time - start_time:.2f} ç§’ã€‚From Cache: {response3.from_cache}"
    )

    logger.info("\n--- [è‡ªæˆ‘æ¸¬è©¦] å®Œæˆ ---")
    session.cache.clear()
    logger.info("æ¸¬è©¦å¿«å–å·²æ¸…ç†ã€‚")


from pathlib import Path
from typing import Tuple

import pandas as pd


def load_ohlcv_data(
    file_path: Path, split_ratio: float = 0.7
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    å¾ CSV æª”æ¡ˆåŠ è¼‰ OHLCV æ•¸æ“šï¼Œä¸¦å°‡å…¶åˆ†å‰²ç‚ºæ¨£æœ¬å…§å’Œæ¨£æœ¬å¤–æ•¸æ“šé›†ã€‚

    :param file_path: CSV æª”æ¡ˆçš„è·¯å¾‘ã€‚
    :param split_ratio: æ¨£æœ¬å…§æ•¸æ“šæ‰€ä½”çš„æ¯”ä¾‹ (ä¾‹å¦‚ 0.7 ä»£è¡¨ 70%)ã€‚
    :return: ä¸€å€‹åŒ…å« (in_sample_df, out_of_sample_df) çš„å…ƒçµ„ã€‚
    """
    if not file_path.exists():
        raise FileNotFoundError(f"æ•¸æ“šæª”æ¡ˆä¸å­˜åœ¨: {file_path}")

    # æ ¹æ“šä½œæˆ°æ‰‹å†Šï¼Œç´¢å¼•æ‡‰ç‚º 'Date' ä¸”æ¬„ä½åç¨±æ‡‰ç‚ºå°å¯«
    df = pd.read_csv(file_path, index_col="Date", parse_dates=True)
    df.columns = [col.lower() for col in df.columns]

    # æ ¹æ“šæ¯”ä¾‹è¨ˆç®—åˆ†å‰²é»
    split_point = int(len(df) * split_ratio)

    in_sample_df = df.iloc[:split_point]
    out_of_sample_df = df.iloc[split_point:]

    logger.info(
        f"[DataLoader] æ•¸æ“šå·²åˆ†å‰²ï¼šæ¨£æœ¬å…§ {len(in_sample_df)} ç­†, æ¨£æœ¬å¤– {len(out_of_sample_df)} ç­†ã€‚"
    )

    return in_sample_df, out_of_sample_df


import zipfile
from typing import Dict, Optional


def prospect_file_content(file_bytes: bytes) -> Dict[str, str]:
    """å˜—è©¦è§£ç¢¼ä¸¦è®€å–ç¬¬ä¸€è¡Œ(æ¨™é ­)ã€‚"""
    for encoding in ["ms950", "big5", "utf-8", "utf-8-sig"]:
        try:
            content = file_bytes.decode(encoding)
            header = content.splitlines()[0].strip()
            return {"status": "success", "encoding": encoding, "header": header}
        except (UnicodeDecodeError, IndexError):
            continue
    return {"status": "failure", "error": "ç„¡æ³•è§£ç¢¼æˆ–æª”æ¡ˆç‚ºç©º"}


def read_file_content(file_path: str) -> Optional[bytes]:
    """è®€å–æª”æ¡ˆå…§å®¹ï¼Œæ”¯æ´ ZIP æª”æ¡ˆã€‚"""
    if zipfile.is_zipfile(file_path):
        try:
            with zipfile.ZipFile(file_path, "r") as zf:
                for member_name in zf.namelist():
                    if member_name.lower().endswith((".csv", ".txt")):
                        return zf.read(member_name)
        except zipfile.BadZipFile:
            return None
    elif file_path.lower().endswith((".csv", ".txt")):
        with open(file_path, "rb") as f:
            return f.read()
    return None


def correct_path():
    pass
# src/prometheus/core/engines/crypto_factor_engine.py

import pandas as pd
import logging
from typing import Dict, Any

from src.prometheus.core.analyzers.base_analyzer import BaseAnalyzer
from src.prometheus.core.clients.client_factory import ClientFactory


logger = logging.getLogger(__name__)


class CryptoFactorEngine(BaseAnalyzer):
    """
    åŠ å¯†è²¨å¹£å› å­å¼•æ“ï¼Œå°ˆé–€è¨ˆç®—èˆ‡åŠ å¯†è²¨å¹£ç›¸é—œçš„å› å­ã€‚
    """

    def __init__(self, client_factory: ClientFactory):
        """
        åˆå§‹åŒ–åŠ å¯†è²¨å¹£å› å­å¼•æ“ã€‚
        """
        super().__init__(analyzer_name="CryptoFactorEngine")
        self.client_factory = client_factory
        self.yfinance_client = self.client_factory.get_client('yfinance')

    def _load_data(self) -> pd.DataFrame:
        """
        æ­¤æ–¹æ³•åœ¨æ­¤å¼•æ“ä¸­ä¸ä½¿ç”¨ï¼Œå› ç‚ºæ•¸æ“šç”± Pipeline æä¾›ã€‚
        """
        self.logger.debug("CryptoFactorEngine._load_data called, but not used in pipeline context.")
        return pd.DataFrame()

    def _perform_analysis(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        æ­¤æ–¹æ³•è¢« run æ–¹æ³•è¦†è“‹ï¼Œå› æ­¤ä¸æœƒè¢«ç›´æ¥èª¿ç”¨ã€‚
        """
        self.logger.debug("CryptoFactorEngine._perform_analysis called, but logic is in run.")
        return data

    def _save_results(self, results: pd.DataFrame) -> None:
        """
        æ­¤æ–¹æ³•åœ¨æ­¤å¼•æ“ä¸­ä¸ä½¿ç”¨ï¼Œå› ç‚ºçµæœç”± Pipeline ä¿å­˜ã€‚
        """
        self.logger.debug("CryptoFactorEngine._save_results called, but not used in pipeline context.")
        pass

    async def run(self, data: pd.DataFrame, config: Dict[str, Any] = None) -> pd.DataFrame:
        """
        åŸ·è¡Œå› å­è¨ˆç®—ã€‚

        :param data: åŒ…å«åŠ å¯†è²¨å¹£åƒ¹æ ¼æ•¸æ“šçš„ DataFrameï¼Œç´¢å¼•ç‚ºæ—¥æœŸï¼Œæ‡‰åŒ…å« 'symbol' æ¬„ä½ã€‚
        :param config: å¯é¸çš„é…ç½®å­—å…¸ã€‚
        :return: åŒ…å«æ–°è¨ˆç®—å› å­çš„ DataFrameã€‚
        """
        if 'symbol' not in data.columns:
            raise ValueError("è¼¸å…¥çš„ DataFrame å¿…é ˆåŒ…å« 'symbol' æ¬„ä½ã€‚")

        symbol = data['symbol'].iloc[0]
        self.logger.info(f"é–‹å§‹ç‚ºåŠ å¯†è²¨å¹£ {symbol} è¨ˆç®—å› å­...")

        # è¤‡è£½æ•¸æ“šä»¥é¿å…ä¿®æ”¹åŸå§‹ DataFrame
        result_df = data.copy()

        # è¨ˆç®—èˆ‡ç´æ–¯é”å…‹æŒ‡æ•¸çš„ç›¸é—œæ€§
        result_df = await self._calculate_nasdaq_correlation(result_df)

        # è¨ˆç®—ææ‡¼èˆ‡è²ªå©ªæŒ‡æ•¸ä»£ç†
        result_df = self._calculate_fear_greed_proxy(result_df)

        self.logger.info(f"åŠ å¯†è²¨å¹£ {symbol} çš„å› å­è¨ˆç®—å®Œæˆã€‚")
        return result_df

    async def _calculate_nasdaq_correlation(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è¨ˆç®—èˆ‡ç´æ–¯é”å…‹100æŒ‡æ•¸æœŸè²¨ (NQ=F) çš„ 30 æ—¥æ»¾å‹•ç›¸é—œæ€§ã€‚
        """
        self.logger.debug("æ­£åœ¨è¨ˆç®—èˆ‡ç´æ–¯é”å…‹æŒ‡æ•¸çš„ç›¸é—œæ€§...")
        try:
            # ç²å– NQ=F çš„æ•¸æ“š
            start_date = df.index.min()
            end_date = df.index.max()
            nasdaq_data = await self.yfinance_client.fetch_data('NQ=F', start_date=start_date, end_date=end_date)
            if nasdaq_data is None or nasdaq_data.empty:
                self.logger.warning("ç„¡æ³•ç²å–ç´æ–¯é”å…‹æ•¸æ“š (NQ=F)ï¼Œè·³éç›¸é—œæ€§è¨ˆç®—ã€‚")
                df['factor_corr_nq'] = None
                return df

            # ç¢ºä¿å…©å€‹ DataFrame çš„ç´¢å¼•éƒ½æ˜¯æ—¥æœŸæ™‚é–“é¡å‹ä¸”æ²’æœ‰é‡è¤‡
            df.index = pd.to_datetime(df.index)
            df = df[~df.index.duplicated(keep='first')]

            nasdaq_data.index = pd.to_datetime(nasdaq_data.index)
            nasdaq_data = nasdaq_data[~nasdaq_data.index.duplicated(keep='first')]

            # åˆä½µæ•¸æ“šä¸¦è¨ˆç®—æ—¥æ”¶ç›Šç‡
            merged_df = pd.merge(df[['close']], nasdaq_data[['close']], left_index=True, right_index=True, suffixes=('_crypto', '_nasdaq'))
            merged_df['crypto_returns'] = merged_df['close_crypto'].pct_change()
            merged_df['nasdaq_returns'] = merged_df['close_nasdaq'].pct_change()

            # è¨ˆç®— 30 æ—¥æ»¾å‹•ç›¸é—œæ€§
            correlation = merged_df['crypto_returns'].rolling(window=30).corr(merged_df['nasdaq_returns'])

            # å°‡è¨ˆç®—å‡ºçš„ç›¸é—œæ€§åˆä½µå›åŸå§‹ DataFrame
            df['factor_corr_nq'] = correlation

            self.logger.debug("æˆåŠŸè¨ˆç®—èˆ‡ç´æ–¯é”å…‹æŒ‡æ•¸çš„ç›¸é—œæ€§ã€‚")

        except Exception as e:
            self.logger.error(f"è¨ˆç®—ç´æ–¯é”å…‹ç›¸é—œæ€§æ™‚å‡ºéŒ¯: {e}", exc_info=True)
            df['factor_corr_nq'] = None

        return df

    def _calculate_fear_greed_proxy(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è¨ˆç®— 7 æ—¥çš„å·²å¯¦ç¾æ³¢å‹•ç‡ï¼Œä½œç‚ºå¸‚å ´æƒ…ç·’çš„ä»£ç†æŒ‡æ¨™ã€‚
        å·²å¯¦ç¾æ³¢å‹•ç‡è¶Šé«˜ï¼Œé€šå¸¸è¡¨ç¤ºå¸‚å ´ææ‡¼æƒ…ç·’è¶Šæ¿ƒã€‚
        """
        self.logger.debug("æ­£åœ¨è¨ˆç®—ææ‡¼èˆ‡è²ªå©ªæŒ‡æ•¸ä»£ç†ï¼ˆ7æ—¥å·²å¯¦ç¾æ³¢å‹•ç‡ï¼‰...")
        try:
            # è¨ˆç®—æ—¥æ”¶ç›Šç‡
            returns = df['close'].pct_change()
            # è¨ˆç®— 7 æ—¥æ»¾å‹•æ¨™æº–å·®ï¼ˆæ³¢å‹•ç‡ï¼‰
            volatility = returns.rolling(window=7).std()
            df['factor_fear_greed_proxy'] = volatility
            self.logger.debug("æˆåŠŸè¨ˆç®—ææ‡¼èˆ‡è²ªå©ªæŒ‡æ•¸ä»£ç†ã€‚")
        except Exception as e:
            self.logger.error(f"è¨ˆç®—ææ‡¼èˆ‡è²ªå©ªæŒ‡æ•¸ä»£ç†æ™‚å‡ºéŒ¯: {e}", exc_info=True)
            df['factor_fear_greed_proxy'] = None
        return df
# This file intentionally left blank.
import pandas as pd
import numpy as np

class BondFactorEngine:
    def calculate(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è¨ˆç®—å‚µåˆ¸å’Œåˆ©ç‡ç‰¹å®šå› å­ã€‚
        """
        result_df = df.copy()

        # ç¢ºä¿å¿…è¦çš„æ¬„ä½å­˜åœ¨
        required_columns = ['yield_curve_slope', 'credit_spread', 'real_yield']
        if not all(col in result_df.columns for col in required_columns):
            missing_cols = [col for col in required_columns if col not in result_df.columns]
            raise ValueError(f"ç¼ºå°‘å¿…è¦çš„æ¬„ä½ä¾†è¨ˆç®—å‚µåˆ¸å› å­: {missing_cols}")

        # æ®–åˆ©ç‡æ›²ç·šæ–œç‡ (ç›´æ¥ä½¿ç”¨)
        result_df['factor_yield_curve_slope'] = result_df['yield_curve_slope']

        # é«˜æ”¶ç›Šå‚µä¿¡ç”¨åˆ©å·® Z-score
        if 'credit_spread' in result_df.columns:
            # å°‡ credit_spread è½‰æ›ç‚ºæ•¸å­—é¡å‹ï¼Œä¸¦å°‡éŒ¯èª¤è½‰æ›ç‚º NaN
            credit_spread_numeric = pd.to_numeric(result_df['credit_spread'], errors='coerce')

            # è¨ˆç®— 252 å¤©æ»¾å‹• Z-score
            rolling_mean = credit_spread_numeric.rolling(window=252).mean()
            rolling_std = credit_spread_numeric.rolling(window=252).std()
            result_df['factor_credit_spread_zscore'] = (credit_spread_numeric - rolling_mean) / rolling_std

        # å¯¦è³ªåˆ©ç‡ (ç›´æ¥ä½¿ç”¨)
        result_df['factor_real_yield'] = result_df['real_yield']

        return result_df
# src/prometheus/core/engines/stock_factor_engine.py

import pandas as pd
import logging
from typing import Dict, Any, List

from src.prometheus.core.analyzers.base_analyzer import BaseAnalyzer
from src.prometheus.core.clients.client_factory import ClientFactory

logger = logging.getLogger(__name__)


class StockFactorEngine(BaseAnalyzer):
    """
    è‚¡ç¥¨å› å­å¼•æ“ï¼Œå°ˆé–€è¨ˆç®—èˆ‡å€‹è‚¡ç›¸é—œçš„è²¡å‹™å› å­ã€‚
    """

    def __init__(self, client_factory: ClientFactory):
        """
        åˆå§‹åŒ–è‚¡ç¥¨å› å­å¼•æ“ã€‚

        :param client_factory: å®¢æˆ¶ç«¯å·¥å» ï¼Œç”¨æ–¼ç²å– yfinance å’Œ FinMind çš„å®¢æˆ¶ç«¯ã€‚
        """
        super().__init__(analyzer_name="StockFactorEngine")
        self.client_factory = client_factory
        self.yfinance_client = self.client_factory.get_client('yfinance')
        self.finmind_client = self.client_factory.get_client('finmind')

    def _load_data(self) -> pd.DataFrame:
        """
        æ­¤æ–¹æ³•åœ¨æ­¤å¼•æ“ä¸­ä¸ä½¿ç”¨ï¼Œå› ç‚ºæ•¸æ“šç”± Pipeline æä¾›ã€‚
        """
        self.logger.debug("StockFactorEngine._load_data called, but not used in pipeline context.")
        return pd.DataFrame()

    def _perform_analysis(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        æ­¤æ–¹æ³•è¢« run æ–¹æ³•è¦†è“‹ï¼Œå› æ­¤ä¸æœƒè¢«ç›´æ¥èª¿ç”¨ã€‚
        """
        self.logger.debug("StockFactorEngine._perform_analysis called, but logic is in run.")
        return data

    def _save_results(self, results: pd.DataFrame) -> None:
        """
        æ­¤æ–¹æ³•åœ¨æ­¤å¼•æ“ä¸­ä¸ä½¿ç”¨ï¼Œå› ç‚ºçµæœç”± Pipeline ä¿å­˜ã€‚
        """
        self.logger.debug("StockFactorEngine._save_results called, but not used in pipeline context.")
        pass

    async def run(self, data: pd.DataFrame, config: Dict[str, Any] = None) -> pd.DataFrame:
        """
        åŸ·è¡Œå› å­è¨ˆç®—ã€‚

        :param data: åŒ…å«è‚¡ç¥¨åƒ¹æ ¼æ•¸æ“šçš„ DataFrameï¼Œç´¢å¼•ç‚ºæ—¥æœŸï¼Œæ‡‰åŒ…å« 'symbol' æ¬„ä½ã€‚
        :param config: å¯é¸çš„é…ç½®å­—å…¸ã€‚
        :return: åŒ…å«æ–°è¨ˆç®—å› å­çš„ DataFrameã€‚
        """
        if 'symbol' not in data.columns:
            raise ValueError("è¼¸å…¥çš„ DataFrame å¿…é ˆåŒ…å« 'symbol' æ¬„ä½ã€‚")

        # è¤‡è£½æ•¸æ“šä»¥é¿å…ä¿®æ”¹åŸå§‹ DataFrame
        result_df = data.copy()

        # è¨ˆç®—åŸºæœ¬é¢å› å­
        result_df = await self._calculate_fundamental_factors(result_df)

        # è¨ˆç®—æŠ€è¡“é¢å› å­ (å¦‚æœéœ€è¦)
        # ...

        return result_df

    async def _calculate_fundamental_factors(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è¨ˆç®—ä¸¦åˆä½µæ‰€æœ‰åŸºæœ¬é¢å› å­ã€‚
        """
        symbol = df['symbol'].iloc[0]
        self.logger.info(f"é–‹å§‹ç‚ºè‚¡ç¥¨ {symbol} è¨ˆç®—åŸºæœ¬é¢å› å­...")

        # ç²å– yfinance çš„ ticker ç‰©ä»¶
        ticker = self.yfinance_client.get_ticker(symbol)

        # è¨ˆç®—æœ¬ç›Šæ¯” (P/E Ratio)
        pe_ratio = self._calculate_pe_ratio(ticker)
        df['factor_pe_ratio'] = pe_ratio

        # è¨ˆç®—è‚¡åƒ¹æ·¨å€¼æ¯” (P/B Ratio)
        pb_ratio = self._calculate_pb_ratio(ticker)
        df['factor_pb_ratio'] = pb_ratio

        # è¨ˆç®—æœˆç‡Ÿæ”¶å¹´å¢ç‡ (åƒ…é™å°è‚¡)
        if '.TW' in symbol:
            df = await self._calculate_monthly_revenue_yoy(df, symbol)
        else:
            df['factor_monthly_revenue_yoy'] = None


        self.logger.info(f"è‚¡ç¥¨ {symbol} çš„åŸºæœ¬é¢å› å­è¨ˆç®—å®Œæˆã€‚")
        return df

    def _calculate_pe_ratio(self, ticker: Any) -> float | None:
        """
        ä½¿ç”¨ yfinance.info ç²å– P/E Ratio (TTM)ã€‚
        """
        try:
            # TTM = Trailing Twelve Months
            pe = ticker.info.get('trailingPE')
            self.logger.debug(f"æˆåŠŸç²å– P/E Ratio: {pe}")
            return pe
        except Exception as e:
            self.logger.warning(f"ç„¡æ³•ç²å– P/E Ratio: {e}")
            return None

    def _calculate_pb_ratio(self, ticker: Any) -> float | None:
        """
        ä½¿ç”¨ yfinance.info ç²å– P/B Ratioã€‚
        """
        try:
            pb = ticker.info.get('priceToBook')
            self.logger.debug(f"æˆåŠŸç²å– P/B Ratio: {pb}")
            return pb
        except Exception as e:
            self.logger.warning(f"ç„¡æ³•ç²å– P/B Ratio: {e}")
            return None

    async def _calculate_monthly_revenue_yoy(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """
        è¨ˆç®—å°è‚¡çš„æœˆç‡Ÿæ”¶å¹´å¢ç‡ (YoY)ã€‚
        """
        stock_id = symbol.replace('.TW', '')
        # æˆ‘å€‘åªéœ€è¦æœ€æ–°çš„æ—¥æœŸä¾†ç²å–å°æ‡‰æœˆä»½çš„ç‡Ÿæ”¶
        latest_date = pd.to_datetime(df.index.max(), unit='ns')

        try:
            # ä½¿ç”¨ FinMind å®¢æˆ¶ç«¯ç²å–æœˆç‡Ÿæ”¶æ•¸æ“š
            revenue_data = await self.finmind_client.get_monthly_revenue(stock_id, latest_date.year - 2, latest_date.month)
            if revenue_data.empty:
                self.logger.warning(f"è‚¡ç¥¨ {symbol} åœ¨ {latest_date} é™„è¿‘ç„¡æœˆç‡Ÿæ”¶æ•¸æ“šã€‚")
                df['factor_monthly_revenue_yoy'] = None
                return df

            # å°‡ç‡Ÿæ”¶æ•¸æ“šçš„æ—¥æœŸè¨­ç‚ºç´¢å¼•
            revenue_data['date'] = pd.to_datetime(revenue_data['date'])
            revenue_data.set_index('date', inplace=True)

            # è¨ˆç®—å¹´å¢ç‡
            # æ‰¾åˆ°èˆ‡ df ä¸­æ¯å€‹æ—¥æœŸå°æ‡‰çš„æœˆä»½
            df['year'] = df.index.year
            df['month'] = df.index.month

            def get_revenue_yoy(row):
                current_month_revenue = revenue_data[
                    (revenue_data.index.year == row['year']) &
                    (revenue_data.index.month == row['month'])
                ]

                last_year_month_revenue = revenue_data[
                    (revenue_data.index.year == row['year'] - 1) &
                    (revenue_data.index.month == row['month'])
                ]

                if not current_month_revenue.empty and not last_year_month_revenue.empty:
                    current_revenue = current_month_revenue['revenue'].iloc[0]
                    last_year_revenue = last_year_month_revenue['revenue'].iloc[0]
                    if last_year_revenue != 0:
                        return (current_revenue - last_year_revenue) / last_year_revenue
                return None

            df['factor_monthly_revenue_yoy'] = df.apply(get_revenue_yoy, axis=1)
            df.drop(columns=['year', 'month'], inplace=True)

            self.logger.debug(f"æˆåŠŸè¨ˆç®—è‚¡ç¥¨ {symbol} çš„æœˆç‡Ÿæ”¶å¹´å¢ç‡ã€‚")

        except Exception as e:
            self.logger.error(f"è¨ˆç®—è‚¡ç¥¨ {symbol} çš„æœˆç‡Ÿæ”¶å¹´å¢ç‡æ™‚å‡ºéŒ¯: {e}")
            df['factor_monthly_revenue_yoy'] = None

        return df
import pandas as pd
import numpy as np

class IndexFactorEngine:
    def calculate(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è¨ˆç®—æŒ‡æ•¸ç‰¹å®šå› å­ã€‚
        """
        result_df = df.copy()

        # ç¢ºä¿å¿…è¦çš„æ¬„ä½å­˜åœ¨
        required_columns = ['vix', 'move', 'close']
        if not all(col in result_df.columns for col in required_columns):
            missing_cols = [col for col in required_columns if col not in result_df.columns]
            raise ValueError(f"ç¼ºå°‘å¿…è¦çš„æ¬„ä½ä¾†è¨ˆç®—æŒ‡æ•¸å› å­: {missing_cols}")

        # è‚¡å‚µæ³¢å‹•ç‡æ¯”ç‡ (VIX / MOVE)
        # é¿å…é™¤ä»¥é›¶
        if 'vix' in result_df.columns and 'move' in result_df.columns:
            result_df['factor_vix_move_ratio'] = result_df['vix'] / result_df['move'].replace(0, np.nan)

        # æ³¢å‹•ç‡é¢¨éšªæº¢åƒ¹ (VIX - å·²å¯¦ç¾æ³¢å‹•ç‡)
        # è¨ˆç®— 20 æ—¥å·²å¯¦ç¾æ³¢å‹•ç‡
        result_df['realized_vol_20d'] = result_df['close'].pct_change().rolling(window=20).std() * np.sqrt(252)
        if 'vix' in result_df.columns:
            result_df['factor_vrp'] = result_df['vix'] - result_df['realized_vol_20d']

        return result_df
# core/analyzers/base_analyzer.py
from __future__ import annotations

import logging
from abc import ABC, abstractmethod

import pandas as pd


class BaseAnalyzer(ABC):
    """
    æ‰€æœ‰åˆ†æå™¨æ‡‰ç”¨çš„æŠ½è±¡åŸºç¤é¡ (Abstract Base Class)ã€‚

    å®ƒæ¡ç”¨äº†ã€Œæ¨¡æ¿æ–¹æ³•ã€è¨­è¨ˆæ¨¡å¼ï¼Œå®šç¾©äº†ä¸€å€‹æ¨™æº–åŒ–çš„åˆ†æå·¥ä½œæµç¨‹éª¨æ¶ (`run` æ–¹æ³•)ï¼Œ
    åŒæ™‚å…è¨±å­é¡é€šéå¯¦ç¾æŠ½è±¡æ–¹æ³•ä¾†å®šç¾©å…·é«”çš„æ­¥é©Ÿã€‚

    æ‰€æœ‰ç¹¼æ‰¿æ­¤é¡çš„åˆ†æå™¨éƒ½å°‡è‡ªå‹•ç²å¾—æ¨™æº–åŒ–çš„æ—¥èªŒè¨˜éŒ„å’ŒåŸ·è¡Œæµç¨‹ã€‚
    """

    def __init__(self, analyzer_name: str):
        """
        åˆå§‹åŒ–åŸºç¤åˆ†æå™¨ã€‚

        Args:
            analyzer_name: åˆ†æå™¨çš„åç¨±ï¼Œå°‡ç”¨æ–¼æ—¥èªŒè¨˜éŒ„ã€‚
        """
        self.analyzer_name = analyzer_name
        self.logger = logging.getLogger(f"analyzer.{self.analyzer_name}")
        self.logger.info(f"åˆ†æå™¨ '{self.analyzer_name}' å·²åˆå§‹åŒ–ã€‚")

    @abstractmethod
    def _load_data(self) -> pd.DataFrame:
        """
        ã€å­é¡å¿…é ˆå¯¦ç¾ã€‘è¼‰å…¥åˆ†ææ‰€éœ€çš„åŸå§‹æ•¸æ“šã€‚

        Returns:
            ä¸€å€‹åŒ…å«åŸå§‹æ•¸æ“šçš„ Pandas DataFrameã€‚
        """
        pass

    @abstractmethod
    def _perform_analysis(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        ã€å­é¡å¿…é ˆå¯¦ç¾ã€‘åŸ·è¡Œå…·é«”çš„æ ¸å¿ƒåˆ†æé‚è¼¯ã€‚

        Args:
            data: å¾ _load_data() æ–¹æ³•è¼‰å…¥çš„æ•¸æ“šã€‚

        Returns:
            ä¸€å€‹åŒ…å«åˆ†æçµæœçš„ Pandas DataFrameã€‚
        """
        pass

    @abstractmethod
    def _save_results(self, results: pd.DataFrame) -> None:
        """
        ã€å­é¡å¿…é ˆå¯¦ç¾ã€‘å°‡åˆ†æçµæœé€²è¡Œä¿å­˜ï¼ˆä¾‹å¦‚å­˜å…¥æ•¸æ“šåº«ã€å¯«å…¥ CSV æ–‡ä»¶ç­‰ï¼‰ã€‚

        Args:
            results: å¾ _perform_analysis() æ–¹æ³•è¿”å›çš„çµæœã€‚
        """
        pass

    def run(self) -> None:
        """
        åŸ·è¡Œå®Œæ•´çš„åˆ†æå·¥ä½œæµç¨‹ã€‚
        é€™æ˜¯ä¸€å€‹æ¨¡æ¿æ–¹æ³•ï¼Œå®ƒä»¥å›ºå®šçš„é †åºèª¿ç”¨å„å€‹æ­¥é©Ÿã€‚
        """
        self.logger.info(f"--- é–‹å§‹åŸ·è¡Œåˆ†ææµç¨‹ï¼š{self.analyzer_name} ---")
        try:
            # ç¬¬ä¸€æ­¥ï¼šè¼‰å…¥æ•¸æ“š
            self.logger.info("æ­¥é©Ÿ 1/3ï¼šæ­£åœ¨è¼‰å…¥æ•¸æ“š...")
            source_data = self._load_data()
            self.logger.info(f"æ•¸æ“šè¼‰å…¥æˆåŠŸï¼Œå…± {len(source_data)} ç­†è¨˜éŒ„ã€‚")

            # ç¬¬äºŒæ­¥ï¼šåŸ·è¡Œåˆ†æ
            self.logger.info("æ­¥é©Ÿ 2/3ï¼šæ­£åœ¨åŸ·è¡Œæ ¸å¿ƒåˆ†æ...")
            analysis_results = self._perform_analysis(source_data)
            self.logger.info("æ ¸å¿ƒåˆ†æåŸ·è¡Œå®Œç•¢ã€‚")

            # ç¬¬ä¸‰æ­¥ï¼šä¿å­˜çµæœ
            self.logger.info("æ­¥é©Ÿ 3/3ï¼šæ­£åœ¨ä¿å­˜çµæœ...")
            self._save_results(analysis_results)
            self.logger.info("çµæœä¿å­˜æˆåŠŸã€‚")

        except Exception as e:
            self.logger.error(
                f"åˆ†ææµç¨‹ '{self.analyzer_name}' ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼š{e}", exc_info=True
            )
            # å¯ä»¥åœ¨æ­¤è™•æ·»åŠ å¤±æ•—é€šçŸ¥ç­‰é‚è¼¯
            raise  # é‡æ–°æ‹‹å‡ºç•°å¸¸ï¼Œè®“ä¸Šå±¤èª¿ç”¨è€…çŸ¥é“ç™¼ç”Ÿäº†å•é¡Œ
        finally:
            self.logger.info(f"--- åˆ†ææµç¨‹åŸ·è¡Œå®Œç•¢ï¼š{self.analyzer_name} ---")
# æª”æ¡ˆ: src/core/context.py
import os

import aiosqlite

from prometheus.core.events.event_store import PersistentEventStream


class AppContext:
    _instance = None

    def __init__(self, db_path: str = "output/results.sqlite", config_path: str = "config.yml"):
        self.db_path = db_path
        from prometheus.core.config import ConfigManager
        self.config = ConfigManager(config_path=config_path)._config
        self.conn = None
        self.event_stream: PersistentEventStream | None = None

    async def __aenter__(self):
        db_dir = os.path.dirname(self.db_path)
        if db_dir:
            os.makedirs(db_dir, exist_ok=True)

        self.conn = await aiosqlite.connect(self.db_path)
        # å•Ÿç”¨ WAL æ¨¡å¼ä»¥ç²å¾—æ›´å¥½çš„ä¸¦ç™¼æ€§èƒ½
        await self.conn.execute("PRAGMA journal_mode=WAL;")

        # åˆå§‹åŒ–äº‹ä»¶æµ
        self.event_stream = PersistentEventStream(self.conn)
        await self.event_stream.initialize()

        return self

    @classmethod
    def get_instance(cls, **kwargs):
        if cls._instance is None:
            if 'config_path' not in kwargs:
                kwargs['config_path'] = 'config.yml'
            cls._instance = cls(config_path=kwargs.get('config_path', 'config.yml'))
        return cls._instance

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.conn:
            await self.conn.close()
# -*- coding: utf-8 -*-
"""
æ ¸å¿ƒæ•¸æ“šå®¢æˆ¶ç«¯ï¼šè¯æº–æœƒç¶“æ¿Ÿæ•¸æ“šåº« (FRED) (v2.1 - å¿«å–èˆ‡é‡‘é‘°ç®¡ç†å‡ç´šç‰ˆ)
"""

from typing import Any, Optional

import pandas as pd

from fredapi import Fred as FredAPILib

from prometheus.core.config import get_fred_api_key
from prometheus.core.logging.log_manager import LogManager

from .base import BaseAPIClient

logger = LogManager.get_instance().get_logger("FredClient")


class FredClient(BaseAPIClient):
    """
    ç”¨æ–¼å¾ FRED API ç²å–ç¶“æ¿Ÿæ•¸æ“šçš„å®¢æˆ¶ç«¯ã€‚
    ä½¿ç”¨å®˜æ–¹ fredapi å‡½å¼åº«é€²è¡Œæ•¸æ“šç²å–ã€‚
    """

    def __init__(
        self, api_key: Optional[str] = None, session: Optional[Any] = None
    ):  # æ·»åŠ  session ä»¥ä¾¿æ¸¬è©¦æ™‚å‚³å…¥
        """
        åˆå§‹åŒ– FredClientã€‚

        Args:
            api_key (Optional[str]): è¦ä½¿ç”¨çš„ FRED API é‡‘é‘°ã€‚
                                     å¦‚æœæä¾›ï¼Œå‰‡ä½¿ç”¨æ­¤é‡‘é‘°ã€‚
                                     å¦‚æœç‚º Noneï¼Œå‰‡å˜—è©¦å¾ config.yml è®€å–ã€‚
            session (Optional[Any]): requests session ç‰©ä»¶ï¼Œä¸»è¦ç”¨æ–¼æ¸¬è©¦æ™‚æ³¨å…¥ mock sessionã€‚
                                     fredapi å‡½å¼åº«æœ¬èº«ä¸ç›´æ¥ä½¿ç”¨æ­¤ sessionï¼Œä½† BaseAPIClient å¯èƒ½æœƒã€‚
        """
        final_api_key: Optional[str] = None
        if api_key:
            final_api_key = api_key
            logger.info("åˆå§‹åŒ–æ™‚åµæ¸¬åˆ°ç›´æ¥å‚³å…¥çš„ API é‡‘é‘°ã€‚")
        else:
            try:
                logger.debug("åˆå§‹åŒ–æ™‚æœªç›´æ¥å‚³å…¥ API é‡‘é‘°ï¼Œå˜—è©¦å¾è¨­å®šæª”ç²å–...")
                final_api_key = get_fred_api_key()
            except ValueError as e:
                logger.error(f"ç„¡æ³•åˆå§‹åŒ– FredClient: {e}", exc_info=True)
                raise ValueError(f"FredClient åˆå§‹åŒ–å¤±æ•—: {e}") from e

        if not final_api_key:
            error_msg = "FredClient åˆå§‹åŒ–å¤±æ•—ï¼šAPI é‡‘é‘°æ—¢æœªç›´æ¥æä¾›ï¼Œä¹Ÿç„¡æ³•å¾è¨­å®šæª”ä¸­ç²å–ã€‚"
            logger.error(error_msg)
            raise ValueError(error_msg)

        super().__init__(
            api_key=final_api_key, base_url="https://api.stlouisfed.org/fred"
        )

        if session:
            self._session = session
            logger.info("å·²ä½¿ç”¨å‚³å…¥çš„ session ç‰©ä»¶ã€‚")

        self._fred_official_client = FredAPILib(api_key=self.api_key)
        self._emergency_cache = {}
        logger.info(f"{self.__class__.__name__} åˆå§‹åŒ–æˆåŠŸã€‚FredAPILib å°‡è‡ªè¡Œç®¡ç†å…¶ç¶²è·¯è«‹æ±‚ã€‚å·²å•Ÿç”¨æ‡‰æ€¥å¿«å–ã€‚")

    def fetch_data(self, symbol: str, **kwargs: Any) -> pd.DataFrame:
        """
        å¾ FRED ç²å–å–®å€‹æ™‚é–“åºåˆ—æ•¸æ“šã€‚
        """
        logger.info(f"æ­£åœ¨ç²å–æŒ‡æ¨™ {symbol}...")
        force_refresh = kwargs.get("force_refresh", False)
        cache_key_params = tuple(
            sorted((k, v) for k, v in kwargs.items() if k != "force_refresh")
        )
        cache_key = (symbol, cache_key_params)

        if not force_refresh and cache_key in self._emergency_cache:
            logger.debug(f"ä½¿ç”¨æ‡‰æ€¥å¿«å–ç²å–æŒ‡æ¨™ {symbol} åŠåƒæ•¸ {cache_key_params}...")
            return self._emergency_cache[cache_key].copy()

        fred_params = {
            k: v
            for k, v in kwargs.items()
            if k in ["observation_start", "observation_end", "realtime_start", "realtime_end", "limit", "offset", "sort_order", "aggregation_method", "frequency", "units"]
        }

        with self._get_request_context(force_refresh=force_refresh):
            if force_refresh and cache_key in self._emergency_cache:
                logger.debug(f"æ‡‰æ€¥å¿«å–å›  force_refresh è€Œæ¸…é™¤ {symbol} / {cache_key_params}ã€‚")
                del self._emergency_cache[cache_key]

            try:
                logger.debug(f"æ­£åœ¨é€é FredAPILib è«‹æ±‚ {symbol}...")
                series_data = self._fred_official_client.get_series(series_id=symbol, **fred_params)
            except Exception as e:
                logger.error(f"ä½¿ç”¨ fredapi ç²å– {symbol} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
                return pd.DataFrame(columns=["Date", symbol]).set_index("Date")

        if not isinstance(series_data, pd.Series):
            logger.warning(f"å¾ FRED ç²å–çš„æŒ‡æ¨™ {symbol} æ•¸æ“šé¡å‹ä¸æ˜¯ pd.Seriesï¼Œè€Œæ˜¯ {type(series_data)}ã€‚")
            return pd.DataFrame(columns=["Date", symbol]).set_index("Date")

        if series_data.empty:
            logger.warning(f"å¾ FRED ç²å–çš„æŒ‡æ¨™ {symbol} æ•¸æ“šç‚ºç©ºã€‚")
            return pd.DataFrame(columns=["Date", symbol]).set_index("Date")

        df = series_data.to_frame(name=symbol)
        df.index.name = "Date"

        if df.empty or (symbol in df and df[symbol].isnull().all()):
            logger.warning(f"ç²å–çš„æŒ‡æ¨™ {symbol} æ•¸æ“šè½‰æ›å¾Œç„¡æ•ˆæˆ–å…¨ç‚ºç©ºå€¼ã€‚")
            return pd.DataFrame(columns=["Date", symbol]).set_index("Date")

        self._emergency_cache[cache_key] = df.copy()
        logger.debug(f"æˆåŠŸç²å–ä¸¦å·²å­˜å…¥æ‡‰æ€¥å¿«å– {len(df)} ç­† {symbol} / {cache_key_params} æ•¸æ“šã€‚")
        return df

    def close_session(self):
        """
        é—œé–‰ç”± BaseAPIClient ç®¡ç†çš„ requests sessionã€‚
        """
        super().close_session()
        logger.info("åŸºç¤ session (å¦‚æœå·²åˆå§‹åŒ–) å·²é—œé–‰ã€‚")


if __name__ == "__main__":
    print("--- FredClient é‡‘é‘°èˆ‡å¿«å–å‡ç´šå¾Œæ¸¬è©¦ ---")
    print("è«‹ç¢ºä¿æ‚¨çš„ config.yml ä¸­å·²å¡«å¯«æœ‰æ•ˆçš„ FRED API Keyã€‚")

    client: Optional[FredClient] = None
    try:
        client = FredClient()

        test_series_id = "DGS10"  # 10å¹´æœŸå…¬å‚µæ®–åˆ©ç‡
        test_params_initial = {
            "observation_start": "2023-01-01",
            "observation_end": "2023-01-10",
        }

        print(f"\n--- æ¸¬è©¦ç²å– {test_series_id} (ç¬¬ä¸€æ¬¡, æ‡‰å¯¦éš›è«‹æ±‚) ---")
        data_first = client.fetch_data(test_series_id, **test_params_initial)
        if not data_first.empty:
            print(f"{test_series_id} æ•¸æ“šç¯„ä¾‹ (ç¬¬ä¸€æ¬¡):")
            print(data_first.tail(3))
        else:
            print(f"ç„¡æ³•ç²å– {test_series_id} æ•¸æ“š (ç¬¬ä¸€æ¬¡)ã€‚")

        # ç”±æ–¼ fredapi ä¸ä½¿ç”¨æˆ‘å€‘çš„ requests-cacheï¼Œé‡è¤‡è«‹æ±‚é€šå¸¸æœƒå†æ¬¡å‘½ä¸­ APIã€‚
        # BaseAPIClient çš„å¿«å–ä¸Šä¸‹æ–‡åœ¨é€™è£¡ä¸»è¦æ˜¯æ—¥èªŒä½œç”¨å’Œæ¦‚å¿µä¸Šçš„ä¸€è‡´æ€§ã€‚
        # è‹¥è¦æ¸¬è©¦ fredapi è‡ªèº«çš„æ½›åœ¨å¿«å–æˆ–é¿å…é‡è¤‡ API å‘¼å«ï¼Œéœ€æ›´è¤‡é›œçš„ mockã€‚
        print(f"\n--- æ¸¬è©¦ç²å– {test_series_id} (ç¬¬äºŒæ¬¡, åƒæ•¸ç›¸åŒ) ---")
        data_second = client.fetch_data(test_series_id, **test_params_initial)
        if not data_second.empty:
            print(f"{test_series_id} æ•¸æ“šç¯„ä¾‹ (ç¬¬äºŒæ¬¡):")
            print(data_second.tail(3))
            if data_first.equals(data_second):
                print("INFO: ç¬¬äºŒæ¬¡ç²å–æ•¸æ“šèˆ‡ç¬¬ä¸€æ¬¡ä¸€è‡´ã€‚")
            else:
                print("WARNING: ç¬¬äºŒæ¬¡ç²å–æ•¸æ“šèˆ‡ç¬¬ä¸€æ¬¡ä¸ä¸€è‡´ã€‚")
        else:
            print(f"ç„¡æ³•ç²å– {test_series_id} æ•¸æ“š (ç¬¬äºŒæ¬¡)ã€‚")

        print(f"\n--- æ¸¬è©¦ç²å– {test_series_id} (å¼·åˆ¶åˆ·æ–°, æ„åœ–) ---")
        data_refresh = client.fetch_data(
            test_series_id, force_refresh=True, **test_params_initial
        )
        if not data_refresh.empty:
            print(f"{test_series_id} æ•¸æ“šç¯„ä¾‹ (å¼·åˆ¶åˆ·æ–°):")
            print(data_refresh.tail(3))
        else:
            print(f"ç„¡æ³•ç²å– {test_series_id} æ•¸æ“š (å¼·åˆ¶åˆ·æ–°)ã€‚")

        # æ¸¬è©¦ä¸€å€‹å¯èƒ½ä¸å­˜åœ¨çš„æŒ‡æ¨™
        print("\n--- æ¸¬è©¦ç²å–ä¸å­˜åœ¨çš„æŒ‡æ¨™ (FAKEID123) ---")
        fake_data = client.fetch_data("FAKEID123")
        if fake_data.empty:
            print("æˆåŠŸè™•ç†ä¸å­˜åœ¨çš„æŒ‡æ¨™ FAKEID123ï¼Œè¿”å›ç©º DataFrameã€‚")
        else:
            print("éŒ¯èª¤ï¼šç²å–ä¸å­˜åœ¨æŒ‡æ¨™ FAKEID123 æ™‚æœªè¿”å›ç©º DataFrameã€‚")

    except ValueError as ve:  # ä¾‹å¦‚é‡‘é‘°æœªè¨­å®š
        print(f"\næ¸¬è©¦éç¨‹ä¸­ç™¼ç”Ÿè¨­å®šéŒ¯èª¤: {ve}")
    except Exception as e:
        print(f"\næ¸¬è©¦éç¨‹ä¸­ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
        import traceback

        traceback.print_exc()
    finally:
        if client:
            client.close_session()

    print("\n--- FredClient æ¸¬è©¦çµæŸ ---")
# core/clients/nyfed.py
# æ­¤æ¨¡çµ„åŒ…å«å¾ç´ç´„è¯å„² (NY Fed) ä¸‹è¼‰å’Œè§£æä¸€ç´šäº¤æ˜“å•†æŒæœ‰é‡æ•¸æ“šçš„å®¢æˆ¶ç«¯é‚è¼¯ã€‚

import traceback
from io import BytesIO
from typing import Any, Dict, List, Optional

import pandas as pd
import requests

from .base import BaseAPIClient
from prometheus.core.logging.log_manager import LogManager

logger = LogManager.get_instance().get_logger("NYFedClient")

# NY Fed API URLs å’Œè§£æè¨­å®š (ä¿æŒä¸è®Š)
NYFED_DATA_CONFIGS: List[Dict[str, Any]] = [
    {
        "url": "https://markets.newyorkfed.org/api/pd/get/SBN2024/timeseries/PDPOSGSC-L2_PDPOSGSC-G2L3_PDPOSGSC-G3L6_PDPOSGSC-G6L7_PDPOSGSC-G7L11_PDPOSGSC-G11L21_PDPOSGSC-G21.xlsx",
        "type": "SBN",
        "sheet_name": 0,
        "header_row": 0,
        "date_column_names": ["AS OF DATE"],
        "value_column_name": "VALUE (MILLIONS)",
        "notes": "SBN2024 - PDPOSGSC series",
    },
    {
        "url": "https://markets.newyorkfed.org/api/pd/get/SBN2022/timeseries/PDPOSGSC-L2_PDPOSGSC-G2L3_PDPOSGSC-G3L6_PDPOSGSC-G6L7_PDPOSGSC-G7L11_PDPOSGSC-G11L21_PDPOSGSC-G21.xlsx",
        "type": "SBN",
        "sheet_name": 0,
        "header_row": 0,
        "date_column_names": ["AS OF DATE"],
        "value_column_name": "VALUE (MILLIONS)",
        "notes": "SBN2022 - PDPOSGSC series",
    },
    {
        "url": "https://markets.newyorkfed.org/api/pd/get/SBN2015/timeseries/PDPOSGSC-L2_PDPOSGSC-G2L3_PDPOSGSC-G3L6_PDPOSGSC-G6L7_PDPOSGSC-G7L11_PDPOSGSC-G11.xlsx",
        "type": "SBN",
        "sheet_name": 0,
        "header_row": 0,
        "date_column_names": ["AS OF DATE"],
        "value_column_name": "VALUE (MILLIONS)",
        "notes": "SBN2015 - PDPOSGSC series (G11 çµå°¾)",
    },
    {
        "url": "https://markets.newyorkfed.org/api/pd/get/SBN2013/timeseries/PDPOSGSC-L2_PDPOSGSC-G2L3_PDPOSGSC-G3L6_PDPOSGSC-G6L7_PDPOSGSC-G7L11_PDPOSGSC-G11.xlsx",
        "type": "SBN",
        "sheet_name": 0,
        "header_row": 0,
        "date_column_names": ["AS OF DATE"],
        "value_column_name": "VALUE (MILLIONS)",
        "notes": "SBN2013 - PDPOSGSC series",
    },
    {
        "url": "https://markets.newyorkfed.org/api/pd/get/SBP2013/timeseries/PDPUSGCS3LNOP_PDPUSGCS36NOP_PDPUSGCS611NOP_PDPUSGCSM11NOP.xlsx",
        "type": "SBP",
        "sheet_name": 0,
        "header_row": 0,
        "date_column_names": ["AS OF DATE"],
        "value_column_name": "VALUE (MILLIONS)",
        "cols_to_sum_if_sbp": [
            "PDPUSGCS3LNOP",
            "PDPUSGCS36NOP",
            "PDPUSGCS611NOP",
            "PDPUSGCSM11NOP",
        ],
        "notes": "SBP2013 - åŠ ç¸½æŒ‡å®šæ¬„ä½",
    },
    {
        "url": "https://markets.newyorkfed.org/api/pd/get/SBP2001/timeseries/PDPUSGCS5LNOP_PDPUSGCS5MNOP.xlsx",
        "type": "SBP",
        "sheet_name": 0,
        "header_row": 0,
        "date_column_names": ["AS OF DATE"],
        "value_column_name": "VALUE (MILLIONS)",
        "cols_to_sum_if_sbp": ["PDPUSGCS5LNOP", "PDPUSGCS5MNOP"],
        "notes": "SBP2001 - åŠ ç¸½æŒ‡å®šæ¬„ä½",
    },
]


class NYFedClient(BaseAPIClient):  # é¡åå¾ NYFedAPIClient æ”¹ç‚º NYFedClient
    """
    ç”¨æ–¼å¾ç´ç´„è¯å„² (NY Fed) API ä¸‹è¼‰å’Œè§£æä¸€ç´šäº¤æ˜“å•†æŒæœ‰é‡æ•¸æ“šçš„å®¢æˆ¶ç«¯ã€‚
    æ­¤å®¢æˆ¶ç«¯ä¸ä½¿ç”¨å‚³çµ±çš„ API Key æˆ– JSON APIï¼Œè€Œæ˜¯ä¸‹è¼‰ Excel æª”æ¡ˆã€‚
    """

    def __init__(self, data_configs: Optional[List[Dict[str, Any]]] = None):
        """
        åˆå§‹åŒ– NYFedClientã€‚

        Args:
            data_configs (Optional[List[Dict[str, Any]]]):
                ç”¨æ–¼æŒ‡å®šä¸‹è¼‰ä¾†æºå’Œè§£ææ–¹å¼çš„é…ç½®åˆ—è¡¨ã€‚
                å¦‚æœæœªæä¾›ï¼Œå‰‡ä½¿ç”¨æ¨¡çµ„ä¸­å®šç¾©çš„é è¨­ NYFED_DATA_CONFIGSã€‚
        """
        # NYFed ä¸ä½¿ç”¨ API Key å’Œæ¨™æº–çš„ base_url æ¨¡å¼ï¼Œä½†ä»èª¿ç”¨çˆ¶é¡æ§‹é€ å‡½æ•¸
        super().__init__(api_key=None, base_url=None)
        self.data_configs = data_configs or NYFED_DATA_CONFIGS
        logger.info(f"NYFedClient åˆå§‹åŒ–å®Œæˆï¼Œå°‡ä½¿ç”¨ {len(self.data_configs)} å€‹æ•¸æ“šæºé…ç½®ã€‚")

    def _download_excel_to_dataframe(
        self, config: Dict[str, Any]
    ) -> Optional[pd.DataFrame]:
        """
        å¾æŒ‡å®šçš„ API URL ä¸‹è¼‰ Excel æª”æ¡ˆä¸¦è®€å–ç‰¹å®š sheet åˆ° DataFrameã€‚
        """
        url = config["url"]
        sheet_name = config.get("sheet_name", 0)
        header_row = config.get("header_row", 0)

        logger.debug(f"æ­£åœ¨å¾ {url} ä¸‹è¼‰ Excel æ•¸æ“š (Sheet: {sheet_name}, Header: {header_row})...")
        try:
            response: requests.Response = self._session.get(url, timeout=60)
            response.raise_for_status()

            excel_file = BytesIO(response.content)
            df = pd.read_excel(excel_file, sheet_name=sheet_name, header=header_row, engine="openpyxl")

            df.columns = [str(col).strip().upper().replace("\n", " ").replace("\r", " ").replace("  ", " ") for col in df.columns]

            logger.debug(f"æˆåŠŸå¾ {url} ä¸‹è¼‰ä¸¦è®€å–äº† {len(df)} è¡Œæ•¸æ“šã€‚")
            return df
        except requests.exceptions.HTTPError as http_err:
            logger.error(f"ä¸‹è¼‰ Excel æª”æ¡ˆ {url} æ™‚ç™¼ç”Ÿ HTTP éŒ¯èª¤: {http_err}", exc_info=True)
            return None
        except requests.exceptions.RequestException as req_err:
            logger.error(f"ä¸‹è¼‰ Excel æª”æ¡ˆ {url} æ™‚ç™¼ç”Ÿç¶²è·¯éŒ¯èª¤: {req_err}", exc_info=True)
            return None
        except Exception as e:
            logger.error(f"è™•ç†ä¾†è‡ª {url} çš„ Excel æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return None

    def _parse_dealer_positions(
        self, df_raw: pd.DataFrame, config: Dict[str, Any]
    ) -> pd.DataFrame:
        """
        æ ¹æ“šè¨­å®šè§£æå¾å–®å€‹ Excel æª”æ¡ˆè®€å–çš„ä¸€ç´šäº¤æ˜“å•†æŒæœ‰é‡æ•¸æ“šã€‚
        """
        date_col_name = config["date_column_names"][0]
        if date_col_name not in df_raw.columns:
            logger.error(f"åœ¨ä¾†æº {config['url']} çš„æ•¸æ“šä¸­æ‰¾ä¸åˆ°é æœŸæ—¥æœŸæ¬„ä½ '{date_col_name}'ã€‚å¯ç”¨æ¬„ä½: {df_raw.columns.tolist()}")
            return pd.DataFrame(columns=["Date", "Total_Positions"])

        df = df_raw.copy()
        df.rename(columns={date_col_name: "Date"}, inplace=True)
        df["Date"] = pd.to_datetime(df["Date"], errors="coerce")

        value_col_name = config.get("value_column_name", "VALUE (MILLIONS)")
        required_core_cols = [value_col_name]
        if config["type"] == "SBP":
            required_core_cols.append("TIME SERIES")

        missing_core_cols = [col for col in required_core_cols if col not in df.columns]
        if missing_core_cols:
            logger.error(f"é¡å‹ {config['type']} çš„æ•¸æ“š ({config['url']}) ç¼ºå°‘æ ¸å¿ƒæ¬„ä½: {missing_core_cols}ã€‚å¯ç”¨æ¬„ä½: {df.columns.tolist()}")
            return pd.DataFrame(columns=["Date", "Total_Positions"])

        df[value_col_name] = pd.to_numeric(df[value_col_name], errors="coerce")

        if config["type"] == "SBP":
            cols_to_sum = config.get("cols_to_sum_if_sbp")
            if not cols_to_sum:
                logger.error(f"SBP é¡å‹æ•¸æ“š ({config['url']}) æœªåœ¨é…ç½®ä¸­æä¾› 'cols_to_sum_if_sbp' åˆ—è¡¨ã€‚")
                return pd.DataFrame(columns=["Date", "Total_Positions"])
            target_series_codes = [code.upper() for code in cols_to_sum]
            df_filtered = df[df["TIME SERIES"].isin(target_series_codes)]
            if df_filtered.empty:
                logger.warning(f"SBP é¡å‹æ•¸æ“š ({config['url']}) åœ¨ç¯©é¸ç›®æ¨™ TIME SERIES {target_series_codes} å¾Œç‚ºç©ºã€‚")
                return pd.DataFrame(columns=["Date", "Total_Positions"])
            summed_df = df_filtered.groupby("Date")[value_col_name].sum().reset_index()
            summed_df.rename(columns={value_col_name: "Total_Positions"}, inplace=True)
            df_processed = summed_df
        elif config["type"] == "SBN":
            summed_df = df.groupby("Date")[value_col_name].sum().reset_index()
            summed_df.rename(columns={value_col_name: "Total_Positions"}, inplace=True)
            df_processed = summed_df
        else:
            logger.error(f"æœªçŸ¥çš„æ•¸æ“šé¡å‹ '{config['type']}' for url {config['url']}")
            return pd.DataFrame(columns=["Date", "Total_Positions"])

        df_processed.dropna(subset=["Date", "Total_Positions"], inplace=True)
        if df_processed.empty:
            logger.warning(f"è™•ç†å¾Œ DataFrame ({config['url']}) ç‚ºç©ºã€‚")
            return pd.DataFrame(columns=["Date", "Total_Positions"])

        df_processed["Total_Positions"] = df_processed["Total_Positions"] * 1_000_000
        if df_processed["Date"].dt.tz is not None:
            df_processed["Date"] = df_processed["Date"].dt.tz_localize(None)
        return df_processed[["Date", "Total_Positions"]].sort_values(by="Date").reset_index(drop=True)

    def fetch_data(self, symbol: str = "", **kwargs) -> pd.DataFrame:
        """
        å¾ NY Fed API ç²å–æ‰€æœ‰è¨­å®šçš„ä¸€ç´šäº¤æ˜“å•†æŒæœ‰é‡æ•¸æ“šï¼Œä¸¦é€²è¡Œåˆä½µå’Œè™•ç†ã€‚
        """
        force_refresh = kwargs.get("force_refresh", False)

        if symbol:
            logger.debug(f"æ¥æ”¶åˆ° symbol='{symbol}'ï¼Œä½†æ­¤åƒæ•¸ç•¶å‰è¢«å¿½ç•¥ã€‚")

        all_data_frames: List[pd.DataFrame] = []
        logger.info(f"é–‹å§‹ç²å–æ‰€æœ‰ä¸€ç´šäº¤æ˜“å•†æ•¸æ“š (å¼·åˆ¶åˆ·æ–°={force_refresh})...")

        with self._get_request_context(force_refresh=force_refresh):
            for config in self.data_configs:
                logger.debug(f"è™•ç†é…ç½®: {config.get('notes', config['url'])}")
                df_raw = self._download_excel_to_dataframe(config)
                if df_raw is not None and not df_raw.empty:
                    df_parsed = self._parse_dealer_positions(df_raw, config)
                    if not df_parsed.empty:
                        all_data_frames.append(df_parsed)
                        logger.debug(f"æˆåŠŸè§£æä¾†è‡ª {config['url']} çš„ {len(df_parsed)} ç­†æœ‰æ•ˆæ•¸æ“šã€‚")
                    else:
                        logger.warning(f"è§£æä¾†è‡ª {config['url']} çš„æ•¸æ“šå¾Œç„¡æœ‰æ•ˆè¨˜éŒ„ã€‚")
                else:
                    logger.warning(f"ä¸‹è¼‰æˆ–è®€å–ä¾†è‡ª {config['url']} çš„æ•¸æ“šå¤±æ•—æˆ–åŸå§‹æ•¸æ“šç‚ºç©ºã€‚")

        if not all_data_frames:
            logger.error("æœªèƒ½å¾ä»»ä½• NY Fed ä¾†æºæˆåŠŸç²å–å’Œè§£æä¸€ç´šäº¤æ˜“å•†æ•¸æ“šã€‚")
            return pd.DataFrame(columns=["Date", "Total_Positions"])

        combined_df = pd.concat(all_data_frames, ignore_index=True)
        combined_df.sort_values(by="Date", inplace=True)
        combined_df.drop_duplicates(subset=["Date"], keep="first", inplace=True)
        combined_df.reset_index(drop=True, inplace=True)

        logger.info(f"æˆåŠŸåˆä½µæ‰€æœ‰ NY Fed ä¸€ç´šäº¤æ˜“å•†æ•¸æ“šï¼Œæœ€çµ‚å…± {len(combined_df)} ç­†å”¯ä¸€æ—¥æœŸè¨˜éŒ„ã€‚")
        return combined_df


# ç¯„ä¾‹ä½¿ç”¨ (ä¸»è¦ç”¨æ–¼é–‹ç™¼æ™‚æ¸¬è©¦)
if __name__ == "__main__":
    print("--- NYFedClient å¿«å–æ•´åˆå¾Œæ¸¬è©¦ (ç›´æ¥åŸ·è¡Œ core/clients/nyfed.py) ---")
    client = NYFedClient()
    try:
        print("\n--- åŸ·è¡Œç¬¬ä¸€æ¬¡ (æ‡‰æœƒä¸‹è¼‰æ‰€æœ‰æª”æ¡ˆ) ---")
        data_first_run = client.fetch_data()
        if not data_first_run.empty:
            print(f"ç¬¬ä¸€æ¬¡åŸ·è¡ŒæˆåŠŸï¼Œç²å– {len(data_first_run)} ç­†æ•¸æ“šã€‚")
            # æª¢æŸ¥æ˜¯å¦æœ‰å¿«å–ç›¸é—œçš„æ—¥èªŒ (åœ¨ _download_excel_to_dataframe æˆ– BaseAPIClient ä¸­)
            # æ³¨æ„ï¼šç”±æ–¼ NYFedClient ä¸‹è¼‰å¤šå€‹æª”æ¡ˆï¼Œæ­¤è™•çš„ from_cache å¯èƒ½ä¸æ˜é¡¯
            # çœŸæ­£çš„å¿«å–æ•ˆæœé«”ç¾åœ¨ç¬¬äºŒæ¬¡é‹è¡Œæ™‚ï¼Œè«‹æ±‚ä¸æ‡‰å¯¦éš›ç™¼å‡º

        print("\n--- åŸ·è¡Œç¬¬äºŒæ¬¡ (æ‡‰å¾å¿«å–è®€å–æ‰€æœ‰æª”æ¡ˆ) ---")
        data_second_run = client.fetch_data()
        if not data_second_run.empty:
            print(f"ç¬¬äºŒæ¬¡åŸ·è¡ŒæˆåŠŸï¼Œç²å– {len(data_second_run)} ç­†æ•¸æ“šã€‚")
            # é€™è£¡éœ€è¦ä¾è³´ BaseAPIClient ä¸­ get_cached_session çš„æ—¥èªŒ
            # æˆ– _download_excel_to_dataframe ä¸­ response.from_cache (å¦‚æœé©ç”¨)
            # ä¾†ç¢ºèªæ˜¯å¦å¾å¿«å–è®€å–ã€‚

        print("\n--- åŸ·è¡Œç¬¬ä¸‰æ¬¡ (å¼·åˆ¶åˆ·æ–°ï¼Œæ‡‰é‡æ–°ä¸‹è¼‰æ‰€æœ‰æª”æ¡ˆ) ---")
        data_third_run = client.fetch_data(force_refresh=True)
        if not data_third_run.empty:
            print(f"ç¬¬ä¸‰æ¬¡åŸ·è¡Œ (å¼·åˆ¶åˆ·æ–°) æˆåŠŸï¼Œç²å– {len(data_third_run)} ç­†æ•¸æ“šã€‚")

        # åŸºæœ¬çš„å¥å…¨æ€§æª¢æŸ¥
        if not (
            data_first_run.equals(data_second_run)
            and data_first_run.equals(data_third_run)
        ):
            print("\nè­¦å‘Šï¼šä¸åŒåŸ·è¡Œä¹‹é–“çš„æ•¸æ“šä¸ä¸€è‡´ï¼Œè«‹æª¢æŸ¥ï¼")
            print(f"ç¬¬ä¸€æ¬¡ vs ç¬¬äºŒæ¬¡æ˜¯å¦ç›¸ç­‰: {data_first_run.equals(data_second_run)}")
            print(f"ç¬¬ä¸€æ¬¡ vs ç¬¬ä¸‰æ¬¡æ˜¯å¦ç›¸ç­‰: {data_first_run.equals(data_third_run)}")
        else:
            print("\næ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥é€šéã€‚")

        if not data_first_run.empty:
            print(
                f"\næœ€çµ‚åˆä½µçš„ä¸€ç´šäº¤æ˜“å•†æŒæœ‰é‡æ•¸æ“šç¯„ä¾‹ (å…± {len(data_first_run)} ç­†):"
            )
            print("æœ€æ—©çš„ 5 ç­†æ•¸æ“š:")
            print(data_first_run.head())
            print("\næœ€æ–°çš„ 5 ç­†æ•¸æ“š:")
            print(data_first_run.tail())
        else:
            print("éŒ¯èª¤ï¼šæœªèƒ½ç²å–ä»»ä½•ä¸€ç´šäº¤æ˜“å•†æŒæœ‰é‡æ•¸æ“šã€‚")

    except Exception as e:
        print(f"åŸ·è¡Œ NYFedClient æ¸¬è©¦æœŸé–“ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
        traceback.print_exc()
    finally:
        client.close_session()  # ç¢ºä¿é—œé–‰ session

    print("\n--- NYFedClient å¿«å–æ•´åˆå¾Œæ¸¬è©¦çµæŸ ---")
# core/clients/yfinance.py
# æ­¤æ¨¡çµ„åŒ…å«å¾ Yahoo Finance ä¸‹è¼‰å¸‚å ´æ•¸æ“šçš„å®¢æˆ¶ç«¯é‚è¼¯ã€‚

import traceback
from typing import (
    Any,
    List,
    cast,
)

import pandas as pd
import yfinance as yf

from .base import BaseAPIClient
from prometheus.core.logging.log_manager import LogManager

logger = LogManager.get_instance().get_logger("YFinanceClient")


class YFinanceClient(BaseAPIClient):
    """
    ç”¨æ–¼å¾ Yahoo Finance ä¸‹è¼‰å¸‚å ´æ•¸æ“šçš„å®¢æˆ¶ç«¯ã€‚
    æ­¤å®¢æˆ¶ç«¯ä½¿ç”¨ yfinance å¥—ä»¶ï¼Œä¸ç›´æ¥é€²è¡Œ HTTP è«‹æ±‚ï¼Œ
    å› æ­¤ä¸ä½¿ç”¨ BaseAPIClient çš„ _request æ–¹æ³•ã€‚
    """

    def __init__(self):
        """
        åˆå§‹åŒ– YFinanceClientã€‚
        Yahoo Finance ä¸éœ€è¦ API Key æˆ–ç‰¹å®šçš„ Base URL (ç”± yfinance å¥—ä»¶è™•ç†)ã€‚
        """
        super().__init__(api_key=None, base_url=None)
        logger.info("YFinanceClient åˆå§‹åŒ–å®Œæˆã€‚")

    async def fetch_data(
        self, symbol: str, **kwargs
    ) -> pd.DataFrame:
        """
        éåŒæ­¥åœ°å¾ Yahoo Finance æŠ“å–æŒ‡å®šå•†å“ä»£ç¢¼çš„ OHLCV æ•¸æ“šã€‚
        """
        import asyncio

        start_date = kwargs.get("start_date")
        end_date = kwargs.get("end_date")
        period = kwargs.get("period")

        if not period and not (start_date and end_date):
            raise ValueError("å¿…é ˆæä¾› 'period' æˆ– 'start_date' èˆ‡ 'end_date' å…¶ä¸­ä¹‹ä¸€ã€‚")

        history_params = {
            "start": start_date,
            "end": end_date,
            "auto_adjust": kwargs.get("auto_adjust", False),
            "interval": kwargs.get("interval", "1d"),
            "actions": kwargs.get("actions", False),
        }
        if period:
            history_params["period"] = period
            history_params.pop("start", None)
            history_params.pop("end", None)

        logger.info(f"é–‹å§‹æŠ“å–æ•¸æ“šï¼šå•†å“ {symbol}, åƒæ•¸: {history_params}")

        def _sync_fetch():
            try:
                ticker_obj: Any = yf.Ticker(symbol)
                history_params.pop("progress", None)
                hist_data: Any = ticker_obj.history(**history_params)

                if hist_data is None or hist_data.empty:
                    logger.warning(f"å•†å“ {symbol} ä½¿ç”¨åƒæ•¸ {history_params} æœªæ‰¾åˆ°æ•¸æ“šæˆ–è¿”å›ç‚ºç©ºã€‚")
                    return pd.DataFrame()

                hist_data = cast(pd.DataFrame, hist_data)
                hist_data.reset_index(inplace=True)
                hist_data["symbol"] = symbol

                date_col_name = "Datetime" if "Datetime" in hist_data.columns else "Date"
                if date_col_name not in hist_data.columns:
                    logger.warning(f"æœªæ‰¾åˆ°é æœŸçš„æ—¥æœŸæ¬„ä½ ('Date' æˆ– 'Datetime')ã€‚å¯ç”¨æ¬„ä½: {hist_data.columns.tolist()}")
                    return pd.DataFrame()

                hist_data[date_col_name] = pd.to_datetime(hist_data[date_col_name], utc=True)
                hist_data[date_col_name] = hist_data[date_col_name].dt.tz_convert(None)

                if date_col_name != "date":
                    hist_data.rename(columns={date_col_name: "date"}, inplace=True)

                rename_map = {"Adj Close": "Adj_Close"}
                final_df = hist_data.rename(columns=rename_map)
                final_df["date"] = pd.to_datetime(final_df["date"])

                required_cols = ["date", "symbol", "Open", "High", "Low", "Close", "Adj_Close", "Volume"]
                cols_to_keep = []
                missing_cols = []

                for col in required_cols:
                    if col in final_df.columns:
                        cols_to_keep.append(col)
                    elif col == "Adj_Close" and "Close" in final_df.columns and history_params.get("auto_adjust") is True:
                        final_df["Adj_Close"] = final_df["Close"]
                        cols_to_keep.append("Adj_Close")
                    elif col not in final_df.columns:
                        missing_cols.append(col)

                if missing_cols:
                    logger.warning(f"æŠ“å–çš„æ•¸æ“šä¸­ç¼ºå°‘ä»¥ä¸‹é æœŸæ¬„ä½: {missing_cols} (Symbol: {symbol})ã€‚")

                valid_cols_to_keep = [col for col in cols_to_keep if col in final_df.columns]
                if not valid_cols_to_keep:
                    logger.warning(f"æ²’æœ‰æœ‰æ•ˆçš„æ¬„ä½å¯ä¾›é¸æ“‡ (Symbol: {symbol})")
                    return pd.DataFrame()

                final_df = final_df[valid_cols_to_keep]

                logger.info(f"æˆåŠŸæŠ“å–ä¸¦è™•ç† {len(final_df)} ç­†æ•¸æ“šï¼Œå•†å“: {symbol}ã€‚")
                return final_df

            except Exception as e:
                logger.error(f"æŠ“å–æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤ (Symbol: {symbol})ï¼š{e}", exc_info=True)
                return pd.DataFrame()

        return await asyncio.to_thread(_sync_fetch)

    async def fetch_multiple_symbols_data(
        self, symbols: List[str], **kwargs
    ) -> pd.DataFrame:
        import asyncio

        if not isinstance(symbols, list) or not symbols:
            logger.error("symbols åƒæ•¸å¿…é ˆæ˜¯ä¸€å€‹éç©ºåˆ—è¡¨ã€‚")
            return pd.DataFrame()

        tasks = [self.fetch_data(symbol=s, **kwargs) for s in symbols]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        all_data_list = []
        for i, res in enumerate(results):
            if isinstance(res, pd.DataFrame) and not res.empty:
                all_data_list.append(res)
            elif isinstance(res, Exception):
                logger.error(
                    f"è™•ç†å•†å“ {symbols[i]} æ™‚ç™¼ç”ŸéŒ¯èª¤: {res}", exc_info=True
                )

        if not all_data_list:
            logger.info("æœªå¾ä»»ä½•æŒ‡å®šå•†å“æŠ“å–åˆ°æ•¸æ“šã€‚")
            return pd.DataFrame()

        combined_df = pd.concat(all_data_list, ignore_index=True)
        logger.info(
            f"æˆåŠŸåˆä½µ {len(combined_df)} ç­†ä¾†è‡ª {len(all_data_list)} å€‹å•†å“çš„æ•¸æ“šã€‚"
        )
        return combined_df

    def get_ticker(self, symbol: str) -> yf.Ticker:
        """
        ç²å–ä¸€å€‹ yfinance Ticker ç‰©ä»¶ã€‚
        """
        return yf.Ticker(symbol)

    def get_move_index(self, start_date: str, end_date: str) -> pd.Series:
        """å¾ yfinance ç²å– ICE BofA MOVE Index (^MOVE) çš„æ­·å²æ”¶ç›¤åƒ¹ã€‚"""
        logger.info(f"æ­£åœ¨ç²å– ^MOVE æŒ‡æ•¸æ•¸æ“šï¼Œæ—¥æœŸç¯„åœ: {start_date} è‡³ {end_date}")
        try:
            move_ticker = yf.Ticker("^MOVE")
            start_date_dt = pd.to_datetime(start_date)
            end_date_dt = pd.to_datetime(end_date)
            end_date_for_yf = (end_date_dt + pd.Timedelta(days=1)).strftime("%Y-%m-%d")
            start_date_for_yf = start_date_dt.strftime("%Y-%m-%d")
            history = move_ticker.history(start=start_date_for_yf, end=end_date_for_yf)

            if history.empty:
                logger.warning(f"^MOVE æŒ‡æ•¸åœ¨ {start_date_for_yf} è‡³ {end_date_for_yf} æœªè¿”å›ä»»ä½•æ•¸æ“šã€‚")
                return pd.Series(dtype="float64", name="Close")

            close_series = history["Close"]
            if not isinstance(close_series.index, pd.DatetimeIndex):
                close_series.index = pd.to_datetime(close_series.index)

            close_series = close_series[close_series.index <= end_date_dt]

            if close_series.empty:
                logger.warning(f"^MOVE æŒ‡æ•¸åœ¨ç¯©é¸æ—¥æœŸ ({start_date_dt.date()} è‡³ {end_date_dt.date()}) å¾Œæ•¸æ“šç‚ºç©ºã€‚")
                return pd.Series(dtype="float64", name="Close")

            logger.info(f"æˆåŠŸç²å– {len(close_series)} ç­† ^MOVE æŒ‡æ•¸æ•¸æ“šã€‚")
            return close_series
        except Exception as e:
            logger.error(f"ç²å– ^MOVE æŒ‡æ•¸æ™‚å¤±æ•—: {e}", exc_info=True)
            return pd.Series(dtype="float64", name="Close")


if __name__ == "__main__":
    print("--- YFinanceClient é‡æ§‹å¾Œæ¸¬è©¦ (ç›´æ¥åŸ·è¡Œ core/clients/yfinance.py) ---")
    try:
        client = YFinanceClient()
        print("YFinanceClient åˆå§‹åŒ–æˆåŠŸã€‚")

        print("\næ¸¬è©¦ç²å– AAPL æ•¸æ“š (2023-12-01 è‡³ 2023-12-05)...")
        aapl_data = client.fetch_data(
            symbol="AAPL", start_date="2023-12-01", end_date="2023-12-05"
        )
        if aapl_data is not None and not aapl_data.empty:
            print(f"æˆåŠŸç²å– AAPL æ•¸æ“š (å…± {len(aapl_data)} ç­†):")
            print(aapl_data.head())
        else:
            print("ç²å– AAPL æ•¸æ“šè¿”å›ç©º DataFrame æˆ– Noneã€‚")

        print("\næ¸¬è©¦ç²å– AAPL å’Œ MSFT æ•¸æ“š (æœ€è¿‘5å¤©, 1d é–“éš”)...")
        multi_data = client.fetch_multiple_symbols_data(
            symbols=["AAPL", "MSFT", "NONEXISTENTICKER"],
            period="5d",
            interval="1d",
        )
        if multi_data is not None and not multi_data.empty:
            print(f"æˆåŠŸç²å–å¤šå€‹å•†å“æ•¸æ“š (å…± {len(multi_data)} ç­†):")
            print(multi_data.head())
            print("...")
            print(multi_data.tail())
            print(f"æ•¸æ“šä¸­åŒ…å«çš„ Symbols: {multi_data['symbol'].unique()}")
        else:
            print("ç²å–å¤šå€‹å•†å“æ•¸æ“šè¿”å›ç©º DataFrame æˆ– Noneã€‚")

        print("\næ¸¬è©¦ç²å– ^GSPC æ•¸æ“š (æœ€è¿‘1å€‹æœˆ)...")
        gspc_data = client.fetch_data(symbol="^GSPC", period="1mo")
        if gspc_data is not None and not gspc_data.empty:
            print(f"æˆåŠŸç²å– ^GSPC æ•¸æ“š (æœ€è¿‘1å€‹æœˆï¼Œå…± {len(gspc_data)} ç­†):")
            print(gspc_data.head())
        else:
            print("ç²å– ^GSPC æ•¸æ“šè¿”å›ç©º DataFrame æˆ– Noneã€‚")

        print("\næ¸¬è©¦ç²å– SPY æ•¸æ“š (æœ€è¿‘1å¤©, 1m é–“éš”)...")
        spy_intraday = client.fetch_data(symbol="SPY", period="1d", interval="1m")
        if spy_intraday is not None and not spy_intraday.empty:
            print(f"æˆåŠŸç²å– SPY 1åˆ†é˜æ•¸æ“š (å…± {len(spy_intraday)} ç­†):")
            assert "Date" in spy_intraday.columns
            assert "Datetime" not in spy_intraday.columns
            print(spy_intraday.head())
        else:
            print(
                "ç²å– SPY 1åˆ†é˜æ•¸æ“šè¿”å›ç©º DataFrame æˆ– None (å¯èƒ½æ˜¯å¸‚å ´æœªé–‹ç›¤æˆ–è¶…å‡º yfinance é™åˆ¶)ã€‚"
            )

    except Exception as e:
        print(f"åŸ·è¡Œ YFinanceClient æ¸¬è©¦æœŸé–“ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
        traceback.print_exc()

    print("--- YFinanceClient é‡æ§‹å¾Œæ¸¬è©¦çµæŸ ---")
from typing import Dict

from ..config import config
from .base import BaseAPIClient
from .fred import FredClient
from .taifex_db import TaifexDBClient
from .yfinance import YFinanceClient


class ClientFactory:
    _clients: Dict[str, BaseAPIClient] = {}

    @classmethod
    def get_client(cls, client_name: str) -> BaseAPIClient:
        if client_name not in cls._clients:
            if client_name == "fred":
                cls._clients[client_name] = FredClient(api_key=config.get("api_keys.fred"))
            elif client_name == "yfinance":
                cls._clients[client_name] = YFinanceClient()
            elif client_name == "taifex":
                cls._clients[client_name] = TaifexDBClient()
            elif client_name == "finmind":
                from .finmind import FinMindClient
                cls._clients[client_name] = FinMindClient(api_token=config.get("api_keys.finmind"))
            else:
                raise ValueError(f"Unknown client: {client_name}")
        return cls._clients[client_name]

    @classmethod
    def close_all(cls):
        for client in cls._clients.values():
            client.close_session()
        cls._clients = {}
# -*- coding: utf-8 -*-
"""
æ ¸å¿ƒå®¢æˆ¶ç«¯æ¨¡çµ„ï¼šåŸºç¤ API å®¢æˆ¶ç«¯ (v2.0 - å¿«å–æ³¨å…¥ç‰ˆ)

åŠŸèƒ½ï¼š
- ä½œç‚ºæ‰€æœ‰ç‰¹å®š API å®¢æˆ¶ç«¯ (å¦‚ FRED, NYFed) çš„çˆ¶é¡åˆ¥ã€‚
- **é—œéµå‡ç´š**: å…§å»ºä¸¦æ•´åˆäº†ä¾†è‡ª core.utils.caching çš„ä¸­å¤®å¿«å–å¼•æ“ã€‚
- ç‚ºæ‰€æœ‰å­é¡åˆ¥æä¾›çµ±ä¸€çš„ã€å…·å‚™æ°¸ä¹…å¿«å–å’Œæ‰‹å‹•åˆ·æ–°èƒ½åŠ›çš„ requests Sessionã€‚
"""

from contextlib import contextmanager
from typing import Iterator, Optional

import requests

from prometheus.core.utils.helpers import get_cached_session, temporary_disabled_cache
from prometheus.core.logging.log_manager import LogManager

logger = LogManager.get_instance().get_logger("BaseAPIClient")


class BaseAPIClient:
    """
    æ‰€æœ‰ API å®¢æˆ¶ç«¯çš„åŸºç¤é¡åˆ¥ï¼Œå…§å»ºäº†åŸºæ–¼ requests-cache çš„åŒæ­¥å¿«å–æ©Ÿåˆ¶ã€‚
    """

    def __init__(self, api_key: str = None, base_url: str = None):
        """
        åˆå§‹åŒ–åŸºç¤å®¢æˆ¶ç«¯ã€‚

        Args:
            api_key (str, optional): API é‡‘é‘° (å¦‚æœéœ€è¦)ã€‚
            base_url (str, optional): API çš„åŸºç¤ URLã€‚
        """
        self.api_key = api_key
        self.base_url = base_url
        self._session: requests.Session = get_cached_session()
        logger.info(f"{self.__class__.__name__} å·²åˆå§‹åŒ–ï¼Œä¸¦æ³¨å…¥äº†æ°¸ä¹…å¿«å– Sessionã€‚")

    @contextmanager
    def _get_request_context(self, force_refresh: bool = False) -> Iterator[None]:
        """
        ä¸€å€‹ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œæ ¹æ“š force_refresh åƒæ•¸æ±ºå®šæ˜¯å¦è¦æš«æ™‚ç¦ç”¨å¿«å–ã€‚
        é€™æ˜¯å¯¦ç¾ã€Œæ‰‹å‹•åˆ·æ–°ã€çš„çµ±ä¸€å…¥å£é»ã€‚

        Args:
            force_refresh (bool): æ˜¯å¦å¼·åˆ¶åˆ·æ–°ã€‚
        """
        if force_refresh:
            logger.info(f"{self.__class__.__name__} åµæ¸¬åˆ°å¼·åˆ¶åˆ·æ–°æŒ‡ä»¤ã€‚")
            with temporary_disabled_cache(self._session):
                yield
        else:
            yield

    def close_session(self):
        """
        é—œé–‰ requests sessionã€‚
        """
        if self._session:
            self._session.close()
            logger.info(f"{self.__class__.__name__} çš„ Session å·²é—œé–‰ã€‚")

    def fetch_data(self, symbol: str, **kwargs):
        """
        ç²å–æ•¸æ“šçš„æŠ½è±¡æ–¹æ³•ï¼Œæ‡‰ç”±å­é¡åˆ¥å¯¦ç¾ã€‚
        é€™ç¢ºä¿äº†æ‰€æœ‰å­é¡åˆ¥éƒ½æœ‰ä¸€å€‹çµ±ä¸€çš„æ•¸æ“šç²å–å…¥å£é»ã€‚

        Args:
            symbol (str): è¦ç²å–çš„æ•¸æ“šæ¨™çš„ (ä¾‹å¦‚è‚¡ç¥¨ä»£ç¢¼ã€æŒ‡æ¨™ä»£ç¢¼)ã€‚
            **kwargs: å…¶ä»–ç‰¹å®šæ–¼è©²æ¬¡è«‹æ±‚çš„åƒæ•¸ï¼Œä¾‹å¦‚ `force_refresh`ã€‚

        Raises:
            NotImplementedError: å¦‚æœå­é¡åˆ¥æ²’æœ‰å¯¦ç¾æ­¤æ–¹æ³•ã€‚
        """
        raise NotImplementedError("å­é¡åˆ¥å¿…é ˆå¯¦ç¾ fetch_data æ–¹æ³•")

    def _perform_request(
        self, endpoint: str, params: Optional[dict] = None, method: str = "GET"
    ) -> requests.Response:
        """
        åŸ·è¡Œå¯¦éš›çš„ HTTP è«‹æ±‚ã€‚

        Args:
            endpoint (str): API çš„ç«¯é»è·¯å¾‘ã€‚
            params (Optional[dict]): è«‹æ±‚åƒæ•¸ã€‚
            method (str): HTTP æ–¹æ³• (ä¾‹å¦‚ "GET", "POST")ã€‚

        Returns:
            requests.Response: API çš„å›æ‡‰ç‰©ä»¶ã€‚

        Raises:
            requests.exceptions.HTTPError: å¦‚æœ API å›æ‡‰ HTTP éŒ¯èª¤ã€‚
            ValueError: å¦‚æœ base_url æœªè¨­å®šã€‚
        """
        if not self.base_url:
            raise ValueError(
                f"{self.__class__.__name__}: base_url is not set, cannot make a request."
            )

        url = f"{self.base_url.rstrip('/')}/{endpoint.lstrip('/')}"

        logger.debug(f"{self.__class__.__name__} æ­£åœ¨å‘ {method} {url} ç™¼é€è«‹æ±‚ï¼Œåƒæ•¸ï¼š{params}")

        if method.upper() == "GET":
            response = self._session.get(url, params=params)
        elif method.upper() == "POST":
            response = self._session.post(
                url, params=params
            )  # Or json=params if API expects JSON body
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„ HTTP æ–¹æ³•: {method}")

        response.raise_for_status()
        return response


# ç¯„ä¾‹ä½¿ç”¨ (ä¸»è¦ç”¨æ–¼é–‹ç™¼æ™‚æ¸¬è©¦)
if __name__ == "__main__":
    print("--- BaseAPIClient å‡ç´šå¾Œæ¸¬è©¦ (ç›´æ¥åŸ·è¡Œ core/clients/base.py) ---")

    class MockClient(BaseAPIClient):
        def __init__(self):
            super().__init__(base_url="https://httpbin.org")

        def fetch_data(self, symbol: str, **kwargs):
            endpoint = f"/delay/{symbol}"
            url = self.base_url + endpoint

            # å¾ kwargs ä¸­æå– force_refresh åƒæ•¸
            force_refresh = kwargs.get("force_refresh", False)

            # ä½¿ç”¨ _get_request_context ä¾†æ§åˆ¶å¿«å–
            with self._get_request_context(force_refresh=force_refresh):
                response = self._session.get(url)

            print(f"è«‹æ±‚ URL: {url}, æ˜¯å¦ä¾†è‡ªå¿«å–: {response.from_cache}")
            response.raise_for_status()
            return response.json()

    client = MockClient()
    try:
        print("\n--- åŸ·è¡Œç¬¬ä¸€æ¬¡ (æ‡‰æœƒä¸‹è¼‰) ---")
        client.fetch_data("2")  # å»¶é² 2 ç§’

        print("\n--- åŸ·è¡Œç¬¬äºŒæ¬¡ (æ‡‰å¾å¿«å–è®€å–) ---")
        client.fetch_data("2")

        print("\n--- åŸ·è¡Œç¬¬ä¸‰æ¬¡ (å¼·åˆ¶åˆ·æ–°) ---")
        client.fetch_data("2", force_refresh=True)

    finally:
        client.close_session()

    print("\n--- BaseAPIClient å‡ç´šå¾Œæ¸¬è©¦çµæŸ ---")
# æª”æ¡ˆè·¯å¾‘: core/clients/taifex_db.py
from typing import Any, Dict

import pandas as pd


class TaifexDBClient:
    """
    å°ç£æœŸè²¨äº¤æ˜“æ‰€æ•¸æ“šåº«å®¢æˆ¶ç«¯ (ä½”ä½ç¬¦)ã€‚
    æœªä¾†å°‡å¯¦ç¾å¾æ•¸æ“šåº«è®€å–æ•¸æ“šçš„åŠŸèƒ½ã€‚
    """

    def __init__(self, db_connection_string: str = None):
        """
        åˆå§‹åŒ– TaifexDBClientã€‚
        :param db_connection_string: è³‡æ–™åº«é€£ç·šå­—ä¸² (æœªä¾†ä½¿ç”¨)
        """
        self.db_connection_string = db_connection_string
        print("TaifexDBClient (ä½”ä½ç¬¦) å·²åˆå§‹åŒ–ã€‚")

    def get_institutional_positions(
        self, start_date: str, end_date: str
    ) -> pd.DataFrame:
        """
        ç²å–ä¸‰å¤§æ³•äººæœŸè²¨åŠé¸æ“‡æ¬Šç±Œç¢¼åˆ†ä½ˆ (ä½”ä½ç¬¦)ã€‚
        :param start_date: é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
        :param end_date: çµæŸæ—¥æœŸ (YYYY-MM-DD)
        :return: åŒ…å«ç±Œç¢¼æ•¸æ“šçš„ DataFrame
        """
        print(
            f"TaifexDBClient (ä½”ä½ç¬¦): æ­£åœ¨ç²å– {start_date} åˆ° {end_date} çš„ä¸‰å¤§æ³•äººæ•¸æ“š..."
        )
        # è¿”å›ä¸€å€‹ç©ºçš„ DataFrame ä½œç‚ºä½”ä½ç¬¦
        return pd.DataFrame()

    def get_futures_ohlcv(
        self, contract: str, start_date: str, end_date: str
    ) -> pd.DataFrame:
        """
        ç²å–æŒ‡å®šæœŸè²¨åˆç´„çš„ OHLCV æ•¸æ“š (ä½”ä½ç¬¦)ã€‚
        :param contract: åˆç´„ä»£ç¢¼ (ä¾‹å¦‚ TXF1)
        :param start_date: é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
        :param end_date: çµæŸæ—¥æœŸ (YYYY-MM-DD)
        :return: åŒ…å« OHLCV æ•¸æ“šçš„ DataFrame
        """
        print(
            f"TaifexDBClient (ä½”ä½ç¬¦): æ­£åœ¨ç²å– {contract} å¾ {start_date} åˆ° {end_date} çš„ OHLCV æ•¸æ“š..."
        )
        # è¿”å›ä¸€å€‹ç©ºçš„ DataFrame ä½œç‚ºä½”ä½ç¬¦
        return pd.DataFrame()

    # å¯ä»¥æ ¹æ“šéœ€è¦æ·»åŠ æ›´å¤šæ–¹æ³•
    def some_other_method(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¦ä¸€å€‹ç¤ºä¾‹æ–¹æ³• (ä½”ä½ç¬¦)ã€‚
        """
        print(f"TaifexDBClient (ä½”ä½ç¬¦): å‘¼å« some_other_methodï¼Œåƒæ•¸: {params}")
        return {"status": "success", "data": "some_placeholder_data"}
# prometheus/core/clients/__init__.py

from .base import BaseAPIClient
from .finmind import FinMindClient
from .fmp import FMPClient
from .fred import FredClient
from .nyfed import NYFedClient
from .yfinance import YFinanceClient

__all__ = [
    "BaseAPIClient",
    "FMPClient",
    "FinMindClient",  # <-- æ›´æ–°ç‚º FinMindClient
    "FredClient",  # <-- å·²ä¿®æ­£ç‚º FredClient
    "NYFedClient",
    "YFinanceClient",
]
# core/clients/fmp.py
# æ­¤æ¨¡çµ„åŒ…å«èˆ‡ Financial Modeling Prep (FMP) API äº’å‹•çš„å®¢æˆ¶ç«¯é‚è¼¯ã€‚

import os
from typing import Any, Dict, List, Optional

import pandas as pd
import requests

from .base import BaseAPIClient
from prometheus.core.logging.log_manager import LogManager

logger = LogManager.get_instance().get_logger("FMPClient")

# å‡è¨­é…ç½®çµ±ä¸€ç”± BaseAPIClient æˆ–ç’°å¢ƒè®Šæ•¸è™•ç†ï¼Œä¸å†å¾ core.config å°å…¥ settings
# å¦‚æœæœ‰çµ±ä¸€çš„ settings ç‰©ä»¶ï¼Œå¯ä»¥å¾ŒçºŒåŠ å…¥

# FMP API åŸºç¤ URL (ä¸å«ç‰ˆæœ¬è™Ÿï¼Œç‰ˆæœ¬è™Ÿå°‡åœ¨ endpoint ä¸­è™•ç†)
FMP_API_BASE_URL_NO_VERSION = "https://financialmodelingprep.com/api"


class FMPClient(BaseAPIClient):
    """
    Financial Modeling Prep (FMP) API å®¢æˆ¶ç«¯ã€‚
    ç”¨æ–¼ç²å–å…¨çƒå¸‚å ´ï¼ˆå°¤å…¶æ˜¯ç¾è‚¡ï¼‰çš„è²¡ç¶“æ•¸æ“šï¼Œå¦‚æ­·å²åƒ¹æ ¼ã€å…¬å¸è²¡å ±ç­‰ã€‚
    """

    def __init__(self, api_key: Optional[str] = None, default_api_version: str = "v3"):
        """
        åˆå§‹åŒ– FMPClientã€‚

        Args:
            api_key (Optional[str]): FMP API keyã€‚å¦‚æœæœªæä¾›ï¼Œå°‡å˜—è©¦å¾ç’°å¢ƒè®Šæ•¸ FMP_API_KEY è®€å–ã€‚
            default_api_version (str): é è¨­ä½¿ç”¨çš„ API ç‰ˆæœ¬ (ä¾‹å¦‚ "v3", "v4")ã€‚
                                       å¯¦éš›è«‹æ±‚æ™‚ï¼Œç«¯é»è·¯å¾‘æ‡‰åŒ…å«ç‰ˆæœ¬è™Ÿã€‚
        """
        fmp_api_key = api_key or os.getenv("FMP_API_KEY")
        if not fmp_api_key:
            raise ValueError(
                "FMP API key æœªè¨­å®šã€‚è«‹è¨­å®š FMP_API_KEY ç’°å¢ƒè®Šæ•¸æˆ–åœ¨åˆå§‹åŒ–æ™‚å‚³å…¥ api_keyã€‚"
            )

        super().__init__(api_key=fmp_api_key, base_url=FMP_API_BASE_URL_NO_VERSION)
        self.default_api_version = default_api_version
        logger.info(f"FMPClient åˆå§‹åŒ–å®Œæˆï¼Œé è¨­ API ç‰ˆæœ¬ '{self.default_api_version}'ã€‚")

    def _prepare_params(
        self, params: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        æº–å‚™è«‹æ±‚åƒæ•¸ï¼Œç‰¹åˆ¥æ˜¯æ·»åŠ  FMP æ‰€éœ€çš„ 'apikey'ã€‚
        """
        request_params = params.copy() if params else {}
        request_params["apikey"] = self.api_key  # FMP ä½¿ç”¨ 'apikey'
        return request_params

    def fetch_data(self, symbol: str, **kwargs) -> pd.DataFrame:
        """
        å¾ FMP API ç²å–æ•¸æ“šã€‚æ­¤æ–¹æ³•ä½œç‚ºä¸€å€‹çµ±ä¸€çš„å…¥å£ï¼Œ
        é€šé kwargs ä¸­çš„ data_type åƒæ•¸ä¾†æ±ºå®šç²å–ä½•ç¨®æ•¸æ“šã€‚

        Args:
            symbol (str): å•†å“ä»£ç¢¼ (ä¾‹å¦‚ "AAPL")ã€‚
            **kwargs:
                data_type (str): å¿…é ˆæä¾›ã€‚æŒ‡å®šè¦ç²å–çš„æ•¸æ“šé¡å‹ã€‚
                                 å¯é¸å€¼: "historical_price", "income_statement",
                                          "balance_sheet_statement", "cash_flow_statement"ã€‚
                api_version (str, optional): æŒ‡å®šæ­¤æ¬¡è«‹æ±‚ä½¿ç”¨çš„ API ç‰ˆæœ¬ï¼Œé è¨­ç‚ºå®¢æˆ¶ç«¯åˆå§‹åŒ–æ™‚çš„ default_api_versionã€‚
                from_date (str, optional): é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)ï¼Œç”¨æ–¼æ­·å²åƒ¹æ ¼ã€‚
                to_date (str, optional): çµæŸæ—¥æœŸ (YYYY-MM-DD)ï¼Œç”¨æ–¼æ­·å²åƒ¹æ ¼ã€‚
                period (str, optional): è²¡å ±é€±æœŸ ("quarter" æˆ– "annual")ï¼Œç”¨æ–¼è²¡å ±ã€‚é è¨­ "quarter"ã€‚
                limit (int, optional): è¿”å›çš„è²¡å ±æœŸæ•¸æˆ–æ­·å²æ•¸æ“šé»æ•¸ã€‚é è¨­ 20 (ç”¨æ–¼è²¡å ±)ã€‚

        Returns:
            pd.DataFrame: åŒ…å«è«‹æ±‚æ•¸æ“šçš„ DataFrameã€‚å¦‚æœå¤±æ•—æˆ–ç„¡æ•¸æ“šï¼Œå‰‡è¿”å›ç©ºçš„ DataFrameã€‚

        Raises:
            ValueError: å¦‚æœ data_type æœªæä¾›æˆ–ä¸å—æ”¯æŒã€‚
            requests.exceptions.HTTPError: å¦‚æœ API è«‹æ±‚å¤±æ•—ã€‚
        """
        data_type = kwargs.pop("data_type", None)
        if not data_type:
            raise ValueError(
                "å¿…é ˆåœ¨ kwargs ä¸­æä¾› 'data_type' åƒæ•¸ (ä¾‹å¦‚ 'historical_price', 'income_statement')ã€‚"
            )

        api_version = kwargs.pop("api_version", self.default_api_version)
        params: Dict[str, Any] = {}
        endpoint_path_template: str = ""

        if data_type == "historical_price":
            endpoint_path_template = f"{api_version}/historical-price-full/{symbol}"
            if "from_date" in kwargs:
                params["from"] = kwargs["from_date"]
            if "to_date" in kwargs:
                params["to"] = kwargs["to_date"]
            # FMP çš„ limit for historical prices might be 'serietype' or implied by date range
            if "limit" in kwargs:
                params["limit"] = str(kwargs["limit"])

        elif data_type in [
            "income-statement",
            "balance-sheet-statement",
            "cash-flow-statement",
        ]:
            endpoint_path_template = f"{api_version}/{data_type}/{symbol}"
            params["period"] = kwargs.get("period", "quarter")
            params["limit"] = str(kwargs.get("limit", 20))

        else:
            raise ValueError(
                f"ä¸æ”¯æ´çš„ data_type: {data_type}ã€‚æ”¯æ´çš„å€¼ç‚º 'historical_price', 'income_statement', 'balance_sheet_statement', 'cash_flow_statement'ã€‚"
            )

        final_params = self._prepare_params(params)

        logger.debug(f"æ­£åœ¨ç²å– '{data_type}' æ•¸æ“šï¼Œä»£ç¢¼: {symbol}, Endpoint: {endpoint_path_template}, Params: {params}")

        try:
            response = super()._perform_request(
                endpoint=endpoint_path_template, params=final_params, method="GET"
            )
            json_response = response.json()

            if isinstance(json_response, dict) and "Error Message" in json_response:
                error_msg = json_response["Error Message"]
                logger.error(f"FMP API è¿”å›æ¥­å‹™é‚è¼¯éŒ¯èª¤ï¼š'{error_msg}' (Endpoint: {endpoint_path_template})")
                return pd.DataFrame()

            data_list: Optional[List[Dict[str, Any]]] = None
            if isinstance(json_response, list):
                data_list = json_response
            elif isinstance(json_response, dict):
                possible_data_keys = ["historical"]
                found_key = False
                for key in possible_data_keys:
                    if key in json_response and isinstance(json_response[key], list):
                        data_list = json_response[key]
                        found_key = True
                        break
                if not found_key and data_type not in ["historical_price"]:
                    logger.warning(f"FMP API è¿”å›äº†ä¸€å€‹å­—å…¸ï¼Œä½†æœªåœ¨é æœŸéµä¸‹æ‰¾åˆ°æ•¸æ“šåˆ—è¡¨ã€‚Endpoint: {endpoint_path_template}")
                    return pd.DataFrame()

            if data_list is None:
                logger.warning(f"FMP API å›æ‡‰ç„¡æ³•è§£æç‚ºé æœŸçš„åˆ—è¡¨çµæ§‹ã€‚Endpoint: {endpoint_path_template}")
                return pd.DataFrame()

            if not data_list:
                logger.info(f"FMP API æœªè¿”å› '{symbol}' çš„ '{data_type}' æ•¸æ“šã€‚")
                return pd.DataFrame()

            df = pd.DataFrame(data_list)

            if "date" in df.columns:
                df["date"] = pd.to_datetime(df["date"])
                if data_type == "historical_price":
                    df = df.sort_values(by="date").reset_index(drop=True)
                elif data_type in ["income-statement", "balance-sheet-statement", "cash-flow-statement"]:
                    df = df.sort_values(by="date", ascending=False).reset_index(drop=True)

            logger.info(f"æˆåŠŸç²å–ä¸¦è™•ç†äº† {len(df)} ç­† '{data_type}' æ•¸æ“šï¼Œä»£ç¢¼: {symbol}ã€‚")
            return df

        except requests.exceptions.HTTPError as http_err:
            logger.error(f"è«‹æ±‚å¤±æ•— (HTTP éŒ¯èª¤)ï¼š{http_err}ã€‚Endpoint: {endpoint_path_template}", exc_info=True)
            return pd.DataFrame()
        except ValueError as json_err:
            logger.error(f"è§£æ JSON å›æ‡‰å¤±æ•—ï¼š{json_err}ã€‚Endpoint: {endpoint_path_template}", exc_info=True)
            return pd.DataFrame()
        except Exception as e:
            logger.error(f"è™•ç†æ•¸æ“šæ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼š{e}ã€‚Endpoint: {endpoint_path_template}", exc_info=True)
            return pd.DataFrame()


# ç¯„ä¾‹ä½¿ç”¨ (ä¸»è¦ç”¨æ–¼é–‹ç™¼æ™‚æ¸¬è©¦)
if __name__ == "__main__":
    print("--- FMPClient é‡æ§‹å¾Œæ¸¬è©¦ (ç›´æ¥åŸ·è¡Œ core/clients/fmp.py) ---")
    # åŸ·è¡Œæ­¤æ¸¬è©¦å‰ï¼Œè«‹ç¢ºä¿è¨­å®šäº† FMP_API_KEY ç’°å¢ƒè®Šæ•¸
    try:
        client = FMPClient(default_api_version="v3")
        print("FMPClient åˆå§‹åŒ–æˆåŠŸã€‚")

        # æ¸¬è©¦ç²å–æ­·å²è‚¡åƒ¹
        print("\næ¸¬è©¦ç²å– AAPL æ­·å²æ—¥ç·šåƒ¹æ ¼ (2023-12-01 è‡³ 2023-12-05)...")
        aapl_prices = client.fetch_data(
            symbol="AAPL",
            data_type="historical_price",
            from_date="2023-12-01",
            to_date="2023-12-05",
        )
        if not aapl_prices.empty:
            print(f"æˆåŠŸç²å– AAPL æ­·å²åƒ¹æ ¼æ•¸æ“š (å…± {len(aapl_prices)} ç­†):")
            print(aapl_prices.head())
        else:
            print(
                "ç²å– AAPL æ­·å²åƒ¹æ ¼æ•¸æ“šè¿”å›ç©º DataFrame (è«‹æª¢æŸ¥ API Key æ¬Šé™ã€æ—¥æœŸç¯„åœæˆ–æ—¥èªŒä¸­çš„éŒ¯èª¤)ã€‚"
            )

        # æ¸¬è©¦ç²å–è²¡å ±æ•¸æ“š (v3 income-statement)
        print("\næ¸¬è©¦ç²å– MSFT å­£åº¦æç›Šè¡¨ (æœ€è¿‘1æœŸ, v3)...")
        income_statement_msft = client.fetch_data(
            symbol="MSFT",
            data_type="income-statement",
            period="quarter",
            limit=1,
            api_version="v3",  # æ˜ç¢ºæŒ‡å®šç‰ˆæœ¬
        )
        if not income_statement_msft.empty:
            print(f"æˆåŠŸç²å– MSFT å­£åº¦æç›Šè¡¨æ•¸æ“š (å…± {len(income_statement_msft)} ç­†):")
            print(income_statement_msft.head())
        else:
            print("ç²å– MSFT å­£åº¦æç›Šè¡¨æ•¸æ“šè¿”å›ç©º DataFrameã€‚")

        # æ¸¬è©¦ä¸€å€‹ä¸å­˜åœ¨çš„è‚¡ç¥¨
        print("\næ¸¬è©¦ç²å–ä¸å­˜åœ¨è‚¡ç¥¨ 'XYZNOTASTOCK' çš„æ­·å²åƒ¹æ ¼...")
        non_existent_prices = client.fetch_data(
            symbol="XYZNOTASTOCK",
            data_type="historical_price",
            from_date="2023-01-01",
            to_date="2023-01-05",
        )
        if non_existent_prices.empty:
            print(
                "ç²å–ä¸å­˜åœ¨è‚¡ç¥¨åƒ¹æ ¼æ•¸æ“šè¿”å›ç©º DataFrame (ç¬¦åˆé æœŸï¼Œæˆ– API è¿”å›éŒ¯èª¤)ã€‚"
            )
        else:
            print(
                f"ç²å–ä¸å­˜åœ¨è‚¡ç¥¨åƒ¹æ ¼æ•¸æ“šè¿”å›äº†éé æœŸçš„æ•¸æ“š: {non_existent_prices.head()}"
            )

        # æ¸¬è©¦ç„¡æ•ˆ data_type
        try:
            print("\næ¸¬è©¦ç„¡æ•ˆçš„ data_type...")
            client.fetch_data(symbol="AAPL", data_type="invalid_type")
        except ValueError as ve:
            print(f"æˆåŠŸæ•ç²åˆ°éŒ¯èª¤ (ç¬¦åˆé æœŸ): {ve}")

    except ValueError as ve_init:  # API Key æœªè¨­å®šç­‰åˆå§‹åŒ–å•é¡Œ
        print(f"åˆå§‹åŒ–éŒ¯èª¤: {ve_init}")
    except Exception as e:
        print(f"åŸ·è¡ŒæœŸé–“ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")

    print("--- FMPClient é‡æ§‹å¾Œæ¸¬è©¦çµæŸ ---")
# core/clients/finmind.py
# æ­¤æ¨¡çµ„åŒ…å«èˆ‡ FinMind API äº’å‹•çš„å®¢æˆ¶ç«¯é‚è¼¯ã€‚
from __future__ import annotations

import os
from datetime import datetime
from io import StringIO
from typing import Any, Dict, List, Optional

import pandas as pd
import requests

from .base import BaseAPIClient
from prometheus.core.logging.log_manager import LogManager

logger = LogManager.get_instance().get_logger("FinMindClient")

# FinMind API åŸºç¤ URL (æ‰€æœ‰è«‹æ±‚éƒ½ä½¿ç”¨æ­¤ URL)
FINMIND_API_BASE_URL = "https://api.finmindtrade.com/api/v4/data"


class FinMindClient(BaseAPIClient):
    """
    ç”¨æ–¼èˆ‡ FinMind API äº’å‹•çš„å®¢æˆ¶ç«¯ã€‚
    FinMind API çš„ç‰¹é»æ˜¯æ‰€æœ‰æ•¸æ“šè«‹æ±‚éƒ½ä½¿ç”¨åŒä¸€å€‹åŸºç¤ URLï¼Œ
    å…·é«”çš„æ•¸æ“šé›†å’Œåƒæ•¸åœ¨è«‹æ±‚çš„ params ä¸­æŒ‡å®šã€‚
    å®ƒå¯èƒ½è¿”å› JSON æˆ– CSV æ ¼å¼çš„æ•¸æ“šã€‚
    """

    def __init__(self, api_token: Optional[str] = None):
        """
        åˆå§‹åŒ– FinMindClientã€‚

        Args:
            api_token (Optional[str]): FinMind API Tokenã€‚å¦‚æœæœªæä¾›ï¼Œ
                                       å°‡å˜—è©¦å¾ç’°å¢ƒè®Šæ•¸ FINMIND_API_TOKEN è®€å–ã€‚
        Raises:
            ValueError: å¦‚æœ API Token æœªæä¾›ä¸”ç’°å¢ƒè®Šæ•¸ä¸­ä¹Ÿæœªè¨­å®šã€‚
        """
        finmind_api_token = api_token or os.getenv("FINMIND_API_TOKEN")
        if not finmind_api_token:
            raise ValueError(
                "FinMind API token æœªè¨­å®šã€‚è«‹è¨­å®š FINMIND_API_TOKEN ç’°å¢ƒè®Šæ•¸æˆ–åœ¨åˆå§‹åŒ–æ™‚å‚³å…¥ api_tokenã€‚"
            )

        super().__init__(api_key=finmind_api_token, base_url=FINMIND_API_BASE_URL)
        logger.info("FinMindClient åˆå§‹åŒ–å®Œæˆã€‚")

    async def _request(
        self, endpoint: str = "", params: Optional[Dict[str, Any]] = None
    ) -> pd.DataFrame:
        import asyncio
        if not params:
            raise ValueError("è«‹æ±‚ FinMind API æ™‚ï¼Œparams åƒæ•¸ä¸å¾—ç‚ºç©ºã€‚")

        request_params = params.copy()
        request_params["token"] = self.api_key

        if not self.base_url:
            raise ValueError(
                "FinMindClient: base_url is not set, cannot make a request."
            )

        current_url = (
            f"{self.base_url.rstrip('/')}/{endpoint.lstrip('/')}"
            if endpoint
            else self.base_url
        )

        logger.debug(f"å‘ FinMind API ç™¼é€è«‹æ±‚ï¼ŒURL: {current_url}, è³‡æ–™é›†ï¼š'{request_params.get('dataset')}', è³‡æ–™IDï¼š'{request_params.get('data_id')}'")

        def _sync_request():
            try:
                if not current_url:
                    raise ValueError("FinMindClient: Calculated URL is empty, cannot make a request.")
                response: requests.Response = self._session.get(current_url, params=request_params)
                response.raise_for_status()

                content_type = response.headers.get("Content-Type", "")
                if "text/csv" in content_type:
                    df = pd.read_csv(StringIO(response.text))
                    return df if not df.empty else pd.DataFrame()
                elif "application/json" in content_type:
                    json_response: Dict[str, Any] = response.json()
                    if json_response.get("status") != 200:
                        error_msg = json_response.get("msg", "æœªçŸ¥ API å…§éƒ¨éŒ¯èª¤")
                        status_code = json_response.get("status", "N/A")
                        logger.error(f"FinMind API é‚è¼¯éŒ¯èª¤ (å…§éƒ¨ status {status_code}): {error_msg}")
                        return pd.DataFrame()
                    data_list: Optional[List[Dict[str, Any]]] = json_response.get("data")
                    if data_list:
                        return pd.DataFrame(data_list)
                    else:
                        logger.info(f"FinMind API æœªè¿”å›ä»»ä½•æ•¸æ“šã€‚")
                        return pd.DataFrame()
                else:
                    logger.error(f"æœªçŸ¥çš„ FinMind API å›æ‡‰ Content-Type: {content_type}")
                    return pd.DataFrame()
            except requests.exceptions.HTTPError as http_err:
                logger.error(f"FinMind API HTTP éŒ¯èª¤ï¼š{http_err}", exc_info=True)
                raise
            except requests.exceptions.RequestException as req_err:
                logger.error(f"è«‹æ±‚ FinMind API æ™‚ç™¼ç”Ÿç¶²è·¯æˆ–è«‹æ±‚é…ç½®éŒ¯èª¤ï¼š{req_err}", exc_info=True)
                return pd.DataFrame()
            except Exception as e:
                logger.error(f"è™•ç† FinMind API å›æ‡‰æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼š{e}", exc_info=True)
                return pd.DataFrame()

        return await asyncio.to_thread(_sync_request)

    async def fetch_data(self, symbol: str, **kwargs) -> pd.DataFrame:
        dataset = kwargs.get("dataset")
        start_date = kwargs.get("start_date")

        if not dataset:
            raise ValueError("'dataset' åƒæ•¸ç‚ºå¿…å¡«é …ã€‚")
        if not start_date:
            raise ValueError("'start_date' åƒæ•¸ç‚ºå¿…å¡«é …ã€‚")

        params: Dict[str, Any] = {
            "dataset": dataset,
            "data_id": symbol,
            "start_date": start_date,
            "end_date": kwargs.get("end_date", datetime.now().strftime("%Y-%m-%d")),
        }

        for key, value in kwargs.items():
            if key not in ["dataset", "start_date", "end_date", "data_id", "symbol"]:
                params[key] = value

        try:
            return await self._request(endpoint="", params=params)
        except requests.exceptions.HTTPError:
            raise

    def get_monthly_revenue(self, stock_id: str, start_year: int, start_month: int) -> pd.DataFrame:
        """
        ç²å–æœˆç‡Ÿæ”¶æ•¸æ“šã€‚
        """
        start_date = f"{start_year}-{start_month:02d}-01"
        end_date = datetime.now().strftime("%Y-%m-%d")
        return self.fetch_data(
            symbol=stock_id,
            dataset="TaiwanStockMonthRevenue",
            start_date=start_date,
            end_date=end_date,
        )

    def get_taiwan_stock_institutional_investors_buy_sell(
        self, stock_id: str, start_date: str, end_date: Optional[str] = None
    ) -> pd.DataFrame:
        return self.fetch_data(
            symbol=stock_id,
            dataset="TaiwanStockInstitutionalInvestorsBuySell",
            start_date=start_date,
            end_date=end_date,
        )


if __name__ == "__main__":
    print("--- FinMindClient é‡æ§‹å¾Œæ¸¬è©¦ (ç›´æ¥åŸ·è¡Œ core/clients/finmind.py) ---")
    try:
        client = FinMindClient()
        print("FinMindClient åˆå§‹åŒ–æˆåŠŸã€‚")

        print("\næ¸¬è©¦ç²å–å°ç©é›» (2330) æ³•äººè²·è³£è¶… (2024-01-01 è‡³ 2024-01-05)...")
        investor_data = client.get_taiwan_stock_institutional_investors_buy_sell(
            stock_id="2330", start_date="2024-01-01", end_date="2024-01-05"
        )
        if not investor_data.empty:
            print(f"æˆåŠŸç²å–è‚¡ç¥¨ 2330 çš„æ³•äººè²·è³£è¶…æ•¸æ“š (å…± {len(investor_data)} ç­†):")
            print(investor_data.head())
        else:
            print(
                "è‚¡ç¥¨ 2330 çš„æ³•äººè²·è³£è¶…æ•¸æ“šè«‹æ±‚æˆåŠŸï¼Œä½†è¿”å›ç‚ºç©º DataFrame (è«‹æª¢æŸ¥ API Key, æ—¥æœŸç¯„åœæˆ–æ—¥èªŒ)ã€‚"
            )

        print(
            "\næ¸¬è©¦ä½¿ç”¨ fetch_data ç²å–è¯ç™¼ç§‘ (2454) è‚¡åƒ¹ (2024-03-01 è‡³ 2024-03-05)..."
        )
        stock_price_data = client.fetch_data(
            symbol="2454",
            dataset="TaiwanStockPrice",
            start_date="2024-03-01",
            end_date="2024-03-05",
        )
        if not stock_price_data.empty:
            print(f"æˆåŠŸç²å–è‚¡ç¥¨ 2454 çš„è‚¡åƒ¹æ•¸æ“š (å…± {len(stock_price_data)} ç­†):")
            print(stock_price_data.head())
        else:
            print("è‚¡ç¥¨ 2454 çš„è‚¡åƒ¹æ•¸æ“šè«‹æ±‚æˆåŠŸï¼Œä½†è¿”å›ç‚ºç©º DataFrameã€‚")

        print("\næ¸¬è©¦ä¸€å€‹ä¸å­˜åœ¨çš„è‚¡ç¥¨ä»£ç¢¼ (XYZABC) ä½¿ç”¨ fetch_data...")
        non_existent_data = client.fetch_data(
            symbol="XYZABC",
            dataset="TaiwanStockPrice",
            start_date="2023-01-01",
            end_date="2023-01-05",
        )
        if non_existent_data.empty:
            print(
                "ç²å– XYZABC æ•¸æ“šè¿”å›ç©º DataFrame (ç¬¦åˆé æœŸï¼Œå› ç‚ºè‚¡ç¥¨ä¸å­˜åœ¨æˆ–è«‹æ±‚éŒ¯èª¤)ã€‚"
            )
        else:
            print(f"ç²å– XYZABC æ•¸æ“šè¿”å›äº†éé æœŸçš„æ•¸æ“š: \n{non_existent_data.head()}")

        try:
            print("\næ¸¬è©¦ fetch_data ç¼ºå°‘ 'dataset'...")
            client.fetch_data(symbol="2330", start_date="2024-01-01")  # Missing dataset
        except ValueError as ve:
            print(f"æˆåŠŸæ•ç²éŒ¯èª¤ (ç¬¦åˆé æœŸ): {ve}")

    except ValueError as ve_init:
        print(f"åˆå§‹åŒ–éŒ¯èª¤: {ve_init}")
    except requests.exceptions.HTTPError as http_e:
        print(f"æ•ç²åˆ° HTTP éŒ¯èª¤ (å¯èƒ½æ˜¯ API Token ç„¡æ•ˆæˆ–ç¶²è·¯å•é¡Œ): {http_e}")
    except Exception as e:
        print(f"åŸ·è¡ŒæœŸé–“ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")

    print("--- FinMindAPIClient é‡æ§‹å¾Œæ¸¬è©¦çµæŸ ---")
import logging
import sys
from logging.handlers import RotatingFileHandler
from pathlib import Path
import os

class LogManager:
    """
    ä¸€å€‹å–®ä¾‹çš„æ—¥èªŒç®¡ç†å™¨ã€‚
    å®ƒèƒ½ç‚ºæ•´å€‹æ‡‰ç”¨ç¨‹å¼é…ç½®æ—¥èªŒï¼Œå°‡æ—¥èªŒè¼¸å‡ºåˆ°æ§åˆ¶å°å’ŒæŒ‡å®šçš„å¯è¼ªæ›¿æª”æ¡ˆä¸­ã€‚
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(LogManager, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self, log_dir: str = "data/logs", log_file: str = "prometheus.log", log_level=logging.INFO):
        """
        åˆå§‹åŒ–æ—¥èªŒç®¡ç†å™¨ã€‚

        :param log_dir: æ—¥èªŒæª”æ¡ˆå­˜æ”¾çš„ç›®éŒ„ã€‚
        :param log_file: æ—¥èªŒæª”æ¡ˆçš„åç¨±ã€‚
        :param log_level: æ—¥èªŒç´šåˆ¥ã€‚
        """
        if self._initialized:
            return

        log_path = Path(log_dir)
        log_path.mkdir(parents=True, exist_ok=True)
        self.log_file_path = log_path / log_file
        self.log_level = log_level
        self.formatter = logging.Formatter(
            "[%(asctime)s] [%(levelname)s] [%(name)s] - %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S"
        )
        self._initialized = True

    @classmethod
    def get_instance(cls):
        """
        ç²å– LogManager çš„å–®ä¾‹å¯¦ä¾‹ã€‚
        """
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def get_logger(self, name: str) -> logging.Logger:
        """
        ç²å–ä¸€å€‹é…ç½®å¥½çš„æ—¥èªŒè¨˜éŒ„å™¨ã€‚

        :param name: æ—¥èªŒè¨˜éŒ„å™¨çš„åç¨±ã€‚
        :return: ä¸€å€‹é…ç½®å¥½çš„ logging.Logger å¯¦ä¾‹ã€‚
        """
        logger = logging.getLogger(name)
        logger.setLevel(self.log_level)

        # ç‚ºäº†é˜²æ­¢é‡è¤‡æ·»åŠ  handlersï¼Œæ¯æ¬¡éƒ½æ¸…ç©º
        if logger.hasHandlers():
            logger.handlers.clear()

        # ç¢ºä¿æ—¥èªŒäº‹ä»¶ä¸æœƒå‘ä¸Šä¼ æ’­åˆ° root logger
        logger.propagate = False

        # æª”æ¡ˆ handler (å¸¶è¼ªæ›¿åŠŸèƒ½)
        file_handler = RotatingFileHandler(
            self.log_file_path,
            maxBytes=10 * 1024 * 1024,  # 10 MB
            backupCount=5,
            encoding="utf-8",
        )
        file_handler.setFormatter(self.formatter)
        logger.addHandler(file_handler)

        # æ§åˆ¶å° handler
        stream_handler = logging.StreamHandler(sys.stdout)
        stream_handler.setFormatter(self.formatter)
        logger.addHandler(stream_handler)

        return logger
# core/pipelines/base_step.py
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Any, Dict

import pandas as pd


class BaseStep(ABC):
    """
    åŒæ­¥æ•¸æ“šè™•ç†ç®¡ç·šä¸­å–®å€‹æ­¥é©Ÿçš„æŠ½è±¡åŸºç¤é¡ã€‚
    """

    @abstractmethod
    async def run(self, data: Any, context: Dict[str, Any]) -> Any:
        """
        åŸ·è¡Œæ­¤æ­¥é©Ÿçš„æ ¸å¿ƒé‚è¼¯ã€‚

        :param data: ä¸Šä¸€å€‹æ­¥é©Ÿå‚³å…¥çš„æ•¸æ“šã€‚
        :param context: Pipeline çš„å…±äº«ä¸Šä¸‹æ–‡ã€‚
        :return: è™•ç†å®Œæˆå¾Œï¼Œå‚³éçµ¦ä¸‹ä¸€æ­¥é©Ÿçš„æ•¸æ“šã€‚
        """
        pass


class BaseETLStep(BaseStep):
    """
    æ•¸æ“šè™•ç†ç®¡ç·šä¸­å–®å€‹æ­¥é©Ÿçš„æŠ½è±¡åŸºç¤é¡ã€‚
    æ¯å€‹ç¹¼æ‰¿æ­¤é¡çš„å…·é«”æ­¥é©Ÿï¼Œéƒ½å¿…é ˆå¯¦ç¾ä¸€å€‹ execute æ–¹æ³•ã€‚
    """

    @abstractmethod
    def execute(self, data: pd.DataFrame | None = None, **kwargs) -> pd.DataFrame | None:
        """
        åŸ·è¡Œæ­¤æ­¥é©Ÿçš„æ ¸å¿ƒé‚è¼¯ã€‚

        Args:
            data: ä¸Šä¸€å€‹æ­¥é©Ÿå‚³å…¥çš„æ•¸æ“šï¼Œå°æ–¼ç¬¬ä¸€å€‹æ­¥é©Ÿï¼Œæ­¤é …ç‚º Noneã€‚
            **kwargs: å¯é¸çš„é—œéµå­—åƒæ•¸ã€‚

        Returns:
            è™•ç†å®Œæˆå¾Œï¼Œå‚³éçµ¦ä¸‹ä¸€æ­¥é©Ÿçš„æ•¸æ“šã€‚å¦‚æœæ­¤æ­¥é©Ÿç‚ºçµ‚é»ï¼Œå¯è¿”å› Noneã€‚
        """
        pass

    def run(self, data: Any, context: Dict[str, Any]) -> Any:
        return self.execute(data, **context)
from prometheus.core.pipelines.base_step import BaseETLStep
from prometheus.core.analysis.stress_index import StressIndexCalculator
import pandas as pd
import logging
from typing import Dict, Any

from src.prometheus.core.pipelines.base_step import BaseStep
from src.prometheus.core.engines.stock_factor_engine import StockFactorEngine
from src.prometheus.core.engines.crypto_factor_engine import CryptoFactorEngine

logger = logging.getLogger(__name__)


class BuildGoldLayerStep(BaseETLStep):
    """
    å°‡å¤šå€‹ä¾†æºçš„æ•¸æ“šèåˆæˆã€Œé»ƒé‡‘å±¤ã€æ•¸æ“šçš„ç®¡ç·šæ­¥é©Ÿã€‚
    """

    def execute(self, data=None):
        print("\n--- [Step] Executing BuildGoldLayerStep ---")
        # åœ¨æ­¤åŸ·è¡Œé»ƒé‡‘å±¤æ•¸æ“šçš„è¤‡é›œETLé‚è¼¯
        # ...
        print("--- [Success] Gold layer data built. ---")
        # ç‚ºäº†æ¸¬è©¦ï¼Œè¿”å›ä¸€å€‹æˆåŠŸçš„æ¨™èªŒ
        return {"status": "gold_layer_ok"}


class CalculateStressIndexStep(BaseETLStep):
    """
    è¨ˆç®—å¸‚å ´å£“åŠ›æŒ‡æ•¸çš„ç®¡ç·šæ­¥é©Ÿã€‚
    """

    def execute(self, data=None):
        print("\n--- [Step] Executing CalculateStressIndexStep ---")
        calculator = None
        try:
            calculator = StressIndexCalculator()
            stress_index_df = calculator.calculate()
            if not stress_index_df.empty:
                latest_stress_index = stress_index_df["Stress_Index"].iloc[-1]
                print("--- [Success] Stress index calculated. ---")
                print(f"å£“åŠ›æŒ‡æ•¸ç•¶å‰å€¼: {latest_stress_index:.2f}")
                return {"status": "success", "stress_index": latest_stress_index}
            else:
                print("--- [Failed] Stress index calculation returned empty data. ---")
                return {"status": "failed", "reason": "Empty data from calculator"}
        except Exception as e:
            print(
                f"--- [Error] An error occurred during stress index calculation: {e} ---"
            )
            return {"status": "error", "reason": str(e)}
        finally:
            if calculator:
                calculator.close_all_sessions()


class RunStockFactorEngineStep(BaseStep):
    """
    ä¸€å€‹ Pipeline æ­¥é©Ÿï¼Œç”¨æ–¼åŸ·è¡Œè‚¡ç¥¨å› å­å¼•æ“ã€‚
    """

    def __init__(self, engine: StockFactorEngine):
        """
        åˆå§‹åŒ–æ­¥é©Ÿã€‚

        :param engine: ä¸€å€‹ StockFactorEngine çš„å¯¦ä¾‹ã€‚
        """
        self.engine = engine

    async def run(self, data: pd.DataFrame, context: Dict[str, Any]) -> pd.DataFrame:
        """
        å°è¼¸å…¥çš„ DataFrame åŸ·è¡Œå› å­å¼•æ“ã€‚

        :param data: åŒ…å«åƒ¹æ ¼æ•¸æ“šçš„ DataFrameã€‚
        :param context: Pipeline çš„å…±äº«ä¸Šä¸‹æ–‡ã€‚
        :return: è™•ç†å¾Œã€åŒ…å«æ–°å› å­æ¬„ä½çš„ DataFrameã€‚
        """
        logger.info("æ­£åœ¨åŸ·è¡Œ RunStockFactorEngineStep...")

        if data.empty:
            logger.warning("è¼¸å…¥çš„æ•¸æ“šç‚ºç©ºï¼Œè·³éå› å­è¨ˆç®—ã€‚")
            return data

        # StockFactorEngine çš„ run æ–¹æ³•æ˜¯æŒ‰ symbol è™•ç†çš„
        # æˆ‘å€‘éœ€è¦ç¢ºä¿è¼¸å…¥çš„ data æ˜¯å–®ä¸€ symbol çš„
        # æˆ–è€…ä¿®æ”¹å¼•æ“ä»¥è™•ç†å¤šå€‹ symbol
        # é€™è£¡æˆ‘å€‘å‡è¨­ Pipeline çš„ä¸Šä¸€æ­¥ (Loader) å·²ç¶“å°‡æ•¸æ“šæŒ‰ symbol åˆ†çµ„

        processed_data = await self.engine.run(data)

        logger.info("RunStockFactorEngineStep åŸ·è¡Œå®Œç•¢ã€‚")
        return processed_data


class RunCryptoFactorEngineStep(BaseStep):
    """
    ä¸€å€‹ Pipeline æ­¥é©Ÿï¼Œç”¨æ–¼åŸ·è¡ŒåŠ å¯†è²¨å¹£å› å­å¼•æ“ã€‚
    """

    def __init__(self, engine: CryptoFactorEngine):
        """
        åˆå§‹åŒ–æ­¥é©Ÿã€‚

        :param engine: ä¸€å€‹ CryptoFactorEngine çš„å¯¦ä¾‹ã€‚
        """
        self.engine = engine

    async def run(self, data: pd.DataFrame, context: Dict[str, Any]) -> pd.DataFrame:
        """
        å°è¼¸å…¥çš„ DataFrame åŸ·è¡Œå› å­å¼•æ“ã€‚

        :param data: åŒ…å«åƒ¹æ ¼æ•¸æ“šçš„ DataFrameã€‚
        :param context: Pipeline çš„å…±äº«ä¸Šä¸‹æ–‡ã€‚
        :return: è™•ç†å¾Œã€åŒ…å«æ–°å› å­æ¬„ä½çš„ DataFrameã€‚
        """
        logger.info("æ­£åœ¨åŸ·è¡Œ RunCryptoFactorEngineStep...")

        if data.empty:
            logger.warning("è¼¸å…¥çš„æ•¸æ“šç‚ºç©ºï¼Œè·³éå› å­è¨ˆç®—ã€‚")
            return data

        processed_data = await self.engine.run(data)

        logger.info("RunCryptoFactorEngineStep åŸ·è¡Œå®Œç•¢ã€‚")
        return processed_data
# æª”æ¡ˆ: src/prometheus/pipelines/steps/verifiers.py
# --- æŠ½è±¡ç¨‹å¼ç¢¼è‰åœ– ---

# æ¦‚å¿µï¼š
# ä¸€å€‹ç”¨æ–¼åœ¨ç®¡ç·šä¸­é©—è­‰ DataFrame å®Œæ•´æ€§çš„æ­¥é©Ÿã€‚

import pandas as pd
from prometheus.core.pipelines.base_step import BaseETLStep
from prometheus.core.logging.log_manager import LogManager

class VerifyDataFrameNotEmptyStep(BaseETLStep):
    def __init__(self):
        self.logger = LogManager.get_instance().get_logger(self.__class__.__name__)

    def execute(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        """
        åŸ·è¡Œé©—è­‰ã€‚å¦‚æœ DataFrame ç‚ºç©ºï¼Œå‰‡æ‹‹å‡ºè‡´å‘½éŒ¯èª¤ã€‚
        """
        self.logger.info("æ­£åœ¨é€²è¡Œè¨˜æ†¶é«”é©—è­‰...")

        if data.empty:
            # æ ¸å¿ƒé‚è¼¯ï¼šå¦‚æœæ•¸æ“šç‚ºç©ºï¼Œå‰‡ä¸»å‹•æ‹‹å‡ºéŒ¯èª¤ï¼Œä¸­æ–·ç®¡ç·š
            error_message = "ç®¡ç·šä¸­æ–·ï¼šå‰ä¸€ç’°ç¯€ç”¢å‡ºçš„æ•¸æ“šç‚ºç©ºï¼"
            self.logger.error(error_message)
            raise ValueError(error_message)

        self.logger.debug("è¨˜æ†¶é«”é©—è­‰é€šéï¼Œæ•¸æ“šå®Œæ•´ã€‚")
        return data # å°‡æœªç¶“ä¿®æ”¹çš„æ•¸æ“šå‚³éçµ¦ä¸‹ä¸€æ­¥
# src/prometheus/core/pipelines/steps/normalize_columns_step.py

import pandas as pd
from typing import Dict, Any
import logging

from prometheus.core.pipelines.base_step import BaseStep

logger = logging.getLogger(__name__)

class NormalizeColumnsStep(BaseStep):
    """
    ä¸€å€‹ Pipeline æ­¥é©Ÿï¼Œç”¨æ–¼å°‡ DataFrame çš„æ‰€æœ‰æ¬„ä½åç¨±æ¨™æº–åŒ–ç‚ºå°å¯«ã€‚
    å®ƒåŒæ™‚å¯¦ç¾äº† run å’Œ execute æ–¹æ³•ï¼Œä»¥å…¼å®¹ä¸åŒçš„ Pipeline è¨­è¨ˆã€‚
    """

    def __init__(self):
        """
        åˆå§‹åŒ–æ­¥é©Ÿã€‚
        """
        pass

    def _normalize(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        åŸ·è¡Œæ¨™æº–åŒ–çš„æ ¸å¿ƒé‚è¼¯ã€‚
        """
        if not isinstance(data, pd.DataFrame) or data.empty:
            logger.warning("è¼¸å…¥çš„æ•¸æ“šä¸æ˜¯ä¸€å€‹æœ‰æ•ˆçš„ DataFrame æˆ–ç‚ºç©ºï¼Œè·³éæ¨™æº–åŒ–ã€‚")
            return data

        logger.info("æ­£åœ¨å°‡æ‰€æœ‰æ¬„ä½åç¨±è½‰æ›ç‚ºå°å¯«...")
        original_columns = data.columns.tolist()
        data.columns = [col.lower() for col in original_columns]
        new_columns = data.columns.tolist()

        if original_columns != new_columns:
            logger.info(f"æ¬„ä½å·²æ¨™æº–åŒ–ï¼šå¾ {original_columns} -> {new_columns}")
        else:
            logger.info("æ‰€æœ‰æ¬„ä½åç¨±å·²ç¶“æ˜¯å°å¯«ï¼Œç„¡éœ€æ›´æ”¹ã€‚")

        return data

    async def run(self, data: pd.DataFrame, context: Dict[str, Any] = None) -> pd.DataFrame:
        """
        ç•°æ­¥åŸ·è¡Œæ¬„ä½åç¨±æ¨™æº–åŒ– (å…¼å®¹ BaseStep)ã€‚
        """
        return self._normalize(data)

    def execute(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        """
        åŒæ­¥åŸ·è¡Œæ¬„ä½åç¨±æ¨™æº–åŒ– (å…¼å®¹ BaseETLStep)ã€‚
        """
        return self._normalize(data)
# core/pipelines/steps/loaders.py
from __future__ import annotations

import datetime
import logging
import os

import duckdb
import pandas as pd
import numpy as np
from typing import List, Dict, Any

from prometheus.core.pipelines.base_step import BaseETLStep, BaseStep
from src.prometheus.core.clients.client_factory import ClientFactory

# --- ä¾è³´ç®¡ç†çš„èªªæ˜ ---
# ç†æƒ³æƒ…æ³ä¸‹ï¼Œ`DatabaseManager` å’Œ `TaifexTick` æ‡‰è©²æ˜¯ `core` çš„ä¸€éƒ¨åˆ†ï¼Œ
# æˆ–è€…ä½æ–¼ä¸€å€‹å¯è¢« `core` å’Œ `apps` å…±äº«çš„é€šç”¨åº«ä¸­ã€‚
# ç›®å‰ï¼Œ`apps.taifex_tick_loader.core` ä¾è³´æ–¼ `pydantic`ã€‚
# å¦‚æœ `pydantic` æœªå®‰è£æˆ– `PYTHONPATH` æœªæ­£ç¢ºè¨­å®šï¼Œå°è‡´ä»¥ä¸‹å°å…¥å¤±æ•—ï¼Œ
# æ­¤æ­¥é©Ÿå°‡ä½¿ç”¨ç°¡åŒ–çš„ fallback é‚è¼¯ã€‚
# è­¦å‘Šï¼šFallback é‚è¼¯å¯èƒ½èˆ‡å®Œæ•´å¯¦ç¾å­˜åœ¨å·®ç•°ã€‚

try:
    # å‡è¨­ apps ç›®éŒ„èˆ‡ core ç›®éŒ„åœ¨åŒä¸€ç´šåˆ¥ï¼Œä¸¦ä¸”éƒ½åœ¨ PYTHONPATH ä¸­
    from apps.taifex_tick_loader.core.db_manager import DatabaseManager
    from apps.taifex_tick_loader.core.schemas import TaifexTick

    # æª¢æŸ¥ TaifexTick æ˜¯å¦çœŸçš„æ˜¯ Pydantic æ¨¡å‹ï¼Œä»¥ç¢ºèªå°å…¥æ˜¯å¦ç¬¦åˆé æœŸ
    if not hasattr(TaifexTick, "model_fields"):  # Pydantic v2 check
        raise ImportError(
            "TaifexTick from apps.taifex_tick_loader.core.schemas does not appear to be a Pydantic v2 model."
        )
    IMPORTED_APP_DEPS = True
    logging.info(
        "Successfully imported DatabaseManager and TaifexTick from apps.taifex_tick_loader.core"
    )
except ImportError as e:
    logging.warning(
        f"Could not import dependencies from apps.taifex_tick_loader.core: {e}. "
        "TaifexTickLoaderStep will use simplified fallback logic for DB interaction and schema. "
        "Ensure 'pydantic' is installed and PYTHONPATH is correctly configured if full functionality is required."
    )
    IMPORTED_APP_DEPS = False

    # Fallback: Define a simple TaifexTick if import fails
    class TaifexTick(dict):  # Simplified fallback
        @staticmethod
        def model_validate(data_dict):  # Mock Pydantic v2's model_validate
            return TaifexTick(data_dict)

        def model_dump(self):  # Mock Pydantic v2's model_dump
            return self

    # Fallback: Define a simplified DatabaseManager
    class DatabaseManager:
        def __init__(self, db_path):
            self.db_path = db_path
            self.conn = None
            self.logger = logging.getLogger("FallbackDatabaseManager")

        def __enter__(self):
            self.logger.info(f"Using fallback DatabaseManager for {self.db_path}")
            if os.path.exists(self.db_path):
                self.logger.debug(
                    f"Fallback: Removing existing DB file: {self.db_path}"
                )
                os.remove(self.db_path)
            if os.path.exists(f"{self.db_path}.wal"):
                self.logger.debug(
                    f"Fallback: Removing existing WAL file: {self.db_path}.wal"
                )
                os.remove(f"{self.db_path}.wal")
            self.conn = duckdb.connect(self.db_path)
            return self

        def __exit__(self, exc_type, exc_val, exc_tb):
            if self.conn:
                self.conn.close()
                self.logger.info(
                    f"Fallback DatabaseManager connection closed for {self.db_path}"
                )

        def create_table_if_not_exists(
            self, table_name: str, model_schema_ignored
        ):  # schema ignored in fallback
            if not self.conn:
                return  # noqa: E701
            # Fallback schema based on expected DataFrame structure
            # This must align with the DataFrame created from simulated_ticks_data
            fallback_schema_sql = """
                CREATE TABLE IF NOT EXISTS {} (
                    timestamp TIMESTAMP,
                    price DOUBLE,
                    volume BIGINT,
                    instrument VARCHAR,
                    tick_type VARCHAR
                )
            """.format(
                table_name
            )  # Use format for duckdb compatibility if f-string causes issues
            self.conn.execute(fallback_schema_sql)
            self.logger.info(
                f"Fallback: Ensured table '{table_name}' with predefined schema."
            )

        def insert_ticks(self, table_name: str, ticks_input: list | pd.DataFrame):
            if not self.conn:
                return  # noqa: E701

            if isinstance(ticks_input, pd.DataFrame):
                ticks_df = ticks_input
            elif isinstance(ticks_input, list) and all(
                isinstance(t, dict) for t in ticks_input
            ):
                ticks_df = pd.DataFrame(ticks_input)
            elif isinstance(ticks_input, list) and all(
                hasattr(t, "model_dump") for t in ticks_input
            ):  # Pydantic-like objects
                ticks_df = pd.DataFrame([t.model_dump() for t in ticks_input])
            else:
                self.logger.error("Fallback: Unsupported type for insert_ticks.")
                return

            if not ticks_df.empty:
                # Ensure column types are compatible with DuckDB table
                # Convert timestamp to datetime if it's not (e.g. from Pydantic's datetime)
                if "timestamp" in ticks_df.columns:
                    ticks_df["timestamp"] = pd.to_datetime(ticks_df["timestamp"])

                self.conn.register("ticks_df_temp_view", ticks_df)
                self.conn.execute(
                    f"INSERT INTO {table_name} SELECT * FROM ticks_df_temp_view"
                )
                self.conn.unregister("ticks_df_temp_view")
                self.logger.info(
                    f"Fallback: Inserted {len(ticks_df)} records into '{table_name}'."
                )
            else:
                self.logger.info("Fallback: No data to insert.")


class LoadRawDataFromWarehouseStep(BaseETLStep):
    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)

    def execute(self, data: pd.DataFrame | None = None, **kwargs) -> pd.DataFrame:
        ticker = kwargs.get("ticker")

        # --- [è‡¨æ™‚ä¿®å¾©] ---
        # å¦‚æœæ²’æœ‰æä¾› tickerï¼Œæˆ‘å€‘å‡è¨­é€™æ˜¯ä¸€å€‹é€šç”¨ç®¡ç·šï¼Œä¸¦è¼‰å…¥ä¸€å€‹é è¨­çš„ã€å»£æ³›çš„æ•¸æ“šé›†ã€‚
        # é€™æ˜¯ç‚ºäº†è®“ P1, P2, P3 èƒ½å¤ åœ¨æ²’æœ‰ç‰¹å®š ticker çš„æƒ…æ³ä¸‹é‹è¡Œã€‚
        if not ticker:
            self.logger.info("æœªæä¾› tickerï¼Œè¼‰å…¥é€šç”¨æ•¸æ“šé›†...")
            # æ¨¡æ“¬ä¸€å€‹åŒ…å«å¤šå€‹ tickers çš„é€šç”¨æ•¸æ“šé›†
            dates = pd.to_datetime(pd.date_range(start="2023-01-01", periods=100))
            df1 = pd.DataFrame({
                "date": dates, "symbol": "SPY",
                "open": [300 + i for i in range(100)], "high": [305 + i for i in range(100)],
                "low": [295 + i for i in range(100)], "close": [302 + i for i in range(100)],
                "volume": [10000000 + i * 10000 for i in range(100)],
            })
            df2 = pd.DataFrame({
                "date": dates, "symbol": "QQQ",
                "open": [200 + i for i in range(100)], "high": [205 + i for i in range(100)],
                "low": [195 + i for i in range(100)], "close": [202 + i for i in range(100)],
                "volume": [15000000 + i * 12000 for i in range(100)],
            })
            return pd.concat([df1, df2], ignore_index=True)

        self.logger.info(f"æ­£åœ¨ç‚ºè³‡ç”¢ {ticker} è¼‰å…¥åŸå§‹æ•¸æ“š...")
        # æ¨¡æ“¬è¿”å›ä¸€å€‹åŒ…å«è™›æ“¬æ•¸æ“šçš„ DataFrame
        date_range = pd.to_datetime(
            pd.date_range(start="2022-01-01", periods=300, freq="D")
        )
        open_prices = np.random.uniform(90, 110, size=300)
        df = pd.DataFrame({
            "open": open_prices,
            "high": open_prices + np.random.uniform(0, 5, size=300),
            "low": open_prices - np.random.uniform(0, 5, size=300),
            "close": open_prices + np.random.uniform(-2, 2, size=300),
            "volume": np.random.randint(100000, 500000, size=300),
        }, index=date_range)
        df.columns = [col.lower() for col in df.columns]
        return df

class TaifexTickLoaderStep(BaseETLStep):
    def __init__(
        self,
        db_path: str = "market_data_loader_step.duckdb",
        table_name: str = "bronze_taifex_ticks_loader_step",
    ):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.db_path = db_path
        self.table_name = table_name
        self.logger.info(
            f"TaifexTickLoaderStep initialized. DB: '{self.db_path}', Table: '{self.table_name}'. Imported app deps: {IMPORTED_APP_DEPS}"
        )

    def execute(self, data: pd.DataFrame | None = None, **kwargs) -> pd.DataFrame | None:
        self.logger.info(
            f"Executing TaifexTickLoaderStep. Output to table '{self.table_name}' in db '{self.db_path}'."
        )

        simulated_ticks_data_dicts = [
            {
                "timestamp": datetime.datetime(2023, 10, 1, 9, 0, 0, 100000),
                "price": 16500.0,
                "volume": 2,
                "instrument": "TXF202310",
                "tick_type": "Trade",
            },
            {
                "timestamp": datetime.datetime(2023, 10, 1, 9, 0, 1, 200000),
                "price": 16501.0,
                "volume": 3,
                "instrument": "TXF202310",
                "tick_type": "Trade",
            },
            {
                "timestamp": datetime.datetime(2023, 10, 1, 9, 0, 2, 300000),
                "price": 16500.5,
                "volume": 1,
                "instrument": "TXF202310",
                "tick_type": "Trade",
            },
        ]

        ticks_df = pd.DataFrame(simulated_ticks_data_dicts)
        ticks_df["timestamp"] = pd.to_datetime(ticks_df["timestamp"])
        # Ensure 'volume' is integer as per fallback schema if it was float from DataFrame creation
        ticks_df["volume"] = ticks_df["volume"].astype("int64")

        self.logger.info(
            f"Successfully simulated fetching {len(ticks_df)} Tick records."
        )

        try:
            db_dir = os.path.dirname(self.db_path)
            if db_dir and not os.path.exists(db_dir):
                os.makedirs(db_dir)
                self.logger.info(f"Created directory for database: {db_dir}")

            with DatabaseManager(db_path=self.db_path) as db_manager:
                if IMPORTED_APP_DEPS:
                    # Using original DatabaseManager, expects Pydantic model for schema and list of Pydantic objects
                    db_manager.create_table_if_not_exists(self.table_name, TaifexTick)
                    simulated_pydantic_ticks = [
                        TaifexTick.model_validate(row)
                        for row in simulated_ticks_data_dicts
                    ]
                    db_manager.insert_ticks(self.table_name, simulated_pydantic_ticks)
                    self.logger.info(
                        f"Used original DatabaseManager. Wrote {len(simulated_pydantic_ticks)} records."
                    )
                else:
                    # Using fallback DatabaseManager, expects table name and DataFrame (or list of dicts)
                    db_manager.create_table_if_not_exists(
                        self.table_name, None
                    )  # Schema ignored in fallback
                    db_manager.insert_ticks(
                        self.table_name, ticks_df.to_dict("records")
                    )  # Pass list of dicts to fallback
                    self.logger.info(
                        f"Used fallback DatabaseManager. Wrote {len(ticks_df)} records."
                    )

        except Exception as e:
            self.logger.error(
                f"Error during database operation in TaifexTickLoaderStep: {e}",
                exc_info=True,
            )
            # Decide if this error should halt the pipeline or just log and return the DataFrame
            # For now, log and return DataFrame to allow pipeline to continue if DB write is not critical for next step

        self.logger.info("TaifexTickLoaderStep execution finished.")
        return ticks_df


class LoadStockDataStep(BaseStep):
    """
    ä¸€å€‹ Pipeline æ­¥é©Ÿï¼Œç”¨æ–¼å¾ yfinance åŠ è¼‰è‚¡ç¥¨æ•¸æ“šã€‚
    """

    def __init__(self, symbols: List[str], client_factory: ClientFactory):
        """
        åˆå§‹åŒ–æ­¥é©Ÿã€‚

        :param symbols: è¦åŠ è¼‰çš„è‚¡ç¥¨ä»£è™Ÿåˆ—è¡¨ã€‚
        :param client_factory: å®¢æˆ¶ç«¯å·¥å» ã€‚
        """
        self.symbols = symbols
        self.yfinance_client = client_factory.get_client('yfinance')
        self.logger = logging.getLogger(self.__class__.__name__)

    async def run(self, data: Any = None, context: Dict[str, Any] = None) -> pd.DataFrame:
        """
        åŸ·è¡Œæ•¸æ“šåŠ è¼‰ã€‚

        :param data: ä¸Šä¸€æ­¥çš„è¼¸å‡º (åœ¨æ­¤è¢«å¿½ç•¥)ã€‚
        :param context: Pipeline çš„å…±äº«ä¸Šä¸‹æ–‡ã€‚
        :return: åŒ…å«æ‰€æœ‰è‚¡ç¥¨æ•¸æ“šçš„å–®ä¸€ DataFrameã€‚
        """
        self.logger.info(f"é–‹å§‹å¾ yfinance åŠ è¼‰è‚¡ç¥¨æ•¸æ“šï¼Œç›®æ¨™: {self.symbols}")
        all_data = []

        for symbol in self.symbols:
            try:
                self.logger.debug(f"æ­£åœ¨ç‚º {symbol} ç²å–æ•¸æ“š...")
                stock_data = await self.yfinance_client.fetch_data(symbol, period="1y") # è¼‰å…¥ä¸€å¹´æ•¸æ“šä½œç‚ºç¯„ä¾‹
                if stock_data.empty:
                    self.logger.warning(f"ç„¡æ³•ç‚º {symbol} ç²å–æ•¸æ“šï¼Œå¯èƒ½è©²ä»£è™Ÿç„¡æ•ˆæˆ–ç„¡æ•¸æ“šã€‚")
                    continue

                stock_data['symbol'] = symbol
                all_data.append(stock_data)
                self.logger.debug(f"æˆåŠŸåŠ è¼‰ {symbol} çš„ {len(stock_data)} ç­†æ•¸æ“šã€‚")

            except Exception as e:
                self.logger.error(f"åŠ è¼‰ {symbol} æ•¸æ“šæ™‚å‡ºéŒ¯: {e}", exc_info=True)

        if not all_data:
            self.logger.error("æœªèƒ½åŠ è¼‰ä»»ä½•è‚¡ç¥¨æ•¸æ“šã€‚")
            # è¿”å›ä¸€å€‹ç©ºçš„ DataFrameï¼Œä¸‹æ¸¸æ­¥é©Ÿæ‡‰èƒ½è™•ç†é€™ç¨®æƒ…æ³
            return pd.DataFrame()

        # å°‡æ‰€æœ‰æ•¸æ“šåˆä½µæˆä¸€å€‹å¤§çš„ DataFrame
        combined_df = pd.concat(all_data)
        self.logger.info(f"æˆåŠŸåŠ è¼‰ä¸¦åˆä½µäº† {len(all_data)} æ”¯è‚¡ç¥¨çš„æ•¸æ“šã€‚")

        # --- [ä¿®å¾©] ---
        # å°‡æ‰€æœ‰åˆ—åæ¨™æº–åŒ–ç‚ºå°å¯«ï¼Œä»¥é¿å…èˆ‡æ•¸æ“šåº«æ¨¡å¼çš„å¤§å°å¯«ä¸åŒ¹é…å•é¡Œ
        combined_df.columns = [col.lower() for col in combined_df.columns]

        # é‡ç½®ç´¢å¼•ï¼Œå› ç‚º yfinance è¿”å›çš„æ•¸æ“šä¸­ï¼Œæ—¥æœŸæ˜¯ç´¢å¼•
        combined_df = combined_df.reset_index()

        return combined_df


class LoadCryptoDataStep(BaseStep):
    """
    ä¸€å€‹ Pipeline æ­¥é©Ÿï¼Œç”¨æ–¼å¾ yfinance åŠ è¼‰åŠ å¯†è²¨å¹£æ•¸æ“šã€‚
    """

    def __init__(self, symbols: List[str], client_factory: ClientFactory):
        """
        åˆå§‹åŒ–æ­¥é©Ÿã€‚

        :param symbols: è¦åŠ è¼‰çš„åŠ å¯†è²¨å¹£ä»£è™Ÿåˆ—è¡¨ (ä¾‹å¦‚: ['BTC-USD', 'ETH-USD'])ã€‚
        :param client_factory: å®¢æˆ¶ç«¯å·¥å» ã€‚
        """
        self.symbols = symbols
        self.yfinance_client = client_factory.get_client('yfinance')
        self.logger = logging.getLogger(self.__class__.__name__)

    async def run(self, data: Any = None, context: Dict[str, Any] = None) -> pd.DataFrame:
        """
        åŸ·è¡Œæ•¸æ“šåŠ è¼‰ã€‚

        :param data: ä¸Šä¸€æ­¥çš„è¼¸å‡º (åœ¨æ­¤è¢«å¿½ç•¥)ã€‚
        :param context: Pipeline çš„å…±äº«ä¸Šä¸‹æ–‡ã€‚
        :return: åŒ…å«æ‰€æœ‰åŠ å¯†è²¨å¹£æ•¸æ“šçš„å–®ä¸€ DataFrameã€‚
        """
        self.logger.info(f"é–‹å§‹å¾ yfinance åŠ è¼‰åŠ å¯†è²¨å¹£æ•¸æ“šï¼Œç›®æ¨™: {self.symbols}")
        all_data = []

        for symbol in self.symbols:
            try:
                self.logger.debug(f"æ­£åœ¨ç‚º {symbol} ç²å–æ•¸æ“š...")
                # ç‚ºåŠ å¯†è²¨å¹£ç²å–æ›´é•·çš„æ­·å²æ•¸æ“šä»¥é€²è¡Œç›¸é—œæ€§è¨ˆç®—
                crypto_data = await self.yfinance_client.fetch_data(symbol, period="2y")
                if crypto_data.empty:
                    self.logger.warning(f"ç„¡æ³•ç‚º {symbol} ç²å–æ•¸æ“šï¼Œå¯èƒ½è©²ä»£è™Ÿç„¡æ•ˆæˆ–ç„¡æ•¸æ“šã€‚")
                    continue

                crypto_data['symbol'] = symbol
                all_data.append(crypto_data)
                self.logger.debug(f"æˆåŠŸåŠ è¼‰ {symbol} çš„ {len(crypto_data)} ç­†æ•¸æ“šã€‚")

            except Exception as e:
                self.logger.error(f"åŠ è¼‰ {symbol} æ•¸æ“šæ™‚å‡ºéŒ¯: {e}", exc_info=True)

        if not all_data:
            self.logger.error("æœªèƒ½åŠ è¼‰ä»»ä½•åŠ å¯†è²¨å¹£æ•¸æ“šã€‚")
            return pd.DataFrame()

        combined_df = pd.concat(all_data)
        self.logger.info(f"æˆåŠŸåŠ è¼‰ä¸¦åˆä½µäº† {len(all_data)} ç¨®åŠ å¯†è²¨å¹£çš„æ•¸æ“šã€‚")

        # --- [ä¿®å¾©] ---
        # å°‡æ‰€æœ‰åˆ—åæ¨™æº–åŒ–ç‚ºå°å¯«ï¼Œä»¥é¿å…èˆ‡æ•¸æ“šåº«æ¨¡å¼çš„å¤§å°å¯«ä¸åŒ¹é…å•é¡Œ
        combined_df.columns = [col.lower() for col in combined_df.columns]

        # é‡ç½®ç´¢å¼•ï¼Œå› ç‚º yfinance è¿”å›çš„æ•¸æ“šä¸­ï¼Œæ—¥æœŸæ˜¯ç´¢å¼•
        combined_df = combined_df.reset_index()

        return combined_df


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.DEBUG,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    test_db_path = "temp_test_tick_loader_step.duckdb"
    # Clean up before test
    if os.path.exists(test_db_path):
        os.remove(test_db_path)  # noqa: E701
    if os.path.exists(f"{test_db_path}.wal"):
        os.remove(f"{test_db_path}.wal")  # noqa: E701

    loader_step = TaifexTickLoaderStep(db_path=test_db_path, table_name="test_ticks")
    loaded_data = loader_step.execute()

    if loaded_data is not None:
        print("\n--- Loaded Data (DataFrame) ---")
        print(loaded_data)
        print(f"\nData types:\n{loaded_data.dtypes}")

        if os.path.exists(test_db_path):
            print(
                f"\n--- Verifying data in database '{test_db_path}', table '{loader_step.table_name}' ---"
            )
            try:
                with duckdb.connect(test_db_path) as conn:
                    count = conn.execute(
                        f"SELECT COUNT(*) FROM {loader_step.table_name}"
                    ).fetchone()[0]
                    print(f"Number of records in table: {count}")
                    assert count == len(
                        loaded_data
                    ), "Mismatch in DB count and DataFrame length"
                    if count > 0:
                        sample_records = conn.execute(
                            f"SELECT * FROM {loader_step.table_name} LIMIT 3"
                        ).df()
                        print("Sample records from DB:")
                        print(sample_records)
            except Exception as e:
                print(f"Error verifying data in DB: {e}")
    else:
        print("Loader step did not return data.")

    # Clean up after test
    # if os.path.exists(test_db_path): os.remove(test_db_path)
    # if os.path.exists(f"{test_db_path}.wal"): os.remove(f"{test_db_path}.wal")
    # print(f"\nCleaned up test database: {test_db_path}")
import pandas as pd
from prometheus.core.pipelines.base_step import BaseETLStep, BaseStep
from prometheus.core.logging.log_manager import LogManager
import duckdb
import os
from typing import Dict, Any
from src.prometheus.core.db.db_manager import DBManager


class SaveFactorsToWarehouseStep(BaseETLStep):
    def __init__(self, table_name: str, db_path: str = "data/analytics_warehouse/factors.duckdb"):
        self.table_name = table_name
        self.db_path = db_path
        self.logger = LogManager.get_instance().get_logger(self.__class__.__name__)

        # ç¢ºä¿æ•¸æ“šåº«ç›®éŒ„å­˜åœ¨
        db_dir = os.path.dirname(self.db_path)
        if not os.path.exists(db_dir):
            os.makedirs(db_dir)

    def execute(self, data: pd.DataFrame, **kwargs) -> pd.DataFrame:
        """
        å°‡å› å­å„²å­˜åˆ°æ•¸æ“šå€‰åº«ã€‚
        """
        ticker = kwargs.get("ticker")
        if not ticker:
            self.logger.warning("åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾ä¸åˆ° tickerï¼Œç„¡æ³•å„²å­˜ã€‚")
            return data

        self.logger.info(f"æ­£åœ¨å°‡å› å­å„²å­˜åˆ°æ•¸æ“šåº« '{self.db_path}' çš„è¡¨æ ¼ '{self.table_name}' for ticker {ticker}...")
        if data.empty:
            self.logger.warning("æ•¸æ“šç‚ºç©ºï¼Œæ²’æœ‰å› å­å¯ä»¥å„²å­˜ã€‚")
        else:
            try:
                with duckdb.connect(self.db_path) as con:
                    # Add ticker column
                    data_to_save = data.copy()
                    data_to_save['ticker'] = ticker

                    # Check if table exists
                    res = con.execute(f"SELECT table_name FROM information_schema.tables WHERE table_name = '{self.table_name}'").fetchone()
                    if res: # Table exists, so append
                        # Remove existing data for the same ticker to avoid duplicates
                        con.execute(f"DELETE FROM {self.table_name} WHERE ticker = '{ticker}'")
                        con.register('factors_df', data_to_save.reset_index())
                        con.execute(f"INSERT INTO {self.table_name} SELECT * FROM factors_df")
                    else: # Table does not exist, so create
                        con.register('factors_df', data_to_save.reset_index())
                        con.execute(f"CREATE TABLE {self.table_name} AS SELECT * FROM factors_df")

                    self.logger.info(f"æˆåŠŸå°‡ {len(data)} ç­†å› å­å„²å­˜åˆ° '{self.table_name}' for ticker {ticker}ã€‚")
            except Exception as e:
                self.logger.error(f"å„²å­˜å› å­æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
                raise
        return data


class SaveToWarehouseStep(BaseStep):
    """
    ä¸€å€‹ Pipeline æ­¥é©Ÿï¼Œç”¨æ–¼å°‡ DataFrame å„²å­˜åˆ°è³‡æ–™å€‰å„²ã€‚
    """

    def __init__(self, db_manager: DBManager, table_name: str):
        """
        åˆå§‹åŒ–æ­¥é©Ÿã€‚

        :param db_manager: è³‡æ–™åº«ç®¡ç†å™¨ã€‚
        :param table_name: è¦å„²å­˜çš„ç›®æ¨™è¡¨æ ¼åç¨±ã€‚
        """
        self.db_manager = db_manager
        self.table_name = table_name
        self.logger = LogManager.get_instance().get_logger(self.__class__.__name__)

    async def run(self, data: pd.DataFrame, context: Dict[str, Any]) -> pd.DataFrame:
        """
        å°‡ DataFrame å„²å­˜åˆ°è³‡æ–™å€‰å„²ã€‚

        :param data: è¦å„²å­˜çš„ DataFrameã€‚
        :param context: Pipeline çš„å…±äº«ä¸Šä¸‹æ–‡ã€‚
        :return: æœªç¶“ä¿®æ”¹çš„åŸå§‹ DataFrameã€‚
        """
        if data.empty:
            self.logger.warning("æ•¸æ“šç‚ºç©ºï¼Œæ²’æœ‰å¯ä»¥å„²å­˜çš„å…§å®¹ã€‚")
            return data

        try:
            self.logger.info(f"æ­£åœ¨å°‡ {len(data)} ç­†æ•¸æ“šå„²å­˜åˆ°è¡¨æ ¼ '{self.table_name}'...")
            self.db_manager.save_data(data, self.table_name)
            self.logger.info("æ•¸æ“šå„²å­˜æˆåŠŸã€‚")
        except Exception as e:
            self.logger.error(f"å„²å­˜æ•¸æ“šåˆ°å€‰å„²æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            raise

        return data
# core/pipelines/steps/aggregators.py
from __future__ import annotations

import datetime  # Required for type hinting and potentially for internal logic
import logging

import pandas as pd

from ..base_step import BaseETLStep

# We will not import TimeAggregator from apps.time_aggregator.core.aggregator
# Instead, its core logic will be embedded into the TimeAggregatorStep.


class TimeAggregatorStep(BaseETLStep):
    """
    ä¸€å€‹ ETL æ­¥é©Ÿï¼Œç”¨æ–¼å°‡ Tick æ•¸æ“šèšåˆç‚ºæŒ‡å®šæ™‚é–“é–“éš”çš„ OHLCV æ•¸æ“šã€‚
    """

    def __init__(
        self, aggregation_level: str = "1Min", db_writer_step: BaseETLStep | None = None
    ):
        """
        åˆå§‹åŒ– TimeAggregatorStepã€‚

        Args:
            aggregation_level (str): èšåˆçš„æ™‚é–“ç´šåˆ¥ï¼Œä¾‹å¦‚ "1Min", "5Min"ã€‚
                                     ç›®å‰ä¸»è¦å¯¦ç¾ "1Min"ã€‚
            db_writer_step (BaseETLStep | None): å¯é¸çš„ä¸‹ä¸€æ­¥é©Ÿï¼Œç”¨æ–¼å°‡èšåˆæ•¸æ“šå¯«å…¥æ•¸æ“šåº«ã€‚
                                                 æ­¤åƒæ•¸æ˜¯ç‚ºäº†æœªä¾†æ“´å±•ï¼Œç›®å‰ execute æ–¹æ³•ç›´æ¥è¿”å› DataFrameã€‚
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        # Correcting Pandas frequency string: 'T' or 'min' for minutes.
        # Pandas documentation suggests 'min' is preferred over 'T' for minutes.
        if "Min" in aggregation_level:
            self.aggregation_level_pd = aggregation_level.replace("Min", "min")
        elif "H" in aggregation_level:  # For hours
            self.aggregation_level_pd = aggregation_level.replace(
                "H", "h"
            )  # Pandas uses lowercase 'h'
        else:  # Add more rules or a default if needed
            self.aggregation_level_pd = (
                aggregation_level  # Use as is if no specific replacement rule
            )

        self.db_writer_step = (
            db_writer_step  # Not used in current execute, but for future design
        )
        self.logger.info(
            f"TimeAggregatorStep initialized for aggregation level: {aggregation_level} (Pandas rule: {self.aggregation_level_pd})"
        )

    def execute(self, data: pd.DataFrame | None = None) -> pd.DataFrame | None:
        """
        åŸ·è¡Œæ™‚é–“åºåˆ—èšåˆã€‚

        Args:
            data: ä¸Šä¸€å€‹æ­¥é©Ÿå‚³å…¥çš„ Tick æ•¸æ“š DataFrameã€‚
                  é æœŸåŒ…å« 'timestamp', 'price', 'volume', 'instrument' æ¬„ä½ã€‚
                  'timestamp' æ¬„ä½å¿…é ˆæ˜¯ datetime-likeã€‚

        Returns:
            ä¸€å€‹åŒ…å«èšåˆå¾Œ OHLCV æ•¸æ“šçš„ Pandas DataFrameã€‚
            æ¬„ä½ç‚º ['timestamp', 'instrument', 'open', 'high', 'low', 'close', 'volume']ã€‚
            å¦‚æœè¼¸å…¥æ•¸æ“šç‚º None æˆ–ç‚ºç©ºï¼Œå‰‡è¿”å› None æˆ–ç©º DataFrameã€‚
        """
        if data is None or data.empty:
            self.logger.warning(
                "Input data is None or empty. TimeAggregatorStep cannot proceed."
            )
            return pd.DataFrame(
                columns=[
                    "timestamp",
                    "instrument",
                    "open",
                    "high",
                    "low",
                    "close",
                    "volume",
                ]
            )

        self.logger.info(f"Executing TimeAggregatorStep with {len(data)} tick records.")

        expected_columns = ["timestamp", "price", "volume", "instrument"]
        if not all(col in data.columns for col in expected_columns):
            missing_cols = [col for col in expected_columns if col not in data.columns]
            self.logger.error(
                f"Input DataFrame is missing required columns: {missing_cols}"
            )
            raise ValueError(
                f"Input DataFrame for TimeAggregatorStep is missing columns: {missing_cols}"
            )

        if not pd.api.types.is_datetime64_any_dtype(data["timestamp"]):
            self.logger.info(
                "Attempting to convert 'timestamp' column to datetime objects."
            )
            try:
                data["timestamp"] = pd.to_datetime(data["timestamp"])
                self.logger.info(
                    "Successfully converted 'timestamp' column to datetime objects."
                )
            except Exception as e:
                self.logger.error(
                    f"Failed to convert 'timestamp' column to datetime: {e}",
                    exc_info=True,
                )
                raise TypeError(
                    "Input DataFrame 'timestamp' column must be datetime-like or convertible to datetime."
                )

        ticks_df = data.copy()

        if (
            not isinstance(ticks_df.index, pd.DatetimeIndex)
            or ticks_df.index.name != "timestamp"
        ):
            if "timestamp" in ticks_df.columns:
                ticks_df = ticks_df.set_index("timestamp")
            else:
                self.logger.error(
                    "Critical: 'timestamp' column not found for setting index after initial checks."
                )
                raise ValueError(
                    "Cannot set 'timestamp' as index as it's not available."
                )

        ohlcv_list = []
        if "instrument" not in ticks_df.columns:
            self.logger.error("Critical: 'instrument' column not found for grouping.")
            raise ValueError("'instrument' column is required for aggregation.")

        for instrument, group_df in ticks_df.groupby("instrument"):
            self.logger.debug(
                f"Aggregating for instrument: {instrument}, {len(group_df)} ticks, rule: {self.aggregation_level_pd}"
            )

            agg_rules = {"price": ["first", "max", "min", "last"], "volume": "sum"}

            try:
                # Ensure the index is sorted for resampling to work correctly and avoid UserWarning
                group_df_sorted = group_df.sort_index()
                resampled_group = group_df_sorted.resample(
                    self.aggregation_level_pd
                ).agg(agg_rules)
            except Exception as e:
                self.logger.error(
                    f"Error during resampling for instrument {instrument} with rule '{self.aggregation_level_pd}': {e}",
                    exc_info=True,
                )
                continue

            if resampled_group.empty:
                self.logger.debug(
                    f"No data after resampling for instrument {instrument} at {self.aggregation_level_pd} interval."
                )
                continue

            resampled_group.columns = [
                "_".join(col).strip() for col in resampled_group.columns.values
            ]

            resampled_group.rename(
                columns={
                    "price_first": "open",
                    "price_max": "high",
                    "price_min": "low",
                    "price_last": "close",
                    "volume_sum": "volume",
                },
                inplace=True,
            )

            resampled_group["instrument"] = instrument
            ohlcv_list.append(resampled_group)

        if not ohlcv_list:
            self.logger.warning(
                "Aggregation resulted in an empty list. No OHLCV data produced."
            )
            return pd.DataFrame(
                columns=[
                    "timestamp",
                    "instrument",
                    "open",
                    "high",
                    "low",
                    "close",
                    "volume",
                ]
            )

        final_ohlcv_df = pd.concat(ohlcv_list)
        final_ohlcv_df.reset_index(inplace=True)

        output_columns = [
            "timestamp",
            "instrument",
            "open",
            "high",
            "low",
            "close",
            "volume",
        ]
        # Ensure all expected columns are present, fill with NaN if any are missing (e.g. if agg_rules somehow failed for a column)
        for col in output_columns:
            if col not in final_ohlcv_df.columns:
                final_ohlcv_df[col] = (
                    pd.NA
                )  # Or appropriate default like 0 for volume, NaN for prices

        final_ohlcv_df = final_ohlcv_df[output_columns]
        final_ohlcv_df.dropna(
            subset=["open", "high", "low", "close"], how="all", inplace=True
        )
        # Convert volume to integer type if it's float after aggregation (e.g. if NaNs were present then filled)
        if "volume" in final_ohlcv_df.columns:
            final_ohlcv_df["volume"] = (
                final_ohlcv_df["volume"].fillna(0).astype("int64")
            )

        self.logger.info(
            f"TimeAggregatorStep successfully aggregated {len(data)} ticks into {len(final_ohlcv_df)} OHLCV records."
        )

        return final_ohlcv_df


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.DEBUG,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    sample_ticks_data = [
        {
            "timestamp": datetime.datetime(2023, 1, 1, 9, 0, 5),
            "price": 100.0,
            "volume": 10,
            "instrument": "INSTR_A",
        },
        {
            "timestamp": datetime.datetime(2023, 1, 1, 9, 0, 15),
            "price": 101.0,
            "volume": 5,
            "instrument": "INSTR_A",
        },
        {
            "timestamp": datetime.datetime(2023, 1, 1, 9, 0, 30),
            "price": 99.0,
            "volume": 8,
            "instrument": "INSTR_A",
        },
        {
            "timestamp": datetime.datetime(2023, 1, 1, 9, 0, 50),
            "price": 100.5,
            "volume": 12,
            "instrument": "INSTR_A",
        },
        {
            "timestamp": datetime.datetime(2023, 1, 1, 9, 1, 10),
            "price": 102.0,
            "volume": 7,
            "instrument": "INSTR_A",
        },
        {
            "timestamp": datetime.datetime(2023, 1, 1, 9, 0, 10),
            "price": 2000.0,
            "volume": 20,
            "instrument": "INSTR_B",
        },
        {
            "timestamp": datetime.datetime(2023, 1, 1, 9, 0, 40),
            "price": 1995.0,
            "volume": 15,
            "instrument": "INSTR_B",
        },
    ]
    input_df = pd.DataFrame(sample_ticks_data)
    # 'timestamp' column is already datetime objects from creation

    print("--- Input DataFrame (Sample Ticks) ---")
    print(input_df)
    print(f"\nInput types:\n{input_df.dtypes}")

    aggregator_step = TimeAggregatorStep(aggregation_level="1Min")
    aggregated_data = aggregator_step.execute(input_df)

    if aggregated_data is not None:
        print("\n--- Aggregated Data (OHLCV) ---")
        print(aggregated_data)
        print(f"\nAggregated types:\n{aggregated_data.dtypes}")

        if not aggregated_data.empty:
            assert "timestamp" in aggregated_data.columns
            assert pd.api.types.is_datetime64_any_dtype(aggregated_data["timestamp"])
            assert "instrument" in aggregated_data.columns
            assert "open" in aggregated_data.columns
            assert "volume" in aggregated_data.columns
            assert pd.api.types.is_integer_dtype(aggregated_data["volume"])

            instr_a_first_min = aggregated_data[
                (aggregated_data["instrument"] == "INSTR_A")
                & (aggregated_data["timestamp"] == pd.Timestamp("2023-01-01 09:00:00"))
            ]
            if not instr_a_first_min.empty:
                print("\nINSTR_A 09:00:00 OHLCV:")
                print(instr_a_first_min)
                assert instr_a_first_min.iloc[0]["open"] == 100.0
                assert instr_a_first_min.iloc[0]["high"] == 101.0
                assert instr_a_first_min.iloc[0]["low"] == 99.0
                assert instr_a_first_min.iloc[0]["close"] == 100.5
                assert instr_a_first_min.iloc[0]["volume"] == 35  # 10+5+8+12
                print("INSTR_A 09:00:00 assertions passed.")
            else:
                print("Warning: INSTR_A 09:00:00 data not found in aggregated output.")
    else:
        print("Aggregator step did not return data.")
# src/prometheus/core/pipelines/steps/splitters.py

import pandas as pd
import logging
from typing import Dict, Any, List

from src.prometheus.core.pipelines.base_step import BaseStep

logger = logging.getLogger(__name__)


class GroupBySymbolStep(BaseStep):
    """
    ä¸€å€‹ Pipeline æ­¥é©Ÿï¼Œç”¨æ–¼å°‡ DataFrame æŒ‰ 'symbol' æ¬„ä½åˆ†çµ„ã€‚
    """

    async def run(self, data: pd.DataFrame, context: Dict[str, Any]):
        """
        å°‡è¼¸å…¥çš„ DataFrame æŒ‰ 'symbol' åˆ†çµ„ã€‚

        :param data: åŒ…å« 'symbol' æ¬„ä½çš„ DataFrameã€‚
        :param context: Pipeline çš„å…±äº«ä¸Šä¸‹æ–‡ã€‚
        :return: ä¸€å€‹ DataFrame çš„åˆ—è¡¨ï¼Œæ¯å€‹ DataFrame å°æ‡‰ä¸€å€‹ symbolã€‚
        """
        logger.info("æ­£åœ¨åŸ·è¡Œ GroupBySymbolStep...")

        if 'symbol' not in data.columns:
            raise ValueError("è¼¸å…¥çš„ DataFrame å¿…é ˆåŒ…å« 'symbol' æ¬„ä½ã€‚")

        grouped = data.groupby('symbol')

        logger.info(f"æˆåŠŸå°‡æ•¸æ“šåˆ†ç‚º {len(grouped)} çµ„ã€‚")
        for _, group in grouped:
            yield group
# core/pipelines/pipeline.py
from __future__ import annotations

import logging
from typing import List, Any
import pandas as pd

from prometheus.core.pipelines.base_step import BaseETLStep, BaseStep


class DataPipeline:
    """
    ä¸€å€‹å¯çµ„åˆçš„æ•¸æ“šè™•ç†ç®¡ç·šåŸ·è¡Œå™¨ã€‚
    å®ƒå¯ä»¥æŒ‰é †åºåŸ·è¡Œä¸€ç³»åˆ—çš„ ETL æ­¥é©Ÿã€‚
    """

    def __init__(self, steps: List[BaseETLStep]):
        """
        åˆå§‹åŒ–æ•¸æ“šç®¡ç·šã€‚

        Args:
            steps: ä¸€å€‹åŒ…å«å¤šå€‹ BaseETLStep å­é¡å¯¦ä¾‹çš„åˆ—è¡¨ã€‚
        """
        self._steps = steps
        self.logger = logging.getLogger(self.__class__.__name__)

    async def run(self, initial_data=None, context: dict | None = None) -> None:
        """
        åŸ·è¡Œå®Œæ•´çš„æ•°æ®è™•ç†æµç¨‹ã€‚
        """
        import asyncio
        data = initial_data
        if context is None:
            context = {}

        self.logger.info(f"æ•¸æ“šç®¡ç·šé–‹å§‹åŸ·è¡Œï¼Œå…± {len(self._steps)} å€‹æ­¥é©Ÿã€‚")
        # step_name åœ¨å¾ªç’°å¤–éƒ¨å¯èƒ½æœªå®šç¾©ï¼Œå› æ­¤åœ¨æ­¤è™•åˆå§‹åŒ–
        step_name = "Unknown"
        try:
            for i, step in enumerate(self._steps, 1):
                # ä¿®æ­£: ç²å–é¡åæ‡‰ç‚º step.__class__.__name__
                step_name = step.__class__.__name__
                self.logger.info(
                    f"--- [æ­¥é©Ÿ {i}/{len(self._steps)}]ï¼šæ­£åœ¨åŸ·è¡Œ {step_name} ---"
                )
                if asyncio.iscoroutinefunction(step.execute):
                    data = await step.execute(data, **context)
                else:
                    data = step.execute(data, **context)
                self.logger.info(f"æ­¥é©Ÿ {step_name} åŸ·è¡Œå®Œç•¢ã€‚")

            self.logger.info("æ•¸æ“šç®¡ç·šæ‰€æœ‰æ­¥é©Ÿå‡å·²æˆåŠŸåŸ·è¡Œã€‚")
            return data  # è¿”å›æœ€å¾Œä¸€å€‹æ­¥é©Ÿçš„çµæœ

        except Exception as e:
            self.logger.error(
                f"æ•¸æ“šç®¡ç·šåœ¨åŸ·è¡Œæ­¥é©Ÿ '{step_name}' æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼š{e}", exc_info=True
            )
            # è€ƒæ…®åˆ°ç®¡ç·šåŸ·è¡Œå¤±æ•—æ™‚çš„å¥å£¯æ€§ï¼Œé€™è£¡å¯ä»¥é¸æ“‡é‡æ–°æ‹‹å‡ºç•°å¸¸
            # æˆ–è€…æ ¹æ“šéœ€æ±‚æ±ºå®šæ˜¯å¦è¦æŠ‘åˆ¶ç•°å¸¸ä¸¦ç¹¼çºŒï¼ˆå„˜ç®¡é€šå¸¸å»ºè­°æ‹‹å‡ºï¼‰
            raise


class Pipeline:
    """
    ä¸€å€‹åŒæ­¥çš„ã€å¯çµ„åˆçš„æ•¸æ“šè™•ç†ç®¡ç·šåŸ·è¡Œå™¨ã€‚
    """

    def __init__(self, steps: List[BaseStep]):
        """
        åˆå§‹åŒ–ç®¡ç·šã€‚

        :param steps: ä¸€å€‹åŒ…å«å¤šå€‹ BaseStep å­é¡å¯¦ä¾‹çš„åˆ—è¡¨ã€‚
        """
        self.steps = steps
        self.logger = logging.getLogger(self.__class__.__name__)
        self.context = {}

    async def run(self, initial_data: Any = None) -> Any:
        """
        åŸ·è¡Œå®Œæ•´çš„æ•¸æ“šè™•ç†æµç¨‹ã€‚
        """
        data = initial_data
        self.logger.info(f"Pipeline é–‹å§‹åŸ·è¡Œï¼Œå…± {len(self.steps)} å€‹æ­¥é©Ÿã€‚")

        for i, step in enumerate(self.steps, 1):
            step_name = step.__class__.__name__
            self.logger.info(f"--- [æ­¥é©Ÿ {i}/{len(self.steps)}]ï¼šæ­£åœ¨åŸ·è¡Œ {step_name} ---")

            try:
                if isinstance(data, pd.DataFrame):
                    result = step.run(data, self.context)
                    if hasattr(result, '__aiter__'):
                        processed_list = [item async for item in result]
                        data = pd.concat(processed_list) if all(isinstance(i, pd.DataFrame) for i in processed_list) else processed_list
                    else:
                        data = await result
                elif isinstance(data, list):
                    processed_list = [await step.run(item, self.context) for item in data]
                    data = pd.concat(processed_list) if all(isinstance(i, pd.DataFrame) for i in processed_list) else processed_list
                else:
                    data = await step.run(data, self.context)

                self.logger.info(f"æ­¥é©Ÿ {step_name} åŸ·è¡Œå®Œç•¢ã€‚")

            except Exception as e:
                self.logger.error(
                    f"Pipeline åœ¨åŸ·è¡Œæ­¥é©Ÿ '{step_name}' æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼š{e}", exc_info=True
                )
                raise

        self.logger.info("Pipeline æ‰€æœ‰æ­¥é©Ÿå‡å·²æˆåŠŸåŸ·è¡Œã€‚")
        return data
import time

from rich.table import Table


def generate_status_table(metrics: dict) -> Table:
    table = Table(title=f"ä½œæˆ°æƒ…å ±ä¸­å¿ƒ (æ›´æ–°æ–¼ {time.strftime('%H:%M:%S')})")
    table.add_column("ç›£æ§æŒ‡æ¨™", justify="right", style="cyan", no_wrap=True)
    table.add_column("æ•¸å€¼", style="magenta")
    total_events = metrics.get("total_events", 0)
    table.add_row("äº‹ä»¶æµç¸½é‡", str(total_events))
    table.add_row("---", "---")
    checkpoints = metrics.get("checkpoints", {})
    if not checkpoints:
        table.add_row("æ¶ˆè²»è€…é€²åº¦", "[yellow]æ­£åœ¨ç­‰å¾…æ¶ˆè²»è€…ä¸Šç·š...[/yellow]")
    else:
        for consumer_id, last_id in checkpoints.items():
            lag = total_events - last_id
            table.add_row(f"æ¶ˆè²»è€… [{consumer_id}] é€²åº¦", str(last_id))
            table.add_row(
                f"æ¶ˆè²»è€… [{consumer_id}] å»¶é²",
                f"[red]{lag}[/red]" if lag > 10 else f"[green]{lag}[/green]",
            )
            table.add_row("---", "---")
    return table
# -*- coding: utf-8 -*-
from typing import Any, Dict

import yaml

from prometheus.core.logging.log_manager import LogManager

logger = LogManager.get_instance().get_logger("ConfigManager")


class ConfigManager:
    _instance = None
    _config: Dict[str, Any] = {}

    def __new__(cls, config_path: str = "config.yml"):
        if cls._instance is None:
            cls._instance = super(ConfigManager, cls).__new__(cls)
        cls._instance._load_config(config_path)
        return cls._instance

    def _load_config(self, config_path: str):
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                self.__class__._config.update(yaml.safe_load(f))
                logger.info(f"è¨­å®šæª” '{config_path}' è¼‰å…¥æˆåŠŸã€‚")
        except FileNotFoundError:
            logger.warning(f"æ‰¾ä¸åˆ°è¨­å®šæª” '{config_path}'ã€‚å°‡ä½¿ç”¨é è¨­å€¼æˆ–ç©ºå€¼ã€‚")
        except Exception as e:
            logger.error(f"è¼‰å…¥è¨­å®šæª” '{config_path}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)

    def get(self, key: str, default: Any = None) -> Any:
        keys = key.split(".")
        value = self.__class__._config
        for k in keys:
            if isinstance(value, dict) and k in value:
                value = value[k]
            else:
                return default
        return value


# å»ºç«‹ä¸€å€‹å…¨åŸŸå¯¦ä¾‹ï¼Œæ–¹ä¾¿åœ¨å°ˆæ¡ˆä¸­å„è™•ç›´æ¥å°å…¥ä½¿ç”¨
# ç¢ºä¿åœ¨æ¨¡çµ„åŠ è¼‰æ™‚ï¼ŒConfigManager('config.yml') è¢«èª¿ç”¨ä¸€æ¬¡ä»¥åŠ è¼‰é…ç½®ã€‚
config = ConfigManager(config_path="config.yml")  # æŒ‡å®šè·¯å¾‘


def get_fred_api_key() -> str:
    """ä¸€å€‹å°ˆç”¨çš„è¼”åŠ©å‡½æ•¸ï¼Œç”¨æ–¼å®‰å…¨åœ°ç²å– FRED API é‡‘é‘°ã€‚"""
    key = config.get("api_keys.fred")
    if not key or "YOUR_REAL" in key:
        error_msg = "FRED API é‡‘é‘°æœªåœ¨ config.yml ä¸­æ­£ç¢ºè¨­å®šæˆ–ä»ç‚ºé ç•™ä½ç½®ã€‚"
        logger.error(error_msg)
        raise ValueError(error_msg)
    return key


if __name__ == "__main__":
    print("--- è¨­å®šæª”ç®¡ç†å™¨æ¸¬è©¦ ---")
    # é‡æ–°è¼‰å…¥è¨­å®šä»¥ç¢ºä¿æ¸¬è©¦æ™‚æ˜¯æœ€æ–°çš„ï¼ˆæˆ–å‰µå»ºä¸€å€‹æ–°çš„è‡¨æ™‚å¯¦ä¾‹ï¼‰
    # test_config = ConfigManager(config_path='config.yml') # ç¢ºä¿æ¸¬è©¦ä½¿ç”¨çš„æ˜¯æœ€æ–°çš„

    db_path = config.get("database.path", "default.db")
    print(f"è³‡æ–™åº«è·¯å¾‘: {db_path}")

    retries = config.get("data_acquisition.retries", 0)
    print(f"é‡è©¦æ¬¡æ•¸: {retries}")

    non_existent = config.get("non_existent.key", "é è¨­å€¼")
    print(f"ä¸å­˜åœ¨çš„éµ: {non_existent}")

    try:
        api_key = get_fred_api_key()
        # å®‰å…¨èµ·è¦‹ï¼Œä¸åœ¨æ—¥èªŒä¸­æ‰“å°é‡‘é‘°æœ¬èº«
        print(f"æˆåŠŸè®€å– FRED API Key (é•·åº¦: {len(api_key)})")
    except ValueError as e:
        print(e)

    # æ¸¬è©¦ç›´æ¥å¾ config å¯¦ä¾‹ç²å–é‡‘é‘°
    fred_key_direct = config.get("api_keys.fred")
    if fred_key_direct and fred_key_direct != "YOUR_FRED_API_KEY_HERE":
        print(f"ç›´æ¥å¾ config å¯¦ä¾‹ç²å– FRED API Key (é•·åº¦: {len(fred_key_direct)})")
    else:
        print("ç„¡æ³•ç›´æ¥å¾ config å¯¦ä¾‹ç²å–æœ‰æ•ˆçš„ FRED API Keyã€‚")

    print("--- æ¸¬è©¦çµæŸ ---")
# æª”æ¡ˆ: src/core/db/evolution_logger.py
from typing import Dict

DB_PATH = "prometheus_fire.duckdb"
TABLE_NAME = "evolution_logs"


def log_generation_stats(generation: int, stats: Dict):
    """
    å°‡å–®ä¸€ä»£çš„æ¼”åŒ–çµ±è¨ˆæ•¸æ“šå„²å­˜è‡³ DuckDBã€‚
    """
    # This function is now a no-op
    pass


def clear_evolution_logs():
    """å®‰å…¨åœ°æ¸…é™¤æ‰€æœ‰æ¼”åŒ–æ—¥èªŒã€‚"""
    # This function is now a no-op
    pass
import sqlite3
from pathlib import Path
from typing import Set


class SchemaRegistry:
    def __init__(self, db_path: str):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.conn = sqlite3.connect(self.db_path)
        self._create_table()

    def _create_table(self):
        with self.conn:
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS schema_registry (
                    format_fingerprint TEXT PRIMARY KEY,
                    header TEXT,
                    encoding TEXT,
                    file_count INTEGER DEFAULT 1,
                    first_seen_file TEXT
                )
            """)

    def add_or_update_schema(self, fingerprint: str, header: str, encoding: str, filename: str):
        with self.conn:
            cursor = self.conn.cursor()
            cursor.execute(
                "SELECT file_count FROM schema_registry WHERE format_fingerprint = ?",
                (fingerprint,),
            )
            existing = cursor.fetchone()

            if existing:
                new_count = existing[0] + 1
                cursor.execute(
                    "UPDATE schema_registry SET file_count = ? WHERE format_fingerprint = ?",
                    (new_count, fingerprint),
                )
                return "updated"
            else:
                cursor.execute(
                    "INSERT INTO schema_registry VALUES (?, ?, ?, 1, ?)",
                    (fingerprint, header, encoding, filename),
                )
                return "new"

    def get_known_fingerprints(self) -> Set[str]:
        with self.conn:
            cursor = self.conn.cursor()
            cursor.execute("SELECT format_fingerprint FROM schema_registry")
            return {row[0] for row in cursor.fetchall()}

    def get_all_schemas(self) -> dict:
        with self.conn:
            cursor = self.conn.cursor()
            cursor.execute("SELECT format_fingerprint, header, encoding FROM schema_registry")
            return {row[0]: (row[1], row[2]) for row in cursor.fetchall()}

    def close(self):
        self.conn.close()
import duckdb
import os
import pandas as pd
from prometheus.core.logging.log_manager import LogManager

class DBManager:
    def __init__(self, db_path: str = "data/analytics_warehouse/factors.duckdb"):
        self.db_path = db_path
        self.logger = LogManager.get_instance().get_logger(self.__class__.__name__)

        # ç¢ºä¿æ•¸æ“šåº«ç›®éŒ„å­˜åœ¨
        db_dir = os.path.dirname(self.db_path)
        if not os.path.exists(db_dir):
            os.makedirs(db_dir)

    def save_data(self, data: pd.DataFrame, table_name: str):
        """
        ä¸€å€‹é¡å‹æ„ŸçŸ¥çš„ç©©å¥å¯«å…¥å‡½æ•¸ï¼Œèƒ½å¤ è‡ªå‹•åµå¯Ÿã€æ¼”é€²ä¸¦å¯«å…¥æ•¸æ“šã€‚
        """
        if data.empty:
            self.logger.warning("æ•¸æ“šç‚ºç©ºï¼Œæ²’æœ‰å¯ä»¥å„²å­˜çš„å…§å®¹ã€‚")
            return

        try:
            with duckdb.connect(self.db_path) as con:
                db_columns = self._get_table_columns(con, table_name)

                if not db_columns:
                    self.logger.info(f"è¡¨æ ¼ '{table_name}' ä¸å­˜åœ¨ï¼Œå°‡æ ¹æ“š DataFrame çµæ§‹å‰µå»ºã€‚")

                    # --- [æ ¸å¿ƒæ”¹é€ ] ---
                    # ç‚ºäº†å¯¦ç¾ç©©å¥çš„ UPSERTï¼Œæˆ‘å€‘å¿…é ˆåœ¨å‰µå»ºæ™‚å°±å®šç¾©ä¸»éµã€‚
                    # æˆ‘å€‘å‡è¨­ 'date' å’Œ 'symbol' æ˜¯å¿…ç„¶å­˜åœ¨çš„æ ¸å¿ƒæ¬„ä½ã€‚
                    create_sql = f"""
                    CREATE TABLE {table_name} (
                        date TIMESTAMP,
                        symbol VARCHAR,
                        PRIMARY KEY (date, symbol)
                    );
                    """
                    con.execute(create_sql)
                    self.logger.info(f"æˆåŠŸå‰µå»ºè¡¨æ ¼ '{table_name}' ä¸¦å®šç¾©äº† (date, symbol) è¤‡åˆä¸»éµã€‚")

                    # è¨»å†Š DataFrame ä»¥ä¾¿å¾ŒçºŒæ’å…¥
                    con.register('df_to_insert', data)

                    # å‹•æ…‹æ·»åŠ å…¶é¤˜æ¬„ä½
                    initial_cols = {'date', 'symbol'}
                    remaining_cols = [col for col in data.columns if col not in initial_cols]

                    for col in remaining_cols:
                        col_dtype = data[col].dtype
                        sql_type = self._map_dtype_to_sql(col_dtype)
                        con.execute(f"ALTER TABLE {table_name} ADD COLUMN \"{col}\" {sql_type};")

                    # æ’å…¥å®Œæ•´æ•¸æ“š
                    all_cols = ['date', 'symbol'] + remaining_cols
                    col_names_str = ", ".join(f'"{c}"' for c in all_cols)
                    con.execute(f"INSERT INTO {table_name} ({col_names_str}) SELECT {col_names_str} FROM df_to_insert")

                    self.logger.info(f"æˆåŠŸå°‡ {len(data)} ç­†åˆå§‹æ•¸æ“šæ’å…¥åˆ°æ–°å‰µå»ºçš„ '{table_name}' è¡¨æ ¼ä¸­ã€‚")
                    # åœ¨å‰µå»ºå¾Œï¼Œé‡æ–°ç²å–æ¬„ä½è³‡è¨Šä»¥é€²è¡Œå¾ŒçºŒçš„åˆä½µæ“ä½œ
                    db_columns = self._get_table_columns(con, table_name)
                else:
                    df_columns = data.columns.tolist()
                    new_columns = set(df_columns) - set(db_columns)

                    if new_columns:
                        self.logger.info(f"åµæ¸¬åˆ°æ–°æ¬„ä½: {new_columns}ã€‚æ­£åœ¨æ¼”é€²è¡¨æ ¼çµæ§‹...")
                        for col in new_columns:
                            col_dtype = data[col].dtype
                            sql_type = self._map_dtype_to_sql(col_dtype)
                            con.execute(f"ALTER TABLE {table_name} ADD COLUMN {col} {sql_type};")
                        self.logger.info("è¡¨æ ¼çµæ§‹æ¼”é€²å®Œæˆã€‚")
                        db_columns = self._get_table_columns(con, table_name)

                    # --- [æ ¸å¿ƒæ”¹é€ ] ---
                    # ä½¿ç”¨ DuckDB çš„ ON CONFLICT (UPSERT) èªæ³•å¯¦ç¾é«˜æ•ˆã€åŸå­æ€§çš„æ•¸æ“šåˆä½µã€‚
                    con.register('df_to_upsert', data)

                    all_columns = [f'"{c}"' for c in data.columns]
                    update_columns = [col for col in all_columns if col.lower() not in ('"date"', '"symbol"')]

                    if not update_columns:
                        self.logger.warning("æ²’æœ‰éœ€è¦æ›´æ–°çš„æ¬„ä½ï¼ˆé™¤äº†ä¸»éµï¼‰ï¼Œå°‡åªåŸ·è¡Œæ’å…¥æ“ä½œã€‚")
                        # å¦‚æœåªæœ‰ä¸»éµï¼Œé‚£éº¼ ON CONFLICT å°±ä¸éœ€è¦ DO UPDATE
                        upsert_sql = f"""
                        INSERT INTO {table_name} ({', '.join(all_columns)})
                        SELECT {', '.join(all_columns)} FROM df_to_upsert
                        ON CONFLICT (date, symbol) DO NOTHING;
                        """
                    else:
                        set_clause = ", ".join([f'{col} = excluded.{col}' for col in update_columns])
                        upsert_sql = f"""
                        INSERT INTO {table_name} ({', '.join(all_columns)})
                        SELECT {', '.join(all_columns)} FROM df_to_upsert
                        ON CONFLICT (date, symbol) DO UPDATE SET
                            {set_clause};
                        """

                    con.execute(upsert_sql)
                    self.logger.info(f"æˆåŠŸå°‡ {len(data)} ç­†æ•¸æ“š UPSERT åˆ° '{table_name}'ã€‚")
        except Exception as e:
            self.logger.error(f"å„²å­˜æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            raise

    def _get_table_columns(self, con, table_name):
        """æŸ¥è©¢ä¸¦è¿”å›è³‡æ–™åº«è¡¨çš„æ¬„ä½åˆ—è¡¨ï¼ˆå…¨éƒ¨è½‰ç‚ºå°å¯«ï¼‰ã€‚"""
        try:
            table_info = con.execute(f"PRAGMA table_info('{table_name}')").fetchall()
            # å°‡æ‰€æœ‰åˆ—åè½‰ç‚ºå°å¯«ï¼Œä»¥å¯¦ç¾ä¸å€åˆ†å¤§å°å¯«çš„æ¯”è¼ƒ
            return [str(info[1]).lower() for info in table_info]
        except duckdb.CatalogException:
            return []

    def fetch_table(self, table_name: str) -> pd.DataFrame:
        """
        å¾æ•¸æ“šåº«ä¸­è®€å–æ•´å€‹è¡¨æ ¼ä¸¦è¿”å›ä¸€å€‹ Pandas DataFrameã€‚

        Args:
            table_name (str): è¦è®€å–çš„è¡¨æ ¼åç¨±ã€‚

        Returns:
            pd.DataFrame: åŒ…å«è¡¨æ ¼æ•¸æ“šçš„ DataFrameã€‚å¦‚æœè¡¨æ ¼ä¸å­˜åœ¨æˆ–ç‚ºç©ºï¼Œå‰‡è¿”å›ä¸€å€‹ç©ºçš„ DataFrameã€‚
        """
        try:
            with duckdb.connect(self.db_path, read_only=True) as con:
                # æª¢æŸ¥è¡¨æ ¼æ˜¯å¦å­˜åœ¨
                tables = con.execute("SHOW TABLES").fetchall()
                if (table_name,) not in tables:
                    self.logger.warning(f"è¡¨æ ¼ '{table_name}' åœ¨æ•¸æ“šåº«ä¸­ä¸å­˜åœ¨ã€‚")
                    return pd.DataFrame()

                df = con.table(table_name).to_df()
                self.logger.info(f"æˆåŠŸå¾ '{table_name}' è¡¨æ ¼ä¸­è®€å– {len(df)} ç­†æ•¸æ“šã€‚")
                return df
        except Exception as e:
            self.logger.error(f"è®€å–è¡¨æ ¼ '{table_name}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return pd.DataFrame() # åœ¨å‡ºéŒ¯æ™‚è¿”å›ä¸€å€‹ç©ºçš„ DataFrame

    def _map_dtype_to_sql(self, dtype):
        """å°‡ Pandas çš„ dtype è½‰æ›ç‚º SQL é¡å‹å­—ä¸²ã€‚"""
        if pd.api.types.is_integer_dtype(dtype):
            return 'BIGINT'
        elif pd.api.types.is_float_dtype(dtype):
            return 'DOUBLE'
        elif pd.api.types.is_datetime64_any_dtype(dtype):
            return 'TIMESTAMP'
        elif pd.api.types.is_string_dtype(dtype) or pd.api.types.is_object_dtype(dtype):
            return 'VARCHAR'
        else:
            return 'VARCHAR'
import duckdb

def get_db_connection(db_path):
    return duckdb.connect(database=db_path, read_only=False)
import json
import sqlite3
from pathlib import Path


class TransactionalWriter:
    """
    äº¤æ˜“å‹å¯«å…¥å™¨ï¼šå°ˆé–€è² è²¬å°‡å›æ¸¬çµæœå®‰å…¨åœ°å¯«å…¥ SQLite è³‡æ–™åº«ã€‚
    """

    def __init__(self, db_path: str | Path = "output/results.sqlite"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False, timeout=10)
        self._init_db()

    def _init_db(self):
        with self.conn:
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS backtest_results (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    symbol TEXT,
                    params TEXT,
                    crossover_points INTEGER,
                    last_price REAL,
                    batch_id TEXT,
                    timestamp TEXT
                )
            """)

    def save_result(self, result_data: dict):
        with self.conn:
            self.conn.execute(
                """
                INSERT INTO backtest_results
                (symbol, params, crossover_points, last_price, batch_id, timestamp)
                VALUES (?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
                """,
                [
                    result_data.get("symbol"),
                    json.dumps(result_data.get("params", {})),
                    result_data.get("crossover_points"),
                    result_data.get("last_price"),
                    result_data.get("batch_id"),
                ],
            )
from pathlib import Path

import duckdb
import pandas as pd


class DataWarehouse:
    def __init__(self, db_path: str):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.conn = duckdb.connect(str(self.db_path))

    def execute_query(self, query: str, params=None):
        return self.conn.execute(query, params)

    def get_results(self, query: str, params=None) -> pd.DataFrame:
        return self.conn.execute(query, params).fetchdf()

    def table_exists(self, table_name: str) -> bool:
        try:
            self.conn.execute(f"DESCRIBE {table_name}")
            return True
        except duckdb.CatalogException:
            return False

    def close(self):
        self.conn.close()


class RawDataWarehouse(DataWarehouse):
    def __init__(self, db_path: str):
        super().__init__(db_path)
        self._create_log_table()

    def _create_log_table(self):
        self.execute_query("""
            CREATE TABLE IF NOT EXISTS raw_import_log (
                file_path VARCHAR PRIMARY KEY,
                content_blob BLOB,
                format_fingerprint VARCHAR
            );
        """)

    def is_file_processed(self, file_path: str) -> bool:
        result = self.execute_query(
            "SELECT COUNT(*) FROM raw_import_log WHERE file_path = ?", (file_path,)
        ).fetchone()
        return result[0] > 0 if result else False

    def log_processed_file(self, file_path: str, content: bytes, fingerprint: str):
        self.execute_query(
            "INSERT INTO raw_import_log VALUES (?, ?, ?)",
            (file_path, content, fingerprint),
        )


class AnalyticsDataWarehouse(DataWarehouse):
    def create_daily_futures_table(self):
        self.execute_query("""
            CREATE TABLE IF NOT EXISTS daily_futures (
                "äº¤æ˜“æ—¥æœŸ" VARCHAR,
                "å¥‘ç´„ä»£ç¢¼" VARCHAR,
                "åˆ°æœŸæœˆä»½(é€±åˆ¥)" VARCHAR,
                "é–‹ç›¤åƒ¹" VARCHAR,
                "æœ€é«˜åƒ¹" VARCHAR,
                "æœ€ä½åƒ¹" VARCHAR,
                "æ”¶ç›¤åƒ¹" VARCHAR,
                "æˆäº¤é‡" VARCHAR
            );
        """)

    def insert_daily_futures(self, df: pd.DataFrame):
        self.conn.register('df_to_load', df)
        self.execute_query(
            "INSERT INTO daily_futures SELECT * FROM df_to_load"
        )
# -*- coding: utf-8 -*-
import joblib
import pandas as pd
from sklearn.linear_model import LinearRegression
from pathlib import Path

class FactorSimulator:
    """
    å› å­ä»£ç†æ¨¡æ“¬å™¨ï¼Œè² è²¬æ¨¡å‹çš„è¨“ç·´ã€ä¿å­˜èˆ‡é æ¸¬ã€‚
    """

    def __init__(self, model_dir: str = "data/models"):
        """
        åˆå§‹åŒ– FactorSimulatorã€‚

        :param model_dir: å­˜æ”¾æ¨¡å‹çš„ç›®éŒ„ã€‚
        """
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        self.model = None

    def train(self, target_series: pd.Series, predictors_df: pd.DataFrame):
        """
        è¨“ç·´æ¨¡å‹ä¸¦ä¿å­˜ã€‚

        :param target_series: ç›®æ¨™å› å­ (ä¾‹å¦‚ T10Y2Y çš„æ­·å²æ•¸æ“š)ã€‚
        :param predictors_df: é æ¸¬å› å­ (ä¾†è‡ª factors.duckdb)ã€‚
        """
        target_name = target_series.name
        model_path = self.model_dir / f"{target_name}_simulator.joblib"

        # è™•ç†æ•¸æ“šå°é½Š
        aligned_df = predictors_df.join(target_series, how='inner')

        # è™•ç†ç¼ºå¤±å€¼
        aligned_df = aligned_df.dropna()

        X = aligned_df[predictors_df.columns]
        y = aligned_df[target_name]

        self.model = LinearRegression()
        self.model.fit(X, y)

        joblib.dump(self.model, model_path)
        print(f"æ¨¡å‹å·²æˆåŠŸè¨“ç·´ä¸¦ä¿å­˜è‡³ï¼š{model_path}")

    def predict(self, predictors_df: pd.DataFrame, target_name: str) -> pd.Series:
        """
        è¼‰å…¥å·²ä¿å­˜çš„æ¨¡å‹ä¸¦é€²è¡Œé æ¸¬ã€‚

        :param predictors_df: ç•¶å‰çš„é æ¸¬å› å­æ•¸æ“šã€‚
        :param target_name: ç›®æ¨™å› å­çš„åç¨±ã€‚
        :return: æ¨¡æ“¬å‡ºçš„ç›®æ¨™å› å­å€¼ã€‚
        """
        model_path = self.model_dir / f"{target_name}_simulator.joblib"
        if not model_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆï¼š{model_path}")

        model = joblib.load(model_path)

        # ç¢ºä¿é æ¸¬æ•¸æ“šçš„æ¬„ä½é †åºèˆ‡è¨“ç·´æ™‚ä¸€è‡´
        predictors_df = predictors_df.reindex(columns=model.feature_names_in_, fill_value=0)

        return pd.Series(model.predict(predictors_df), index=predictors_df.index)
import pickle
from pathlib import Path
from typing import Optional

from prometheus.core.logging.log_manager import LogManager

logger = LogManager.get_instance().get_logger("CheckpointManager")


class CheckpointManager:
    """
    ä¸€å€‹è² è²¬å„²å­˜å’Œè®€å–æ¼”åŒ–éç¨‹ç‹€æ…‹çš„æª¢æŸ¥é»ç®¡ç†å™¨ã€‚
    """

    def __init__(self, checkpoint_path: Path):
        self.path = checkpoint_path

    def save_checkpoint(self, state: dict):
        """å°‡æ¼”åŒ–ç‹€æ…‹ä»¥ pickle æ ¼å¼å„²å­˜åˆ°æª”æ¡ˆã€‚"""
        try:
            self.path.parent.mkdir(exist_ok=True, parents=True)
            with open(self.path, "wb") as f:
                pickle.dump(state, f)
            logger.info(f"æ¼”åŒ–ç‹€æ…‹å·²æˆåŠŸå„²å­˜è‡³: {self.path}")
        except Exception as e:
            logger.error(f"å„²å­˜æª¢æŸ¥é»å¤±æ•—: {e}", exc_info=True)

    def load_checkpoint(self) -> Optional[dict]:
        """å¾æª”æ¡ˆè®€å–æ¼”åŒ–ç‹€æ…‹ã€‚"""
        if not self.path.exists():
            return None

        try:
            with open(self.path, "rb") as f:
                state = pickle.load(f)
            logger.info(f"æˆåŠŸå¾ {self.path} è®€å–åˆ°æª¢æŸ¥é»ã€‚")
            return state
        except Exception as e:
            logger.error(f"è®€å–æª¢æŸ¥é»å¤±æ•—: {e}", exc_info=True)
            return None

    def clear_checkpoint(self):
        """æ¸…é™¤èˆŠçš„æª¢æŸ¥é»æª”æ¡ˆã€‚"""
        if self.path.exists():
            self.path.unlink()
            logger.info(f"å·²æ¸…é™¤èˆŠçš„æª¢æŸ¥é»æª”æ¡ˆ: {self.path}")
# -*- coding: utf-8 -*-
"""
æ¼”åŒ–å®¤ï¼šä½¿ç”¨éºå‚³æ¼”ç®—æ³•ä¾†ç™¼ç¾é«˜æ•ˆçš„äº¤æ˜“ç­–ç•¥ã€‚
"""
import random
from typing import List, Tuple
import numpy as np

from deap import base, creator, tools

from prometheus.services.backtesting_service import BacktestingService
from prometheus.models.strategy_models import Strategy

class EvolutionChamber:
    """
    ä¸€å€‹ã€Œæ¼”åŒ–å®¤ã€ï¼Œå°‡å› å­åº«è½‰åŒ–ç‚ºåŸºå› æ± ï¼Œä¸¦ä½¿ç”¨éºå‚³æ¼”ç®—æ³•é€²è¡Œç­–ç•¥æ¼”åŒ–ã€‚
    """
    def __init__(self, backtesting_service: BacktestingService, available_factors: List[str], target_asset: str = 'SPY'):
        """
        åˆå§‹åŒ–æ¼”åŒ–å®¤ã€‚

        Args:
            backtesting_service (BacktestingService): ç”¨æ–¼è©•ä¼°ç­–ç•¥é©æ‡‰åº¦çš„å›æ¸¬æœå‹™ã€‚
            available_factors (List[str]): å¯ä¾›æ¼”åŒ–é¸æ“‡çš„æ‰€æœ‰å› å­åç¨±åˆ—è¡¨ã€‚
            target_asset (str): æ¼”åŒ–å’Œå›æ¸¬çš„ç›®æ¨™è³‡ç”¢ã€‚
        """
        self.backtester = backtesting_service
        self.available_factors = available_factors
        self.target_asset = target_asset
        self.num_factors_to_select = 5 # æš«å®šæ¯å€‹ç­–ç•¥ç”±5å€‹å› å­æ§‹æˆ

        # --- DEAP æ ¸å¿ƒè¨­å®š ---
        # ç¢ºä¿ FitnessMax å’Œ Individual åªè¢«å‰µå»ºä¸€æ¬¡ï¼Œé¿å…åœ¨å¤šå€‹å¯¦ä¾‹ä¸­é‡è¤‡å‰µå»ºå°è‡´éŒ¯èª¤
        if not hasattr(creator, "FitnessMax"):
            creator.create("FitnessMax", base.Fitness, weights=(1.0,))
        if not hasattr(creator, "Individual"):
            # æ¯å€‹ã€Œå€‹é«”ã€éƒ½æ˜¯ä¸€å€‹åˆ—è¡¨ï¼Œä»£è¡¨ä¸€å€‹ç­–ç•¥
            creator.create("Individual", list, fitness=creator.FitnessMax)

        self.toolbox = base.Toolbox()
        self._setup_toolbox()

    def _evaluate_strategy(self, individual: List[int]) -> Tuple[float]:
        """
        è©•ä¼°å–®ä¸€å€‹é«”çš„é©æ‡‰åº¦ï¼Œæ­¤ç‚ºæ¼”åŒ–æ ¸å¿ƒçš„ã€Œé©æ‡‰åº¦å‡½æ•¸ã€ã€‚
        """
        # 1. è§£ç¢¼åŸºå› ï¼šå°‡å› å­ç´¢å¼•è½‰æ›ç‚ºå› å­åç¨±
        raw_factors = [self.available_factors[i] for i in individual]
        # ã€ä¿®æ­£ã€‘ç¢ºä¿å› å­åˆ—è¡¨çš„å”¯ä¸€æ€§ï¼Œé˜²æ­¢å› äº¤å‰çªè®Šå°è‡´çš„é‡è¤‡
        selected_factors = list(dict.fromkeys(raw_factors))

        # å¦‚æœå»é‡å¾Œå› å­å°‘æ–¼1å€‹ï¼Œé€™æ˜¯ä¸€å€‹ç„¡æ•ˆç­–ç•¥
        if not selected_factors:
            return (0.0,)

        # 2. å»ºç«‹ç­–ç•¥ç‰©ä»¶ (æ­¤è™•ä½¿ç”¨ç­‰æ¬Šé‡ä½œç‚ºç¯„ä¾‹)
        strategy_to_test = Strategy(
            factors=selected_factors,
            weights={factor: 1.0 / len(selected_factors) for factor in selected_factors},
            target_asset=self.target_asset # ä½¿ç”¨æ¼”åŒ–å®¤æŒ‡å®šçš„ç›®æ¨™è³‡ç”¢
        )

        # 3. åŸ·è¡Œå›æ¸¬ä»¥ç²å¾—ç¸¾æ•ˆ
        report = self.backtester.run(strategy_to_test)

        # 4. è¿”å›é©æ‡‰åº¦åˆ†æ•¸ (ä»¥å…ƒçµ„å½¢å¼)
        return (report.sharpe_ratio,)

    def _setup_toolbox(self):
        """
        è¨­å®š DEAP çš„ toolboxï¼Œå®šç¾©åŸºå› ã€å€‹é«”ã€æ—ç¾¤çš„ç”Ÿæˆè¦å‰‡èˆ‡æ¼”åŒ–ç®—å­ã€‚
        """
        # å®šç¾©ã€ŒåŸºå› ã€ï¼šä¸€å€‹ä»£è¡¨å› å­ç´¢å¼•çš„æ•´æ•¸
        self.toolbox.register("factor_indices", random.sample, range(len(self.available_factors)), self.num_factors_to_select)

        # å®šç¾©ã€Œå€‹é«”ã€ï¼šç”±ä¸€çµ„ä¸é‡è¤‡çš„å› å­ç´¢å¼•æ§‹æˆ
        self.toolbox.register("individual", tools.initIterate, creator.Individual, self.toolbox.factor_indices)

        # å®šç¾©ã€Œæ—ç¾¤ã€ï¼šç”±å¤šå€‹ã€Œå€‹é«”ã€çµ„æˆçš„åˆ—è¡¨
        self.toolbox.register("population", tools.initRepeat, list, self.toolbox.individual)

        # --- æ–°å¢ï¼šè¨»å†Šæ ¸å¿ƒéºå‚³ç®—å­ ---
        self.toolbox.register("evaluate", self._evaluate_strategy)
        # äº¤å‰ç®—å­ï¼šå…©é»äº¤å‰
        self.toolbox.register("mate", tools.cxTwoPoint)
        # çªè®Šç®—å­ï¼šéš¨æ©Ÿäº¤æ›ç´¢å¼•ï¼Œindpb ç‚ºæ¯å€‹åŸºå› çš„çªè®Šæ©Ÿç‡
        self.toolbox.register("mutate", tools.mutShuffleIndexes, indpb=0.1)
        # é¸æ“‡ç®—å­ï¼šéŒ¦æ¨™è³½é¸æ“‡ï¼Œtournsize ç‚ºæ¯æ¬¡ç«¶è³½çš„å€‹é«”æ•¸
        self.toolbox.register("select", tools.selTournament, tournsize=3)

    def run_evolution(self, n_pop: int = 50, n_gen: int = 10, cxpb: float = 0.5, mutpb: float = 0.2):
        """
        åŸ·è¡Œå®Œæ•´çš„æ¼”åŒ–æµç¨‹ã€‚

        Args:
            n_pop (int): æ—ç¾¤å¤§å°ã€‚
            n_gen (int): æ¼”åŒ–ä¸–ä»£æ•¸ã€‚
            cxpb (float): äº¤å‰æ©Ÿç‡ã€‚
            mutpb (float): çªè®Šæ©Ÿç‡ã€‚

        Returns:
            tools.HallOfFame: åŒ…å«æ¼”åŒ–éç¨‹ä¸­ç™¼ç¾çš„æœ€å„ªå€‹é«”ã€‚
        """
        pop = self.toolbox.population(n=n_pop)
        hof = tools.HallOfFame(1) # åäººå ‚ï¼Œåªå„²å­˜æœ€å„ªçš„ä¸€å€‹å€‹é«”
        stats = tools.Statistics(lambda ind: ind.fitness.values)
        stats.register("avg", np.mean)
        stats.register("std", np.std)
        stats.register("min", np.min)
        stats.register("max", np.max)

        # 1. é¦–æ¬¡è©•ä¼°æ‰€æœ‰å€‹é«”
        fitnesses = map(self.toolbox.evaluate, pop)
        for ind, fit in zip(pop, fitnesses):
            ind.fitness.values = fit

        print(f"--- é–‹å§‹æ¼”åŒ–ï¼Œå…± {n_gen} ä»£ ---")

        for g in range(n_gen):
            # 2. é¸æ“‡ä¸‹ä¸€ä»£çš„å€‹é«”
            offspring = self.toolbox.select(pop, len(pop))
            offspring = list(map(self.toolbox.clone, offspring))

            # 3. åŸ·è¡Œäº¤å‰èˆ‡çªè®Š
            for child1, child2 in zip(offspring[::2], offspring[1::2]):
                if random.random() < cxpb:
                    self.toolbox.mate(child1, child2)
                    del child1.fitness.values
                    del child2.fitness.values

            for mutant in offspring:
                if random.random() < mutpb:
                    self.toolbox.mutate(mutant)
                    del mutant.fitness.values

            # 4. è©•ä¼°è¢«æ”¹è®Šçš„å€‹é«”
            invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
            fitnesses = map(self.toolbox.evaluate, invalid_ind)
            for ind, fit in zip(invalid_ind, fitnesses):
                ind.fitness.values = fit

            # 5. æ›´æ–°æ—ç¾¤èˆ‡åäººå ‚
            pop[:] = offspring
            hof.update(pop)

            record = stats.compile(pop)
            print(f"> ç¬¬ {g+1} ä»£: æœ€å„ªå¤æ™® = {record['max']:.4f}, å¹³å‡å¤æ™® = {record['avg']:.4f}")

        print("--- æ¼”åŒ–çµæŸ ---")
        return hof
# -*- coding: utf-8 -*-
"""
ç­–ç•¥å ±å‘Šç”Ÿæˆå™¨ã€‚
"""
import os
from typing import List
from deap import tools
from prometheus.models.strategy_models import PerformanceReport, Strategy

class StrategyReporter:
    """
    å°‡æ¼”åŒ–éç¨‹ä¸­ç™¼ç¾çš„æœ€å„ªç­–ç•¥ï¼Œç”Ÿæˆä¸€ä»½æ¸…æ™°çš„å ±å‘Šã€‚
    """
    def __init__(self, report_dir: str = 'reports'):
        self.report_dir = report_dir
        if not os.path.exists(self.report_dir):
            os.makedirs(self.report_dir)

    def generate_report(
        self,
        best_individual: tools.HallOfFame,
        performance_report: PerformanceReport,
        available_factors: List[str]
    ):
        """
        ç”Ÿæˆä¸¦å„²å­˜ç­–ç•¥å ±å‘Šã€‚

        Args:
            best_individual (tools.HallOfFame): åŒ…å«æœ€å„ªå€‹é«”çš„åäººå ‚ã€‚
            performance_report (PerformanceReport): æœ€å„ªå€‹é«”çš„å›æ¸¬ç¸¾æ•ˆå ±å‘Šã€‚
            available_factors (List[str]): æ‰€æœ‰å¯ç”¨å› å­çš„åˆ—è¡¨ã€‚
        """
        if not best_individual:
            print("WARN: åäººå ‚ç‚ºç©ºï¼Œç„¡æ³•ç”Ÿæˆå ±å‘Šã€‚")
            return

        best_strategy_indices = best_individual[0]
        best_strategy_factors = [available_factors[i] for i in best_strategy_indices]

        report_content = f"""# ã€æ™®ç¾…ç±³ä¿®æ–¯ä¹‹ç«ï¼šæœ€å„ªç­–ç•¥å ±å‘Šã€‘

æ¼”åŒ–å®Œæˆï¼Œé€™æ˜¯æœ¬æ¬¡ç™¼ç¾çš„æœ€ä½³ç­–ç•¥è©³ç´°è³‡è¨Šã€‚

## ğŸ“ˆ æ ¸å¿ƒç¸¾æ•ˆæŒ‡æ¨™

| æŒ‡æ¨™                        | æ•¸å€¼                  |
| --------------------------- | --------------------- |
| å¤æ™®æ¯”ç‡ (Sharpe Ratio)     | {performance_report.sharpe_ratio:.4f} |
| å¹´åŒ–å ±é…¬ç‡ (Annualized Return) | {performance_report.annualized_return:.2%}  |
| æœ€å¤§å›æ’¤ (Max Drawdown)     | {performance_report.max_drawdown:.2%}   |
| ç¸½äº¤æ˜“å¤©æ•¸ (Total Trades)   | {performance_report.total_trades}     |

## ğŸ§¬ ç­–ç•¥åŸºå› æ§‹æˆ

æ­¤ç­–ç•¥ç”±ä»¥ä¸‹ **{len(best_strategy_factors)}** å€‹å› å­ç­‰æ¬Šé‡æ§‹æˆï¼š

```
{', '.join(best_strategy_factors)}
```
"""
        report_path = os.path.join(self.report_dir, 'best_strategy_report.md')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)

        print(f"âœ… ç­–ç•¥å ±å‘Šå·²æˆåŠŸç”Ÿæˆæ–¼: {report_path}")
# -*- coding: utf-8 -*-
"""
å›æ¸¬æœå‹™ï¼šè² è²¬è©•ä¼°å–®ä¸€ç­–ç•¥çš„æ­·å²ç¸¾æ•ˆã€‚
"""
import pandas as pd
import numpy as np
from prometheus.core.db.db_manager import DBManager
from prometheus.models.strategy_models import Strategy, PerformanceReport

class BacktestingService:
    """
    ä¸€å€‹ç¨ç«‹ã€é«˜æ•ˆçš„å›æ¸¬æœå‹™ã€‚
    æ­¤æœå‹™æ˜¯æ•´å€‹æ¼”åŒ–ç³»çµ±çš„å¿ƒè‡Ÿï¼Œå°ˆè·è² è²¬ç²¾æº–è©•ä¼°ä»»ä½•å–®ä¸€ç­–ç•¥ï¼ˆåŸºå› çµ„ï¼‰çš„æ­·å²ç¸¾æ•ˆã€‚
    """
    def __init__(self, db_manager: DBManager):
        """
        åˆå§‹åŒ–å›æ¸¬æœå‹™ã€‚

        Args:
            db_manager (DBManager): ç”¨æ–¼å¾æ•¸æ“šå€‰å„²è®€å–å› å­èˆ‡åƒ¹æ ¼æ•¸æ“šçš„æ•¸æ“šåº«ç®¡ç†å™¨ã€‚
        """
        self.db_manager = db_manager

    def _load_data(self, strategy: Strategy) -> pd.DataFrame:
        """
        å¾æ•¸æ“šåº«åŠ è¼‰ä¸¦åˆä½µå› å­èˆ‡ç›®æ¨™è³‡ç”¢åƒ¹æ ¼æ•¸æ“šã€‚
        """
        # 1. åŠ è¼‰æ‰€æœ‰å› å­æ•¸æ“š
        all_factors_df = self.db_manager.fetch_table('factors')

        # 2. ç¯©é¸å‡ºç­–ç•¥æ‰€éœ€çš„å› å­
        required_factors = all_factors_df[['date', 'symbol'] + strategy.factors]

        # 3. åŠ è¼‰ç›®æ¨™è³‡ç”¢çš„åƒ¹æ ¼æ•¸æ“š (å‡è¨­åƒ¹æ ¼ä¹Ÿå­˜åœ¨ 'factors' è¡¨ä¸­ï¼Œä»¥ 'close' æ¬„ä½è¡¨ç¤º)
        #    åœ¨çœŸå¯¦å ´æ™¯ä¸­ï¼Œé€™å¯èƒ½æœƒå¾ä¸€å€‹å°ˆé–€çš„åƒ¹æ ¼è¡¨ä¸­ç²å–
        target_prices_df = all_factors_df[all_factors_df['symbol'] == strategy.target_asset][['date', 'close']]

        # 4. åˆä½µæ•¸æ“š
        merged_df = pd.merge(required_factors[required_factors['symbol'] == strategy.target_asset], target_prices_df, on='date')
        merged_df['date'] = pd.to_datetime(merged_df['date'])
        merged_df = merged_df.set_index('date').sort_index()

        return merged_df

    def run(self, strategy: Strategy) -> PerformanceReport:
        """
        åŸ·è¡Œä¸€æ¬¡å®Œæ•´çš„ç­–ç•¥å›æ¸¬ã€‚
        """
        # 1. æ•¸æ“šåŠ è¼‰
        data = self._load_data(strategy)
        if data.empty:
            print(f"WARN: æ‰¾ä¸åˆ°ç­–ç•¥ {strategy.target_asset} çš„æ•¸æ“šï¼Œè·³éå›æ¸¬ã€‚")
            return PerformanceReport()

        # 2. è¨Šè™Ÿç”Ÿæˆ (æ­£è¦åŒ– + åŠ æ¬Š)
        # å°å› å­é€²è¡Œ z-score æ­£è¦åŒ–
        for factor in strategy.factors:
            data[f'{factor}_norm'] = (data[factor] - data[factor].mean()) / data[factor].std()

        # è¨ˆç®—åŠ æ¬Šå¾Œçš„çµ„åˆè¨Šè™Ÿ
        data['signal'] = 0
        for factor in strategy.factors:
            data['signal'] += data[f'{factor}_norm'] * strategy.weights.get(factor, 0)

        # 3. æŠ•è³‡çµ„åˆæ¨¡æ“¬
        # è¨ˆç®—ç›®æ¨™è³‡ç”¢çš„æ—¥å ±é…¬ç‡
        data['asset_returns'] = data['close'].pct_change()

        # æ ¹æ“šè¨Šè™Ÿè¨ˆç®—ç­–ç•¥å ±é…¬ç‡ (å‡è¨­ T+1 ç”Ÿæ•ˆ)
        # è¨Šè™Ÿç‚ºæ­£ -> åšå¤š, è¨Šè™Ÿç‚ºè²  -> åšç©º
        data['strategy_returns'] = data['signal'].shift(1) * data['asset_returns']

        # 4. ç¸¾æ•ˆè¨ˆç®—
        # è™•ç†å¯èƒ½å‡ºç¾çš„ NaN æˆ– Inf
        data.replace([np.inf, -np.inf], np.nan, inplace=True)
        data.dropna(inplace=True)

        if data.empty:
            return PerformanceReport()

        # è¨ˆç®—ç´¯ç©å ±é…¬
        cumulative_returns = (1 + data['strategy_returns']).cumprod()

        # è¨ˆç®—å¹´åŒ–å ±é…¬
        days = (data.index[-1] - data.index[0]).days
        annualized_return = (cumulative_returns.iloc[-1]) ** (365.0 / days) - 1 if days > 0 else 0.0

        # è¨ˆç®—å¹´åŒ–å¤æ™®æ¯”ç‡ (å‡è¨­ç„¡é¢¨éšªåˆ©ç‡ç‚º 0)
        annualized_volatility = data['strategy_returns'].std() * np.sqrt(252)
        sharpe_ratio = (annualized_return / annualized_volatility) if annualized_volatility != 0 else 0.0

        # è¨ˆç®—æœ€å¤§å›æ’¤
        peak = cumulative_returns.expanding(min_periods=1).max()
        drawdown = (cumulative_returns - peak) / peak
        max_drawdown = drawdown.min()

        return PerformanceReport(
            sharpe_ratio=float(sharpe_ratio),
            annualized_return=float(annualized_return),
            max_drawdown=float(max_drawdown),
            total_trades=len(data) # ç°¡åŒ–ç‚ºäº¤æ˜“å¤©æ•¸
        )
import json
import duckdb
from prometheus.core.logging.log_manager import LogManager
from prometheus.core.db import get_db_connection
from prometheus.core.config import config

class OptimizerService:
    def __init__(self, db_path=None, table_name="optimized_strategies"):
        self.db_path = db_path or config.get("database.main_db_path")
        self.table_name = table_name
        self.log_manager = LogManager(session_name="optimizer_service")
        self._initialize_db()

    def _initialize_db(self):
        with get_db_connection(self.db_path) as conn:
            conn.execute(f"""
                CREATE TABLE IF NOT EXISTS {self.table_name} (
                    strategy_id VARCHAR PRIMARY KEY,
                    params VARCHAR,
                    fitness_score FLOAT,
                    crossover_points INTEGER
                )
            """)

    def get_best_strategy(self):
        with get_db_connection(self.db_path) as conn:
            try:
                best_result = conn.execute(
                    f"SELECT params FROM {self.table_name} ORDER BY crossover_points DESC LIMIT 1"
                ).fetchone()
                if best_result:
                    return json.loads(best_result[0])
            except duckdb.CatalogException:
                self.log_manager.log_error(f"Table {self.table_name} not found.")
            return None

    def save_strategy(self, strategy_id, params, fitness_score, crossover_points):
        with get_db_connection(self.db_path) as conn:
            conn.execute(
                f"INSERT OR REPLACE INTO {self.table_name} VALUES (?, ?, ?, ?)",
                (strategy_id, json.dumps(params), fitness_score, crossover_points)
            )
        self.log_manager.log_info(f"Strategy {strategy_id} saved successfully.")

if __name__ == "__main__":
    service = OptimizerService()
    # Example usage:
    # service.save_strategy("strategy_1", {"param1": 10}, 0.95, 5)
    # best_params = service.get_best_strategy()
    # print(f"Best strategy params: {best_params}")
import time

from prometheus.core.queue.sqlite_queue import SQLiteQueue

# ã€æ ¸å¿ƒã€‘å®šç¾©ä¸€å€‹æ‰€æœ‰å·¥ä½œè€…éƒ½èªå¯çš„ã€æ˜ç¢ºçš„é—œé–‰ä¿¡è™Ÿ
POISON_PILL = "STOP_WORKING"


def projector_loop(results_queue: SQLiteQueue):
    """
    ä¸€å€‹éµå®ˆé‹¼éµå¥‘ç´„çš„çµæœæŠ•å½±å™¨ï¼šæ°¸ä¸æ”¾æ£„ï¼Œç›´åˆ°æ”¶åˆ°æ¯’ä¸¸ã€‚
    """
    print("[Projector] çµæœæŠ•å½±å™¨å·²å•Ÿå‹•ï¼Œæ­£åœ¨ç­‰å¾…çµæœ...")

    while True:
        try:
            # ã€æ ¸å¿ƒæ”¹è®Šã€‘get() ç¾åœ¨æœƒä¸€ç›´é˜»å¡ï¼Œç›´åˆ°æ‹¿åˆ°çµæœæˆ–æ¯’ä¸¸
            result = results_queue.get(block=True)

            # ã€æ ¸å¿ƒæ”¹è®Šã€‘æª¢æŸ¥æ˜¯å¦æ”¶åˆ°äº†ä¸‹ç­æŒ‡ä»¤
            if result == POISON_PILL:
                print("[Projector] æ”¶åˆ°é—œé–‰ä¿¡è™Ÿï¼Œæ­£åœ¨å„ªé›…é€€å‡º...")
                break  # é€€å‡º while è¿´åœˆ

            # ç°¡å–®åœ°æ‰“å°çµæœï¼Œåœ¨çœŸå¯¦æ‡‰ç”¨ä¸­é€™è£¡æœƒå¯«å…¥æ•¸æ“šåº«
            print(f"[Projector] æ”¶åˆ°çµæœ: {result}")

        except Exception as e:
            # å³ä½¿å–®ä¸€çµæœå‡ºéŒ¯ï¼Œä¹Ÿçµ•ä¸é€€å‡ºï¼Œç¹¼çºŒç­‰å¾…ä¸‹ä¸€å€‹
            print(f"!!!!!! [Projector] è™•ç†çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e} !!!!!!")
            time.sleep(5)

    print("[Projector] å·²æˆåŠŸé—œé–‰ã€‚")
import time

from prometheus.core.queue.sqlite_queue import SQLiteQueue
from prometheus.services.backtesting_service import BacktestingService
from prometheus.core.logging.log_manager import LogManager

POISON_PILL = "STOP_WORKING"


def backtest_worker_loop(
    task_queue: SQLiteQueue, results_queue: SQLiteQueue, price_data, worker_id: int
):
    """
    ä¸€å€‹éµå®ˆé‹¼éµå¥‘ç´„çš„å›æ¸¬å·¥ä½œè€…ï¼šæ°¸ä¸æ”¾æ£„ï¼Œç›´åˆ°æ”¶åˆ°æ¯’ä¸¸ã€‚
    """
    logger = LogManager.get_instance().get_logger(f"Backtest-Worker-{worker_id}")
    logger.info("å›æ¸¬å·¥ä½œè€…å·²å•Ÿå‹•ï¼Œæ­£åœ¨ç­‰å¾…ä»»å‹™...")
    backtester = BacktestingService(price_data)

    while True:
        try:
            task = task_queue.get(block=True)

            if task == POISON_PILL:
                logger.info("æ”¶åˆ°é—œé–‰ä¿¡è™Ÿï¼Œæ­£åœ¨å„ªé›…é€€å‡º...")
                break

            if not isinstance(task, (list, tuple)) or len(task) != 2:
                logger.warning(f"æ”¶åˆ°ç„¡æ•ˆä»»å‹™æ ¼å¼ï¼Œå·²å¿½ç•¥: {task}")
                continue

            item_id, genome_task = task

            if not isinstance(genome_task, dict):
                logger.warning(f"æ”¶åˆ°ç„¡æ•ˆçš„ genome_task æ ¼å¼ï¼Œå·²å¿½ç•¥: {genome_task}")
                continue

            params = genome_task.get("params", {})
            logger.info(f"æ­£åœ¨å›æ¸¬ä»»å‹™ #{item_id}...")
            logger.debug(f"ä»»å‹™ #{item_id} çš„åŸºå› : {params}")

            try:
                # ã€è¬è±¡å¼•æ“ã€‘èª¿ç”¨æ–°çš„å›æ¸¬æ–¹æ³•
                report = backtester.run_backtest(genome=params)
            except Exception as e:
                logger.error(f"å›æ¸¬å‡½æ•¸å…§éƒ¨å‡ºéŒ¯: {e}", exc_info=True)
                report = {"error": str(e), "is_valid": False}

            result_payload = {
                "genome_id": genome_task.get("id"),
                "params": params,
                "report": report,
                "processed_by": worker_id,
            }
            results_queue.put(result_payload)
            logger.debug(f"ä»»å‹™ #{item_id} å›æ¸¬å®Œæˆã€‚")

        except Exception as e:
            logger.error(f"è™•ç†è¿´åœˆç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
            time.sleep(10)

    logger.info("å·²æˆåŠŸé—œé–‰ã€‚")
import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
from pathlib import Path

# --- æª”æ¡ˆè·¯å¾‘ ---
REPORTS_DIR = Path("data/reports")
ANALYSIS_REPORT_PATH = REPORTS_DIR / "analysis_report.md"
EQUITY_CURVE_PATH = REPORTS_DIR / "equity_curve.png"

app = FastAPI(
    title="æ™®ç¾…ç±³ä¿®æ–¯ä¹‹ç« - ç¥è«­å„€è¡¨æ¿ API",
    description="æä¾›ç”± AI åˆ†æå¸«ç”Ÿæˆçš„æœ€æ–°ç­–ç•¥æ´å¯Ÿå ±å‘Šã€‚",
    version="1.0.0",
)

@app.get("/api/v1/reports/latest",
         summary="ç²å–æœ€æ–°åˆ†æå ±å‘Š",
         response_description="åŒ…å« Markdown å ±å‘Šå…§å®¹çš„ JSON ç‰©ä»¶")
def get_latest_report():
    """
    è®€å–ä¸¦å›å‚³æœ€æ–°çš„ Markdown åˆ†æå ±å‘Šã€‚
    """
    if not ANALYSIS_REPORT_PATH.exists():
        raise HTTPException(status_code=404, detail="åˆ†æå ±å‘Šä¸å­˜åœ¨ã€‚è«‹å…ˆåŸ·è¡Œ 'analyze' æŒ‡ä»¤ã€‚")

    try:
        with open(ANALYSIS_REPORT_PATH, "r", encoding="utf-8") as f:
            content = f.read()
        return JSONResponse(content={"report_content": content})
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è®€å–å ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

@app.get("/api/v1/reports/equity_curve.png",
         summary="ç²å–æ¬Šç›Šæ›²ç·šåœ–",
         response_description="æ¬Šç›Šæ›²ç·šçš„ PNG åœ–ç‰‡æª”æ¡ˆ")
def get_equity_curve():
    """
    ç›´æ¥æä¾›æ¬Šç›Šæ›²ç·šçš„åœ–ç‰‡æª”æ¡ˆã€‚
    """
    if not EQUITY_CURVE_PATH.exists():
        raise HTTPException(status_code=404, detail="æ¬Šç›Šæ›²ç·šåœ–ä¸å­˜åœ¨ã€‚è«‹å…ˆåŸ·è¡Œ 'analyze' æŒ‡ä»¤ã€‚")

    return FileResponse(EQUITY_CURVE_PATH, media_type="image/png")

@app.get("/")
def serve_report_at_root():
    """
    ç›´æ¥åœ¨æ ¹ç›®éŒ„æä¾›æœ€æ–°çš„åˆ†æå ±å‘Š JSONã€‚
    """
    return get_latest_report()

def run_dashboard_service(ctx, host: str, port: int):
    """å•Ÿå‹• FastAPI å„€è¡¨æ¿æœå‹™ã€‚"""
    # ctx åƒæ•¸ä¿ç•™ä»¥ç¬¦åˆç¾æœ‰ cli çµæ§‹ï¼Œä½†åœ¨æ­¤è™•æœªä½¿ç”¨
    print(f"INFO:     æ­£åœ¨æ–¼ http://{host}:{port} å•Ÿå‹•ç¥è«­å„€è¡¨æ¿å¾Œç«¯...")
    uvicorn.run(app, host=host, port=port, log_level="info")
import json
import random
import uuid
from pathlib import Path

from deap import creator, tools

from prometheus.core.queue.sqlite_queue import SQLiteQueue
from prometheus.services.checkpoint_manager import CheckpointManager
from prometheus.services.evolution_chamber import EvolutionChamber
from prometheus.core.logging.log_manager import LogManager

# --- æ¼”åŒ–è¨­å®š ---
POPULATION_SIZE = 10
MAX_GENERATIONS = 5
CHECKPOINT_FREQ = 2

# --- æª”æ¡ˆè·¯å¾‘ ---
HALL_OF_FAME_PATH = Path("data/hall_of_fame.json")
CHECKPOINT_PATH = Path("data/checkpoints/evolution_state.pkl")

# åˆå§‹åŒ–æ—¥èªŒè¨˜éŒ„å™¨
logger = LogManager.get_instance().get_logger("Evolution-Engine")


def evolution_loop(
    task_queue: SQLiteQueue,
    results_queue: SQLiteQueue,
    resume: bool = False,
    clean: bool = False,
):
    """
    æ™ºæ…§æ¼”åŒ–å¼•æ“ v4ï¼šæ•´åˆäº†çµæ§‹åŒ–æ—¥èªŒèˆ‡è¬è±¡å¼•æ“ã€‚
    """
    logger.info("ç­–ç•¥æ¼”åŒ–å¼•æ“å·²å•Ÿå‹•...")
    chamber = EvolutionChamber()
    checkpoint_manager = CheckpointManager(CHECKPOINT_PATH)

    start_gen = 0
    population = None
    hall_of_fame = tools.HallOfFame(1)

    if clean:
        logger.info("--clean æ¨¡å¼ï¼šå°‡é€²è¡Œä¸€æ¬¡å…¨æ–°çš„æ¼”åŒ–ã€‚")
        checkpoint_manager.clear_checkpoint()

    if resume:
        state = checkpoint_manager.load_checkpoint()
        if state:
            population = state["population"]
            start_gen = state["generation"] + 1
            hall_of_fame = state["hall_of_fame"]
            random.setstate(state["random_state"])
            logger.info(f"å¾ç¬¬ {start_gen} ä»£æ¢å¾©æ¼”åŒ–ã€‚")

    if population is None:
        logger.info("æ­£åœ¨å‰µå»ºåˆå§‹æ—ç¾¤...")
        population = chamber.create_population(n=POPULATION_SIZE)

    # --- æ¼”åŒ–ä¸»è¿´åœˆ ---
    for gen in range(start_gen, MAX_GENERATIONS):
        logger.info(f"æ­£åœ¨è™•ç†ç¬¬ {gen} ä»£...")

        pending_tasks = {}
        for i, individual in enumerate(population):
            task_id = str(uuid.uuid4())
            genome_task = {"id": task_id, "params": individual}
            task_queue.put((task_id, genome_task))
            pending_tasks[task_id] = individual
            logger.debug(f"å·²ç™¼é€ä»»å‹™: {genome_task}")

        logger.info("ç­‰å¾…æ‰€æœ‰å›æ¸¬çµæœ...")
        evaluated_count = 0
        while evaluated_count < len(pending_tasks):
            result = results_queue.get(block=True, timeout=20)
            if result:
                if isinstance(result, tuple) and len(result) == 2:
                    _, result_payload = result
                else:
                    result_payload = result

                if not result_payload:
                    continue

                genome_id = result_payload.get("genome_id")
                if genome_id in pending_tasks:
                    individual = pending_tasks.pop(genome_id)
                    report = result_payload.get("report", {})
                    fitness = report.get("sharpe_ratio", -1.0) if report.get("is_valid") else -1.0
                    individual.fitness.values = (fitness,)
                    evaluated_count += 1
                    logger.debug(f"æ”¶åˆ°çµæœ: {genome_id}, é©æ‡‰åº¦: {fitness:.2f} ({evaluated_count}/{len(population)})")
            else:
                logger.warning("ç­‰å¾…çµæœè¶…æ™‚ï¼Œå¯èƒ½éƒ¨åˆ†ä»»å‹™å·²ä¸Ÿå¤±ã€‚")
                for task_id in pending_tasks:
                    pending_tasks[task_id].fitness.values = (-1.0,)
                    evaluated_count += 1
                break

        hall_of_fame.update(population)

        if len(hall_of_fame) > 0:
            best_ind = hall_of_fame[0]
            logger.info(f"ç¬¬ {gen} ä»£æœ€ä½³ç­–ç•¥: å¤æ™®æ¯”ç‡ = {best_ind.fitness.values[0]:.2f}, åŸºå›  = {best_ind}")

        if (gen + 1) % CHECKPOINT_FREQ == 0:
            current_state = {
                "population": population, "generation": gen,
                "hall_of_fame": hall_of_fame, "random_state": random.getstate(),
            }
            checkpoint_manager.save_checkpoint(current_state)

        if gen < MAX_GENERATIONS - 1:
            logger.info("æ­£åœ¨ç”¢ç”Ÿä¸‹ä¸€ä»£æ—ç¾¤...")
            offspring = chamber.select_offspring(population)
            new_population = chamber.apply_mating_and_mutation(offspring)
            if len(hall_of_fame) > 0:
                new_population[0] = hall_of_fame[0]
            population = new_population

    logger.info("æ¼”åŒ–å®Œæˆ")
    if len(hall_of_fame) > 0:
        best_overall = hall_of_fame[0]
        logger.info(f"æ­·å²æœ€ä½³ç­–ç•¥ (åäººå ‚): å¤æ™®æ¯”ç‡ = {best_overall.fitness.values[0]:.2f}, åŸºå›  = {best_overall}")

        try:
            HALL_OF_FAME_PATH.parent.mkdir(exist_ok=True, parents=True)
            with open(HALL_OF_FAME_PATH, "w") as f:
                fitness_data = {"sharpe_ratio": best_overall.fitness.values[0]}
                # å°‡ deap å€‹é«”è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„åˆ—è¡¨
                genome_list = list(best_overall)
                json.dump([{"params": genome_list, "fitness": fitness_data}], f, indent=4)
            logger.info(f"åäººå ‚å·²å„²å­˜è‡³: {HALL_OF_FAME_PATH}")
        except Exception as e:
            logger.error(f"å„²å­˜åäººå ‚å¤±æ•—: {e}", exc_info=True)

    logger.info("æ¼”åŒ–å¼•æ“å·²åœæ­¢ã€‚")
# -*- coding: utf-8 -*-
# ä½¿å¾— apps å¯ä»¥è¢«è¦–ç‚ºä¸€å€‹å¥—ä»¶
import time
from prometheus.core.logging.log_manager import LogManager
from prometheus.core.queue.sqlite_queue import SQLiteQueue
from prometheus.services.optimizer_service import OptimizerService

def optimizer_loop(task_queue: SQLiteQueue, results_queue: SQLiteQueue):
    log_manager = LogManager(session_name="optimizer_app")
    optimizer = OptimizerService()

    while True:
        log_manager.log_info("Optimizer loop running...")
        # This is a placeholder for the actual optimization logic.
        # In a real scenario, this would involve reading from the results_queue,
        # running some optimization algorithm, and putting new tasks into the task_queue.
        time.sleep(10)

if __name__ == '__main__':
    task_q = SQLiteQueue("task_queue.db")
    results_q = SQLiteQueue("results_queue.db")
    optimizer_loop(task_q, results_q)
# æª”æ¡ˆ: src/prometheus/entrypoints/ai_analyst_app.py
import json
from pathlib import Path
import pandas as pd
import vectorbt as vbt

# --- æª”æ¡ˆè·¯å¾‘ ---
DATA_DIR = Path("data")
REPORTS_DIR = DATA_DIR / "reports"
HALL_OF_FAME_PATH = DATA_DIR / "hall_of_fame.json"
OHLCV_DATA_PATH = DATA_DIR / "ohlcv_data.csv"
EQUITY_CURVE_PATH = REPORTS_DIR / "equity_curve.png"
ANALYSIS_REPORT_PATH = REPORTS_DIR / "analysis_report.md"

# --- æ¨¡æ“¬ Gemini å®¢æˆ¶ç«¯ ---
class MockGeminiClient:
    def generate_report(self, prompt: str) -> str:
        print("\n[AI-Analyst] æ­£åœ¨èª¿ç”¨æ¨¡æ“¬çš„ Gemini API...")
        # åœ¨çœŸå¯¦å ´æ™¯ä¸­ï¼Œé€™è£¡æœƒæ˜¯ API å‘¼å«
        # ç‚ºäº†æ¼”ç¤ºï¼Œæˆ‘å€‘è¿”å›ä¸€å€‹åŸºæ–¼æç¤ºçš„ç°¡å–®æ¨¡æ¿åŒ–å›æ‡‰
        return """
**AI æ´å¯Ÿ**

*   **è¡Œç‚ºæ¨¡å¼åˆ†æ**: æ ¹æ“šå›æ¸¬æ•¸æ“šï¼Œæ­¤ç­–ç•¥ä¼¼ä¹åœ¨å¸‚å ´å‘ˆç¾æº«å’Œä¸Šæ¼²è¶¨å‹¢æ™‚è¡¨ç¾æœ€ä½³ã€‚å®ƒåˆ©ç”¨çŸ­æœŸå‹•é‡é€²å ´ï¼Œä¸¦åœ¨ç›¸å°å¼·å¼±æŒ‡æ¨™ï¼ˆRSIï¼‰é¡¯ç¤ºè¶…è²·æ™‚é€€å‡ºï¼Œé€™æ˜¯ä¸€ç¨®å…¸å‹çš„ã€Œè¶¨å‹¢è·Ÿéš¨ã€èˆ‡ã€Œå‡å€¼å›æ­¸ã€çš„æ··åˆç­–ç•¥ã€‚

*   **æ½›åœ¨é¢¨éšª**: è©²ç­–ç•¥åœ¨æ©«ç›¤éœ‡ç›ªå¸‚å ´ä¸­å¯èƒ½æœƒå› ç‚ºé »ç¹çš„é€²å‡ºå ´è€Œç”¢ç”Ÿè¼ƒå¤šäº¤æ˜“æˆæœ¬ï¼Œå¾è€Œä¾µè•åˆ©æ½¤ã€‚æ­¤å¤–ï¼Œåœ¨å¸‚å ´å‡ºç¾åŠ‡çƒˆåè½‰æ™‚ï¼ŒåŸºæ–¼RSIçš„å‡ºå ´ä¿¡è™Ÿå¯èƒ½æ»¯å¾Œï¼Œå°è‡´è¼ƒå¤§çš„å›æ’¤ã€‚

*   **å»ºè­°**: å»ºè­°åœ¨å¯¦éš›éƒ¨ç½²å‰ï¼Œå°ç­–ç•¥çš„åƒæ•¸é€²è¡Œæ›´ç´°ç·»çš„å„ªåŒ–ï¼Œç‰¹åˆ¥æ˜¯é‡å°ä¸åŒçš„å¸‚å ´æ³¢å‹•ç‡é€²è¡Œèª¿æ•´ã€‚åŒæ™‚ï¼Œå¯ä»¥è€ƒæ…®åŠ å…¥æ­¢æè¨‚å–®ä¾†é™åˆ¶æœ€å¤§è™§æã€‚
"""

def get_best_strategy():
    """å¾åäººå ‚è®€å–æœ€ä½³ç­–ç•¥ã€‚"""
    if not HALL_OF_FAME_PATH.exists():
        print(f"[AI-Analyst] éŒ¯èª¤: åäººå ‚æª”æ¡ˆä¸å­˜åœ¨æ–¼ {HALL_OF_FAME_PATH}")
        return None
    with open(HALL_OF_FAME_PATH, "r") as f:
        return json.load(f)

def run_backtest(strategy_genome, price_data):
    """ä½¿ç”¨ vectorbt åŸ·è¡Œå›æ¸¬ã€‚"""
    # é€™è£¡æˆ‘å€‘éœ€è¦ä¸€å€‹ç­–ç•¥å‡½æ•¸ä¾†è§£é‡‹åŸºå› é«”
    # ç‚ºäº†ç°¡åŒ–ï¼Œæˆ‘å€‘å‡è¨­åŸºå› é«”ç›´æ¥å°æ‡‰åˆ°æŸå€‹æŒ‡æ¨™çš„åƒæ•¸
    # ä¾‹å¦‚ï¼šåŸºå› é«”çš„å‰å…©å€‹å€¼æ˜¯å¿«æ…¢å‡ç·šçš„çª—å£
    fast_ma_window = int(strategy_genome[0] * 10) + 5  # ç¤ºä¾‹ï¼šçª—å£ç¯„åœ 5-15
    slow_ma_window = int(strategy_genome[1] * 30) + 20 # ç¤ºä¾‹ï¼šçª—å£ç¯„åœ 20-50

    fast_ma = vbt.MA.run(price_data, fast_ma_window)
    slow_ma = vbt.MA.run(price_data, slow_ma_window)

    entries = fast_ma.ma_crossed_above(slow_ma)
    exits = fast_ma.ma_crossed_below(slow_ma)

    portfolio = vbt.Portfolio.from_signals(price_data, entries, exits, init_cash=100000)
    return portfolio

def generate_markdown_report(stats, equity_curve_path):
    """ç”Ÿæˆ Markdown æ ¼å¼çš„åˆ†æå ±å‘Šã€‚"""

    # å¾ stats Series ä¸­æå–æŒ‡æ¨™
    total_return = stats['Total Return [%]']
    max_drawdown = stats['Max Drawdown [%]']
    sharpe_ratio = stats['Sharpe Ratio']
    calmar_ratio = stats['Calmar Ratio']

    # æ¨¡æ“¬ Gemini å®¢æˆ¶ç«¯ç”Ÿæˆ AI æ´å¯Ÿ
    client = MockGeminiClient()
    # ç‚ºäº†ç°¡åŒ–ï¼Œæˆ‘å€‘å‚³éä¸€å€‹å›ºå®šçš„ prompt
    ai_insights = client.generate_report("è«‹åˆ†ææ­¤ç­–ç•¥")

    report_content = f"""
# AI é¦–å¸­åˆ†æå¸«å ±å‘Š

## ç­–ç•¥æ¦‚è¿°

æœ¬å ±å‘Šæ—¨åœ¨åˆ†æå¾æ¼”åŒ–è¨ˆç®—ä¸­è„«ç©è€Œå‡ºçš„æœ€ä½³äº¤æ˜“ç­–ç•¥ã€‚æˆ‘å€‘å°‡é€éå›æ¸¬ä¾†è©•ä¼°å…¶æ­·å²è¡¨ç¾ï¼Œä¸¦æä¾›ç”± AI ç”Ÿæˆçš„æ´å¯Ÿã€‚

## æ ¸å¿ƒç¸¾æ•ˆæŒ‡æ¨™

| æŒ‡æ¨™           | æ•¸å€¼                  |
| -------------- | --------------------- |
| ç¸½å›å ± (%)     | {total_return:.2f}    |
| æœ€å¤§å›æ’¤ (%)   | {max_drawdown:.2f}    |
| å¤æ™®æ¯”ç‡       | {sharpe_ratio:.2f}    |
| å¡ç‘ªæ¯”ç‡       | {calmar_ratio:.2f}    |

## æ¬Šç›Šæ›²ç·š

![æ¬Šç›Šæ›²ç·š]({equity_curve_path.name})

{ai_insights}
"""
    return report_content.strip()


def ai_analyst_job():
    """AI åˆ†æå¸«çš„ä¸»å·¥ä½œæµç¨‹ã€‚"""
    print("[AI-Analyst] AI é¦–å¸­åˆ†æå¸«å·²å•Ÿå‹•ã€‚")

    # 1. ç¢ºä¿å ±å‘Šç›®éŒ„å­˜åœ¨
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)

    # 2. è®€å–æœ€ä½³ç­–ç•¥å’Œåƒ¹æ ¼æ•¸æ“š
    strategy_data = get_best_strategy()
    if not strategy_data:
        return

    if not OHLCV_DATA_PATH.exists():
        print(f"[AI-Analyst] éŒ¯èª¤: åƒ¹æ ¼æ•¸æ“šæª”æ¡ˆä¸å­˜åœ¨æ–¼ {OHLCV_DATA_PATH}")
        return

    price_data = pd.read_csv(OHLCV_DATA_PATH, index_col='Date', parse_dates=True)['Close']

    # 3. åŸ·è¡Œå›æ¸¬
    print("[AI-Analyst] æ­£åœ¨å°æœ€ä½³ç­–ç•¥é€²è¡Œå›æ¸¬...")
    portfolio = run_backtest(strategy_data["genome"], price_data)
    stats = portfolio.stats()

    # 4. ç”Ÿæˆä¸¦å„²å­˜æ¬Šç›Šæ›²ç·šåœ–
    print(f"[AI-Analyst] æ­£åœ¨ç”Ÿæˆæ¬Šç›Šæ›²ç·šåœ–ä¸¦å„²å­˜è‡³ {EQUITY_CURVE_PATH}...")
    fig = portfolio.plot()
    fig.write_image(EQUITY_CURVE_PATH)

    # 5. ç”Ÿæˆä¸¦å„²å­˜ Markdown å ±å‘Š
    print(f"[AI-Analyst] æ­£åœ¨ç”Ÿæˆ Markdown åˆ†æå ±å‘Š...")
    report_content = generate_markdown_report(stats, EQUITY_CURVE_PATH)

    try:
        with open(ANALYSIS_REPORT_PATH, "w", encoding="utf-8") as f:
            f.write(report_content)
        print("\n" + "="*20 + " ä»»å‹™å®Œæˆ " + "="*20)
        print(f"ğŸ‰ åˆ†æå ±å‘Šå·²æˆåŠŸç”Ÿæˆï¼è«‹æŸ¥çœ‹: {ANALYSIS_REPORT_PATH}")
        print(f"ğŸ“ˆ æ¬Šç›Šæ›²ç·šåœ–å·²å„²å­˜ï¼è«‹æŸ¥çœ‹: {EQUITY_CURVE_PATH}")
        print("="*52)
    except Exception as e:
        print(f"!!!!!! [AI-Analyst] å„²å­˜å ±å‘Šå¤±æ•—: {e} !!!!!!")

if __name__ == "__main__":
    ai_analyst_job()
import json
from pathlib import Path

from prometheus.core.queue.sqlite_queue import SQLiteQueue
from prometheus.core.logging.log_manager import LogManager

HALL_OF_FAME_PATH = Path("data/hall_of_fame.json")
VALIDATION_REPORT_PATH = Path("data/reports/validation_report.json")
logger = LogManager.get_instance().get_logger("Validator")


def validation_loop(task_queue: SQLiteQueue, results_queue: SQLiteQueue):
    """
    é©—è­‰è€…çš„ä¸»è¿´åœˆã€‚å®ƒåªåŸ·è¡Œä¸€æ¬¡ï¼Œç”¨æ–¼é©—è­‰åäººå ‚ä¸­çš„æœ€ä½³ç­–ç•¥ã€‚
    """
    logger.info("é©—è­‰è€…å·²å•Ÿå‹•ã€‚")

    if not HALL_OF_FAME_PATH.exists():
        logger.error(f"æ‰¾ä¸åˆ°åäººå ‚æª”æ¡ˆ {HALL_OF_FAME_PATH}ã€‚ç„¡æ³•é€²è¡Œé©—è­‰ã€‚")
        return

    try:
        with open(HALL_OF_FAME_PATH, "r") as f:
            # ã€è¬è±¡å¼•æ“ã€‘åäººå ‚ç¾åœ¨æ˜¯ä¸€å€‹åˆ—è¡¨
            best_strategies = json.load(f)
            best_strategy = best_strategies[0]

        in_sample_params = best_strategy["params"]
        in_sample_fitness = best_strategy.get("fitness", {})
        in_sample_sharpe = in_sample_fitness.get("sharpe_ratio", "N/A")

        sharpe_to_print = (
            f"{in_sample_sharpe:.2f}"
            if isinstance(in_sample_sharpe, (int, float))
            else in_sample_sharpe
        )
        logger.info(f"å·²å¾åäººå ‚è®€å–åˆ°æœ€ä½³ç­–ç•¥ (æ¨£æœ¬å…§å¤æ™®: {sharpe_to_print})")

        task_id = "out_of_sample_validation"
        validation_task = {"id": task_id, "params": in_sample_params}
        task_queue.put((task_id, validation_task))
        logger.info(f"å·²ç™¼é€æ¨£æœ¬å¤–å›æ¸¬ä»»å‹™: {in_sample_params}")

        logger.info("ç­‰å¾…æ¨£æœ¬å¤–å›æ¸¬çµæœ...")
        result = results_queue.get(block=True, timeout=60)
        _, result_payload = result

        if result_payload:
            out_of_sample_report = result_payload.get("report", {})

            report_str = "\n" + "=" * 20 + " æœ€çµ‚é©—è­‰å ±å‘Š " + "=" * 20 + "\n"
            report_str += f"ç­–ç•¥åƒæ•¸: {in_sample_params}\n"
            report_str += "-" * 55 + "\n"
            report_str += "æ¨£æœ¬å…§è¡¨ç¾ (å­¸ç¿’å€):\n"
            report_str += f"  - å¤æ™®æ¯”ç‡: {in_sample_sharpe:.2f}\n"
            report_str += "æ¨£æœ¬å¤–è¡¨ç¾ (æœªçŸ¥å€):\n"
            report_str += f"  - å¤æ™®æ¯”ç‡: {out_of_sample_report.get('sharpe_ratio', 'N/A')}\n"
            report_str += f"  - ç¸½å ±é…¬ç‡: {out_of_sample_report.get('total_return', 'N/A')}%\n"
            report_str += f"  - æœ€å¤§å›æ’¤: {out_of_sample_report.get('max_drawdown', 'N/A')}%\n"
            report_str += "=" * 55
            logger.info(report_str)

            if out_of_sample_report.get("is_valid") and out_of_sample_report.get("sharpe_ratio", -99) > 0.5:
                logger.info("çµè«–ï¼š[é€šé] ç­–ç•¥åœ¨æ¨£æœ¬å¤–è¡¨ç¾ç©©å¥ï¼Œå…·å‚™ä¸€å®šçš„æ³›åŒ–èƒ½åŠ›ã€‚")
            else:
                logger.warning("çµè«–ï¼š[è­¦å‘Š] ç­–ç•¥åœ¨æ¨£æœ¬å¤–è¡¨ç¾ä¸ä½³ï¼Œå¯èƒ½å­˜åœ¨éæ“¬åˆé¢¨éšªã€‚")

            VALIDATION_REPORT_PATH.parent.mkdir(parents=True, exist_ok=True)
            with open(VALIDATION_REPORT_PATH, "w", encoding="utf-8") as f:
                json.dump(result_payload, f, indent=4)
            logger.info(f"é©—è­‰çµæœå·²å„²å­˜è‡³ {VALIDATION_REPORT_PATH}")

    except Exception as e:
        logger.error(f"é©—è­‰éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)

    logger.info("é©—è­‰å®Œæˆï¼Œå³å°‡é—œé–‰ã€‚")
# src/prometheus/pipelines/p5_crypto_factor_generation.py

import logging
import asyncio
from typing import List

from src.prometheus.core.config import config
from src.prometheus.core.clients.client_factory import ClientFactory
from src.prometheus.core.db.db_manager import DBManager
from src.prometheus.core.engines.crypto_factor_engine import CryptoFactorEngine
from src.prometheus.core.pipelines.pipeline import Pipeline
from src.prometheus.core.pipelines.steps.loaders import LoadCryptoDataStep
from src.prometheus.core.pipelines.steps.savers import SaveToWarehouseStep
from src.prometheus.core.pipelines.steps.financial_steps import RunCryptoFactorEngineStep
from src.prometheus.core.pipelines.steps.splitters import GroupBySymbolStep

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def create_crypto_factor_pipeline(symbols: List[str], db_manager: DBManager, client_factory: ClientFactory) -> Pipeline:
    """
    å‰µå»ºä¸€å€‹ç”¨æ–¼ç”ŸæˆåŠ å¯†è²¨å¹£å› å­çš„æ¨™æº–åŒ– Pipelineã€‚

    :param symbols: è¦è™•ç†çš„åŠ å¯†è²¨å¹£ä»£è™Ÿåˆ—è¡¨ã€‚
    :param db_manager: è³‡æ–™åº«ç®¡ç†å™¨ã€‚
    :param client_factory: å®¢æˆ¶ç«¯å·¥å» ã€‚
    :return: ä¸€å€‹é…ç½®å¥½çš„ Pipeline å¯¦ä¾‹ã€‚
    """
    # åˆå§‹åŒ–åŠ å¯†è²¨å¹£å› å­å¼•æ“
    crypto_factor_engine = CryptoFactorEngine(client_factory=client_factory)

    from src.prometheus.core.pipelines.steps.normalize_columns_step import NormalizeColumnsStep
    # å®šç¾© Pipeline çš„æ­¥é©Ÿ
    steps = [
        LoadCryptoDataStep(symbols=symbols, client_factory=client_factory),
        NormalizeColumnsStep(),
        GroupBySymbolStep(),
        RunCryptoFactorEngineStep(engine=crypto_factor_engine),
        SaveToWarehouseStep(db_manager=db_manager, table_name="factors"),
    ]

    return Pipeline(steps=steps)


def main():
    """
    ä¸»åŸ·è¡Œå‡½æ•¸ï¼Œè¨­ç½®ä¸¦é‹è¡ŒåŠ å¯†è²¨å¹£å› å­ç”Ÿæˆæµç¨‹ã€‚
    """
    logger.info("===== é–‹å§‹åŸ·è¡Œç¬¬äº”è™Ÿç”Ÿç”¢ç·šï¼šåŠ å¯†è²¨å¹£å› å­ç”Ÿæˆ =====")

    # --- é…ç½®å€ ---
    # å®šç¾©ç›®æ¨™åŠ å¯†è²¨å¹£æ¸…å–®
    target_symbols = ['BTC-USD', 'ETH-USD']

    # åˆå§‹åŒ–è³‡æ–™åº«ç®¡ç†å™¨
    db_manager = DBManager(db_path=config.get('database.main_db_path'))

    # åˆå§‹åŒ–å®¢æˆ¶ç«¯å·¥å» 
    client_factory = ClientFactory()

    # --- åŸ·è¡Œå€ ---
    logger.info(f"ç›®æ¨™åŠ å¯†è²¨å¹£: {target_symbols}")

    # å‰µå»º Pipeline
    pipeline = create_crypto_factor_pipeline(
        symbols=target_symbols,
        db_manager=db_manager,
        client_factory=client_factory,
    )

    # åŸ·è¡Œ Pipeline
    try:
        asyncio.run(pipeline.run())
        logger.info("Pipeline åŸ·è¡ŒæˆåŠŸã€‚")
    except Exception as e:
        logger.error(f"Pipeline åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        # å¯ä»¥åœ¨æ­¤è™•æ·»åŠ éŒ¯èª¤è™•ç†é‚è¼¯ï¼Œä¾‹å¦‚ç™¼é€é€šçŸ¥

    # --- æ¸…ç†å€ ---
    logger.info("===== ç¬¬äº”è™Ÿç”Ÿç”¢ç·šåŸ·è¡Œå®Œç•¢ =====")


if __name__ == "__main__":
    main()
# -*- coding: utf-8 -*-
from prometheus.core.pipelines.base_step import BaseStep

class LoadHistoricalTargetStep(BaseStep):
    """
    è¼‰å…¥ç›®æ¨™å› å­çš„æ­·å²æ•¸æ“šã€‚
    """

    def __init__(self, target_factor: str):
        """
        åˆå§‹åŒ– LoadHistoricalTargetStepã€‚

        :param target_factor: è¦è¼‰å…¥çš„ç›®æ¨™å› å­åç¨±ã€‚
        """
        super().__init__()
        self.target_factor = target_factor

    async def run(self, data, context):
        """
        åŸ·è¡Œè¼‰å…¥æ­¥é©Ÿã€‚

        :param data: ä¸Šä¸€å€‹æ­¥é©Ÿçš„æ•¸æ“šã€‚
        :param context: ç®¡ç·šä¸Šä¸‹æ–‡ã€‚
        """
        all_factors = context.get('all_factors')
        target_factor_lower = self.target_factor.lower()

        if target_factor_lower not in all_factors.columns:
            raise KeyError(f"ç›®æ¨™å› å­ '{target_factor_lower}' åœ¨ all_factors ä¸­æ‰¾ä¸åˆ°ã€‚å¯ç”¨çš„å› å­æœ‰: {all_factors.columns.tolist()}")

        target_series = all_factors[[target_factor_lower]].dropna()
        context['target_series'] = target_series[target_factor_lower]
        return data
# -*- coding: utf-8 -*-
from prometheus.core.pipelines.base_step import BaseStep
from prometheus.services.factor_simulator import FactorSimulator

class TrainFactorSimulatorStep(BaseStep):
    """
    è¨“ç·´å› å­æ¨¡æ“¬å™¨æ¨¡å‹ã€‚
    """

    def __init__(self, target_factor: str):
        """
        åˆå§‹åŒ– TrainFactorSimulatorStepã€‚

        :param target_factor: è¦æ¨¡æ“¬çš„ç›®æ¨™å› å­åç¨±ã€‚
        """
        super().__init__()
        self.target_factor = target_factor
        self.simulator = FactorSimulator()

    async def run(self, data, context):
        """
        åŸ·è¡Œè¨“ç·´æ­¥é©Ÿã€‚

        :param data: ä¸Šä¸€å€‹æ­¥é©Ÿçš„æ•¸æ“šã€‚
        :param context: ç®¡ç·šä¸Šä¸‹æ–‡ã€‚
        """
        all_factors = context.get('all_factors')
        target_series = context.get('target_series')

        if target_series.empty:
            print(f"è­¦å‘Šï¼šç›®æ¨™å› å­ '{self.target_factor}' çš„æ•¸æ“šç‚ºç©ºï¼Œè·³éæ¨¡å‹è¨“ç·´ã€‚")
            return data

        # æ’é™¤ç›®æ¨™å› å­æœ¬èº«ä½œç‚ºé æ¸¬è®Šæ•¸
        predictors_df = all_factors.drop(columns=[self.target_factor.lower()])

        self.simulator.train(target_series, predictors_df)
        return data
# -*- coding: utf-8 -*-
import pandas as pd
import duckdb
from prometheus.core.pipelines.base_step import BaseStep
from prometheus.core.config import config

class LoadAllFactorsStep(BaseStep):
    """
    å¾ factors.duckdb è¼‰å…¥æ‰€æœ‰å¯ç”¨çš„å› å­ã€‚
    """

    async def run(self, data, context):
        """
        åŸ·è¡Œè¼‰å…¥æ­¥é©Ÿã€‚

        :param data: ä¸Šä¸€å€‹æ­¥é©Ÿçš„æ•¸æ“šã€‚
        :param context: ç®¡ç·šä¸Šä¸‹æ–‡ã€‚
        """
        db_path = config.get('database.main_db_path')
        with duckdb.connect(db_path) as con:
            all_factors = con.execute("SELECT * FROM factors").fetchdf()
            all_factors['date'] = pd.to_datetime(all_factors['date'])
            all_factors = all_factors.set_index('date')
            context['all_factors'] = all_factors
        return data
# src/prometheus/pipelines/p4_stock_factor_generation.py

import logging
import pandas as pd
from typing import List

from src.prometheus.core.config import config
from src.prometheus.core.clients.client_factory import ClientFactory
from src.prometheus.core.db.db_manager import DBManager
from src.prometheus.core.engines.stock_factor_engine import StockFactorEngine
from src.prometheus.core.pipelines.pipeline import Pipeline
from src.prometheus.core.pipelines.steps.loaders import LoadStockDataStep
from src.prometheus.core.pipelines.steps.savers import SaveToWarehouseStep
from src.prometheus.core.pipelines.steps.financial_steps import RunStockFactorEngineStep
from src.prometheus.core.pipelines.steps.splitters import GroupBySymbolStep

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def create_stock_factor_pipeline(symbols: List[str], db_manager: DBManager, client_factory: ClientFactory) -> Pipeline:
    """
    å‰µå»ºä¸€å€‹ç”¨æ–¼ç”Ÿæˆè‚¡ç¥¨å› å­çš„æ¨™æº–åŒ– Pipelineã€‚

    :param symbols: è¦è™•ç†çš„è‚¡ç¥¨ä»£è™Ÿåˆ—è¡¨ã€‚
    :param db_manager: è³‡æ–™åº«ç®¡ç†å™¨ã€‚
    :param client_factory: å®¢æˆ¶ç«¯å·¥å» ã€‚
    :return: ä¸€å€‹é…ç½®å¥½çš„ Pipeline å¯¦ä¾‹ã€‚
    """
    # åˆå§‹åŒ–è‚¡ç¥¨å› å­å¼•æ“
    stock_factor_engine = StockFactorEngine(client_factory)

    from src.prometheus.core.pipelines.steps.normalize_columns_step import NormalizeColumnsStep
    # å®šç¾© Pipeline çš„æ­¥é©Ÿ
    steps = [
        LoadStockDataStep(symbols=symbols, client_factory=client_factory),
        NormalizeColumnsStep(),
        GroupBySymbolStep(),
        RunStockFactorEngineStep(engine=stock_factor_engine),
        SaveToWarehouseStep(db_manager=db_manager, table_name="factors"),
    ]

    return Pipeline(steps=steps)


import asyncio

def main():
    """
    ä¸»åŸ·è¡Œå‡½æ•¸ï¼Œè¨­ç½®ä¸¦é‹è¡Œè‚¡ç¥¨å› å­ç”Ÿæˆæµç¨‹ã€‚
    """
    logger.info("===== é–‹å§‹åŸ·è¡Œç¬¬å››è™Ÿç”Ÿç”¢ç·šï¼šè‚¡ç¥¨å› å­ç”Ÿæˆ =====")

    # --- é…ç½®å€ ---
    # å®šç¾©ç›®æ¨™è‚¡ç¥¨æ¸…å–®
    # 'AAPL' - ç¾è‚¡, '2330.TW' - å°è‚¡
    target_symbols = ['AAPL', '2330.TW']

    # åˆå§‹åŒ–è³‡æ–™åº«ç®¡ç†å™¨
    db_manager = DBManager(db_path=config.get('database.main_db_path'))

    # åˆå§‹åŒ–å®¢æˆ¶ç«¯å·¥å» 
    client_factory = ClientFactory()

    # --- åŸ·è¡Œå€ ---
    logger.info(f"ç›®æ¨™è‚¡ç¥¨: {target_symbols}")

    # å‰µå»º Pipeline
    pipeline = create_stock_factor_pipeline(
        symbols=target_symbols,
        db_manager=db_manager,
        client_factory=client_factory,
    )

    # åŸ·è¡Œ Pipeline
    try:
        asyncio.run(pipeline.run())
        logger.info("Pipeline åŸ·è¡ŒæˆåŠŸã€‚")
    except Exception as e:
        logger.error(f"Pipeline åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        # å¯ä»¥åœ¨æ­¤è™•æ·»åŠ éŒ¯èª¤è™•ç†é‚è¼¯ï¼Œä¾‹å¦‚ç™¼é€é€šçŸ¥

    # --- æ¸…ç†å€ ---
    logger.info("===== ç¬¬å››è™Ÿç”Ÿç”¢ç·šåŸ·è¡Œå®Œç•¢ =====")


if __name__ == "__main__":
    main()
# -*- coding: utf-8 -*-
from prometheus.core.pipelines.pipeline import Pipeline
from prometheus.pipelines.steps.load_all_factors_step import LoadAllFactorsStep
from prometheus.pipelines.steps.load_historical_target_step import LoadHistoricalTargetStep
from prometheus.pipelines.steps.train_factor_simulator_step import TrainFactorSimulatorStep

import asyncio

async def main(target_factor: str):
    """
    åŸ·è¡Œå› å­æ¨¡æ“¬æ¨¡å‹è¨“ç·´ç”Ÿç”¢ç·šã€‚

    :param target_factor: è¦æ¨¡æ“¬çš„ç›®æ¨™å› å­åç¨±ã€‚
    """
    pipeline = Pipeline(
        steps=[
            LoadAllFactorsStep(),
            LoadHistoricalTargetStep(target_factor=target_factor),
            TrainFactorSimulatorStep(target_factor=target_factor),
        ]
    )
    await pipeline.run()

def run_main(target_factor: str):
    asyncio.run(main(target_factor))

if __name__ == "__main__":
    # é€™è£¡å¯ä»¥æ·»åŠ ä¸€å€‹ç°¡å–®çš„æ¸¬è©¦ï¼Œä¾‹å¦‚ï¼š
    # main(target_factor="T10Y2Y")
    pass
# -*- coding: utf-8 -*-
"""
æœ¬æ¨¡çµ„å®šç¾©äº†ç­–ç•¥å›æ¸¬æ‰€éœ€çš„æ ¸å¿ƒæ•¸æ“šå¥‘ç´„ã€‚
"""
from dataclasses import dataclass, field
from typing import List, Dict

@dataclass
class Strategy:
    """
    å®šç¾©ä¸€å€‹æŠ½è±¡çš„äº¤æ˜“ç­–ç•¥ã€‚

    Attributes:
        factors (List[str]): æ­¤ç­–ç•¥æ‰€ä½¿ç”¨çš„å› å­åç¨±åˆ—è¡¨ã€‚
        weights (Dict[str, float]): å„å€‹å› å­çš„æ¬Šé‡ã€‚
        target_asset (str): äº¤æ˜“çš„ç›®æ¨™è³‡ç”¢ä»£ç¢¼ï¼Œä¾‹å¦‚ 'SPY'ã€‚
    """
    factors: List[str]
    weights: Dict[str, float]
    target_asset: str = 'SPY'

@dataclass
class PerformanceReport:
    """
    å®šç¾©å›æ¸¬å¾Œçš„ç¸¾æ•ˆå ±å‘Šã€‚

    Attributes:
        sharpe_ratio (float): å¤æ™®æ¯”ç‡ã€‚
        annualized_return (float): å¹´åŒ–å ±é…¬ç‡ã€‚
        max_drawdown (float): æœ€å¤§å›æ’¤ã€‚
        total_trades (int): ç¸½äº¤æ˜“æ¬¡æ•¸ã€‚
    """
    sharpe_ratio: float = 0.0
    annualized_return: float = 0.0
    max_drawdown: float = 0.0
    total_trades: int = 0
