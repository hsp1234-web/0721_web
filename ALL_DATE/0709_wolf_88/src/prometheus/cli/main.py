import typer
from prometheus.entrypoints.ai_analyst_app import ai_analyst_job
from prometheus.entrypoints.query_gateway import run_dashboard_service
from prometheus.core.logging.log_manager import LogManager

app = typer.Typer()
# ç”±æ–¼ LogManager ä¸å†æ˜¯å–®ä¾‹ï¼Œæˆ‘å€‘ç‚º CLI çš„ä¸»é€²ç¨‹å‰µå»ºä¸€å€‹å¸¸è¦çš„ logger
log_manager = LogManager(log_file="prometheus_cli.log")
logger = log_manager.get_logger("Conductor")

@app.command(name="analyze")
def cli_analyze():
    """
    å•Ÿå‹• AI åˆ†æå¸«å ±å‘Šç”Ÿæˆå™¨ã€‚
    """
    logger.info("æ­£åœ¨å•Ÿå‹• AI åˆ†æå¸«...")
    ai_analyst_job()
    logger.info("AI åˆ†æå¸«å·¥ä½œå®Œæˆã€‚")



@app.command(name="dashboard")
def cli_dashboard(
    host: str = typer.Option("127.0.0.1", help="ç¶å®šä¸»æ©Ÿ"),
    port: int = typer.Option(8000, help="ç¶å®šåŸ è™Ÿ"),
):
    """å•Ÿå‹•ç¶²é å„€è¡¨æ¿ã€‚"""
    logger.info(f"æº–å‚™åœ¨ http://{host}:{port} å•Ÿå‹•å„€è¡¨æ¿...")
    run_dashboard_service(None, host, port)

data_app = typer.Typer()
app.add_typer(data_app, name="data")

@data_app.command("create-dummy")
def create_dummy():
    """
    å»ºç«‹ä¸€å€‹ç”¨æ–¼æ¸¬è©¦çš„è™›æ§‹ OHLCV CSV æª”æ¡ˆã€‚
    """
    from pathlib import Path
    import numpy as np
    import pandas as pd

    DATA_DIR = Path("data")
    DATA_DIR.mkdir(exist_ok=True)
    file_path = DATA_DIR / "ohlcv_data.csv"

    date_range = pd.to_datetime(
        pd.date_range(start="2022-01-01", periods=1000, freq="D")
    )

    open_prices = np.random.uniform(90, 110, size=1000)

    data = {
        "Date": date_range,
        "Open": open_prices,
        "High": open_prices + np.random.uniform(0, 5, size=1000),
        "Low": open_prices - np.random.uniform(0, 5, size=1000),
        "Close": open_prices + np.random.uniform(-2, 2, size=1000),
        "Volume": np.random.randint(100000, 500000, size=1000),
    }

    df = pd.DataFrame(data)

    df.to_csv(file_path, index=False)
    logger.info(f"å·²æˆåŠŸå»ºç«‹è™›æ§‹æ•¸æ“šæª”æ¡ˆæ–¼: {file_path}")


results_app = typer.Typer()
app.add_typer(results_app, name="results")

@results_app.command("clear")
def clear_results():
    """
    æ¸…é™¤æ‰€æœ‰ç”Ÿæˆçš„çµæœã€ä½‡åˆ—ã€æ—¥èªŒå’Œæª¢æŸ¥é»ã€‚
    """
    import shutil

    logger.info("é–‹å§‹æ¸…é™¤æ‰€æœ‰åŸ·è¡Œæ•¸æ“š...")

    RESULTS_DB_PATH = "output/results.sqlite"
    QUEUE_DIR = "data/queues"
    LOG_DIR = "data/logs"
    CHECKPOINT_DIR = "data/checkpoints"
    REPORTS_DIR = "data/reports"

    def remove_path(path_str, is_dir=False):
        if is_dir:
            if os.path.isdir(path_str):
                shutil.rmtree(path_str)
                logger.info(f"å·²åˆªé™¤ä¸¦æ¸…ç©ºç›®éŒ„: {path_str}")
        else:
            if os.path.exists(path_str):
                os.remove(path_str)
                logger.info(f"å·²åˆªé™¤æª”æ¡ˆ: {path_str}")

    try:
        remove_path(RESULTS_DB_PATH, is_dir=False)
        remove_path(QUEUE_DIR, is_dir=True)
        remove_path(LOG_DIR, is_dir=True)
        remove_path(CHECKPOINT_DIR, is_dir=True)
        remove_path(REPORTS_DIR, is_dir=True)

        # é‡å»ºç©ºç›®éŒ„
        os.makedirs(QUEUE_DIR, exist_ok=True)
        os.makedirs(LOG_DIR, exist_ok=True)
        os.makedirs(CHECKPOINT_DIR, exist_ok=True)
        os.makedirs(REPORTS_DIR, exist_ok=True)

        logger.info("æ¸…é™¤ç¨‹åºå®Œæˆã€‚")
    except Exception as e:
        logger.error(f"æ¸…é™¤éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)


@results_app.command("show")
def show_results():
    """
    å¾ SQLite è³‡æ–™åº«æŸ¥è©¢ä¸¦é¡¯ç¤ºå›æ¸¬çµæœã€‚
    """
    import sqlite3
    import pandas as pd

    logger.info("æ­£åœ¨å¾ SQLite è³‡æ–™åº«æŸ¥è©¢çµæœ...")
    DB_PATH = "output/results.sqlite"
    TABLE_NAME = "backtest_results"
    try:
        conn = sqlite3.connect(DB_PATH)
        df = pd.read_sql_query(f"SELECT * FROM {TABLE_NAME}", conn)
        conn.close()

        if df.empty:
            logger.warning("è³‡æ–™åº«ä¸­å°šç„¡ä»»ä½•çµæœã€‚")
        else:
            logger.info("æŸ¥è©¢å®Œæˆã€‚")
            # ä½¿ç”¨ä¸€å€‹ logger å‘¼å«ä¾†é¡¯ç¤ºæ•´å€‹ DataFrame
            logger.info(f"\n--- å›æ¸¬çµæœ ---\n{df.to_string()}\n----------------")

    except Exception as e:
        logger.error(f"æŸ¥è©¢çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)


@results_app.command("generate-report")
def generate_report(
    xml_path: str = typer.Option(
        "output/reports/report.xml", help="JUnit XML å ±å‘Šçš„è·¯å¾‘"
    ),
    md_path: str = typer.Option(
        "TEST_REPORT.md", help="è¦ç”Ÿæˆçš„ Markdown å ±å‘Šçš„è·¯å¾‘"
    ),
):
    """
    å¾ JUnit XML æª”æ¡ˆç”¢ç”Ÿ Markdown å ±å‘Šã€‚
    """
    import xml.etree.ElementTree as ET
    from datetime import datetime

    logger.info(f"AI å ±å‘Šç”Ÿæˆå™¨å•Ÿå‹•ï¼Œæ­£åœ¨è®€å–åŸå§‹æ•¸æ“š: {xml_path}")
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
        suite = root.find("testsuite")

        # æå–æ ¸å¿ƒæ•¸æ“š
        total = int(suite.get("tests", 0))
        failures = int(suite.get("failures", 0))
        errors = int(suite.get("errors", 0))
        skipped = int(suite.get("skipped", 0))
        exec_time = float(suite.get("time", 0))
        passed = total - failures - errors - skipped

        # é–‹å§‹æ§‹å»º Markdown å ±å‘Š
        report_content = []
        report_content.append("# **ã€æ™®ç¾…ç±³ä¿®æ–¯ä¹‹ç«ã€‘ç³»çµ±æ¸¬è©¦ä½œæˆ°å ±å‘Š**")
        report_content.append(
            f"> å ±å‘Šç”Ÿæˆæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        )

        # ç¸½çµå€å¡Š
        report_content.append("## **ä¸€ã€ æˆ°æ³ç¸½è¦½**")
        if failures == 0 and errors == 0:
            report_content.append(
                "> **çµè«–ï¼š<font color='green'>ä»»å‹™æˆåŠŸ (SUCCESS)</font>** - æ‰€æœ‰å“è³ªé–˜é–€å‡å·²é€šéã€‚ç³»çµ±æˆ°å‚™ç‹€æ…‹è‰¯å¥½ã€‚"
            )
        else:
            report_content.append(
                "> **çµè«–ï¼š<font color='red'>ä»»å‹™å¤±æ•— (FAILURE)</font>** - ç™¼ç¾é—œéµæ€§éŒ¯èª¤ã€‚ç³»çµ±å­˜åœ¨é¢¨éšªï¼Œéœ€ç«‹å³å¯©æŸ¥ã€‚"
            )

        summary_table = [
            "| æŒ‡æ¨™ (Metric) | æ•¸é‡ (Count) |",
            "|:---|:---:|",
            f"| âœ… **æ¸¬è©¦é€šé (Passed)** | {passed} |",
            f"| âŒ **æ¸¬è©¦å¤±æ•— (Failed)** | {failures} |",
            f"| ğŸ”¥ **åŸ·è¡ŒéŒ¯èª¤ (Errors)** | {errors} |",
            f"| ğŸš§ **æ¸¬è©¦è·³é (Skipped)** | {skipped} |",
            f"| â±ï¸ **ç¸½åŸ·è¡Œæ™‚é–“ (Time)** | {exec_time:.2f} ç§’ |",
            f"| ğŸ§® **ç¸½åŸ·è¡Œæ•¸é‡ (Total)** | {total} |",
        ]
        report_content.append("\n".join(summary_table))

        # å¤±æ•—èˆ‡éŒ¯èª¤è©³æƒ…
        if failures > 0 or errors > 0:
            report_content.append("\n## **äºŒã€ å¤±æ•—èˆ‡éŒ¯èª¤è©³æƒ…**")
            count = 1
            for testcase in suite.findall("testcase"):
                failure = testcase.find("failure")
                error = testcase.find("error")

                detail = failure if failure is not None else error

                if detail is not None:
                    test_name = testcase.get("name", "æœªçŸ¥æ¸¬è©¦")
                    class_name = testcase.get("classname", "æœªçŸ¥é¡åˆ¥")
                    error_type = detail.tag.capitalize()  # "failure" -> "Failure"
                    message = detail.get("message", "ç„¡è¨Šæ¯").splitlines()[0]

                    report_content.append(f"\n### {count}. {error_type}: {message}")
                    report_content.append(
                        f"- **æ¸¬è©¦ä½ç½®:** `{class_name}.{test_name}`"
                    )
                    report_content.append("- **è©³ç´°å †ç–Šè¿½è¹¤:**")
                    # æª¢æŸ¥ detail.text æ˜¯å¦ç‚º None
                    stack_trace = (
                        detail.text.strip() if detail.text else "ç„¡å †ç–Šè¿½è¹¤è³‡è¨Šã€‚"
                    )
                    report_content.append(f"```\n{stack_trace}\n```")
                    count += 1

        # å¯«å…¥æª”æ¡ˆ
        with open(md_path, "w", encoding="utf-8") as f:
            f.write("\n".join(report_content))

        logger.info(f"ä½œæˆ°å ±å‘Šå·²æˆåŠŸç”Ÿæˆè‡³: {md_path}")

    except FileNotFoundError:
        logger.error(f"æ‰¾ä¸åˆ°åŸå§‹æ•¸æ“šæª”æ¡ˆ: {xml_path}")
    except ET.ParseError:
        logger.error(f"åŸå§‹æ•¸æ“šæª”æ¡ˆæ ¼å¼éŒ¯èª¤: {xml_path}")
    except Exception as e:
        logger.error(f"ç”Ÿæˆå ±å‘Šæ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")


@results_app.command("add-tasks")
def add_tasks(
    num_tasks: int = typer.Option(10, help="è¦æ·»åŠ çš„ä»»å‹™æ•¸é‡"),
):
    """
    å‘ä»»å‹™ä½‡åˆ—ä¸­æ·»åŠ æŒ‡å®šæ•¸é‡çš„éš¨æ©Ÿå›æ¸¬ä»»å‹™ã€‚
    """
    import random
    import uuid
    from prometheus.core.context import AppContext

    with AppContext() as ctx:
        logger.info(f"æ­£åœ¨ç”Ÿæˆ {num_tasks} å€‹å›æ¸¬ä»»å‹™...")
        batch_id = str(uuid.uuid4())
        for i in range(num_tasks):
            # ä»»å‹™ç¾åœ¨æ˜¯ä¸€å€‹å­—å…¸
            task = {
                "task_id": str(uuid.uuid4()),
                "type": "backtest",
                "strategy": "SMA_Crossover",
                "symbol": random.choice(["BTC/USDT", "ETH/USDT", "XRP/USDT"]),
                "params": {
                    "fast": random.randint(5, 15), "slow": random.randint(20, 40)
                },
                "batch_id": batch_id,
            }
            ctx.queue.put(task)
            logger.debug(f"å·²å°‡ä»»å‹™ {i+1}/{num_tasks} ({task['strategy']}) æ·»åŠ åˆ°ä½‡åˆ—ã€‚")
        logger.info(f"æˆåŠŸå°‡ {num_tasks} å€‹ä»»å‹™æ·»åŠ åˆ°ä½‡åˆ—ã€‚")


pipelines_app = typer.Typer()
app.add_typer(pipelines_app, name="pipelines")

@pipelines_app.command("run-downloader")
def run_downloader(
    start_date: str = typer.Option(..., help="ä¸‹è¼‰é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)"),
    end_date: str = typer.Option(..., help="ä¸‹è¼‰çµæŸæ—¥æœŸ (YYYY-MM-DD)"),
    output_dir: str = typer.Option("data/downloads", help="æª”æ¡ˆå„²å­˜ç›®éŒ„"),
    max_workers: int = typer.Option(16, help="æœ€å¤§åŒæ™‚ä¸‹è¼‰ä»»å‹™æ•¸"),
):
    """
    TAIFEX è‡ªå‹•åŒ–æ•¸æ“šæ¡é›†å™¨ v1.0
    """
    import os
    import random
    import time
    from collections import Counter
    from concurrent.futures import ThreadPoolExecutor, as_completed
    from datetime import datetime, timedelta
    import requests
    from prometheus.core.config import config

    logger.info("--- å•Ÿå‹•æ•¸æ“šæ¡é›†ä»»å‹™ ---")
    logger.info(f"æ™‚é–“ç¯„åœ: {start_date} åˆ° {end_date}")
    logger.info(f"è¼¸å‡ºç›®éŒ„: {output_dir}")

    tasks = []
    start_dt = datetime.strptime(start_date, "%Y-%m-%d")
    end_dt = datetime.strptime(end_date, "%Y-%m-%d")
    date_range = [
        start_dt + timedelta(days=x) for x in range((end_dt - start_dt).days + 1)
    ]

    base_url = config.get("data_acquisition.taifex.base_url")
    for current_date in date_range:
        date_str = current_date.strftime("%Y_%m_%d")
        # ç¯„ä¾‹ï¼šåƒ…ä¸‹è¼‰æœŸè²¨é€ç­†è³‡æ–™
        tasks.append(
            {
                "url": f"{base_url}/file/taifex/Dailydownload/DailydownloadCSV/Daily_{date_str}.zip",
                "file_name": f"Daily_{date_str}.zip",
                "min_delay": 0.2,
                "max_delay": 1.0,
            }
        )

    results_counter = Counter()
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        with requests.Session() as session:
            future_to_task = {
                executor.submit(execute_download, session, task, output_dir): task
                for task in tasks
            }
            for future in as_completed(future_to_task):
                try:
                    status, message = future.result()
                    results_counter[status] += 1
                    logger.info(f"[{status.upper()}] {message}")
                except Exception as exc:
                    logger.info(f"[CRITICAL] ä»»å‹™åŸ·è¡Œç•°å¸¸: {exc}")

    logger.info("\n--- æ¡é›†ä»»å‹™ç¸½çµ ---")
    for status, count in results_counter.items():
        logger.info(f"  {status}: {count} å€‹")


def execute_download(session, task_info, output_dir):
    """åŸ·è¡Œå–®ä¸€æª”æ¡ˆä¸‹è¼‰ä»»å‹™ï¼ŒåŒ…å«é‡è©¦èˆ‡éŒ¯èª¤è™•ç†ã€‚"""
    import os
    import random
    import time
    import requests
    from prometheus.core.config import config

    file_path = os.path.join(output_dir, task_info["file_name"])
    if os.path.exists(file_path):
        return "exists", f"æª”æ¡ˆå·²å­˜åœ¨: {task_info['file_name']}"

    time.sleep(random.uniform(task_info["min_delay"], task_info["max_delay"]))

    user_agents = config.get("data_acquisition.taifex.user_agents")
    base_url = config.get("data_acquisition.taifex.base_url")

    for attempt in range(3):  # é‡è©¦3æ¬¡
        try:
            headers = {
                "User-Agent": random.choice(user_agents),
                "Referer": task_info.get("referer", base_url),
            }
            response = (
                session.post(
                    task_info["url"],
                    data=task_info.get("payload", {}),
                    headers=headers,
                    timeout=120,
                )
                if task_info.get("payload")
                else session.get(task_info["url"], headers=headers, timeout=120)
            )

            if (
                response.status_code == 200
                and len(response.content) > 100
                and "æŸ¥ç„¡è³‡æ–™" not in response.text
            ):
                os.makedirs(output_dir, exist_ok=True)
                with open(file_path, "wb") as f:
                    f.write(response.content)
                return "success", f"æˆåŠŸä¸‹è¼‰: {task_info['file_name']}"
            elif response.status_code == 404:
                return "not_found", f"404 Not Found: {task_info['file_name']}"
            else:
                return (
                    "error",
                    f"ä¼ºæœå™¨éŒ¯èª¤ {response.status_code}: {task_info['file_name']}",
                )

        except requests.exceptions.RequestException as e:
            if attempt == 2:
                return "error", f"ç¶²è·¯è«‹æ±‚å¤±æ•—: {e}"
            time.sleep(5 * (attempt + 1))

    return "error", f"é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸: {task_info['file_name']}"


@pipelines_app.command("run-explorer")
def run_explorer(
    input_dir: str = typer.Option("data/downloads", help="æƒæçš„åŸå§‹æª”æ¡ˆç›®éŒ„"),
    db_path: str = typer.Option("data/metadata/schema_registry.db", help="æ ¼å¼è¨»å†Šè¡¨è³‡æ–™åº«è·¯å¾‘"),
):
    """
    TAIFEX æ ¼å¼æ¢å‹˜èˆ‡è¨»å†Šå™¨ v1.0
    """
    import hashlib
    import os
    from prometheus.core.db.schema_registry import SchemaRegistry
    from prometheus.core.utils.helpers import (
        prospect_file_content,
        read_file_content,
    )

    registry = SchemaRegistry(db_path)
    logger.info(f"é–‹å§‹æƒæç›®éŒ„: {input_dir}")
    new_formats = 0
    updated_formats = 0

    for filename in os.listdir(input_dir):
        file_path = os.path.join(input_dir, filename)
        if not os.path.isfile(file_path):
            continue

        try:
            file_bytes_content = read_file_content(file_path)
            if file_bytes_content is None:
                continue

            result = prospect_file_content(file_bytes_content)

            if result["status"] == "success":
                fingerprint = get_header_fingerprint(result["header"])
                status = registry.add_or_update_schema(fingerprint, result["header"], result["encoding"], filename)
                if status == "new":
                    new_formats += 1
                else:
                    updated_formats += 1

        except Exception as e:
            logger.error(f"è™•ç†æª”æ¡ˆ {filename} å¤±æ•—: {e}", exc_info=True)

    registry.close()
    logger.info("--- æ ¼å¼æ¢å‹˜ç¸½çµ ---")
    logger.info(f"ç™¼ç¾æ–°æ ¼å¼: {new_formats} ç¨®")
    logger.info(f"æ›´æ–°ç¾æœ‰æ ¼å¼è¨ˆæ•¸: {updated_formats} æ¬¡")


def get_header_fingerprint(header_line: str) -> str:
    """å°æ¨™æº–åŒ–å¾Œçš„æ¨™é ­è¨ˆç®—æŒ‡ç´‹ã€‚"""
    import hashlib
    normalized_header = "".join(header_line.lower().split()).replace('"', "")
    return hashlib.sha256(normalized_header.encode("utf-8")).hexdigest()


@pipelines_app.command("run-elt")
def run_elt(
    input_dir: str = typer.Option("data/downloads", help="ä¸‹è¼‰æª”æ¡ˆçš„ä¾†æºç›®éŒ„ (ä¾› Loader ä½¿ç”¨)"),
    raw_db_path: str = typer.Option("data/raw_warehouse/raw_taifex.duckdb", help="åŸå§‹æ•¸æ“šè‰™è³‡æ–™åº«è·¯å¾‘"),
    schema_db_path: str = typer.Option("data/metadata/schema_registry.db", help="æ ¼å¼è¨»å†Šè¡¨è³‡æ–™åº«è·¯å¾‘"),
    analytics_db_path: str = typer.Option("data/analytics_warehouse/analytics_taifex.duckdb", help="åˆ†ææ•¸æ“šåº«è·¯å¾‘"),
):
    """
    TAIFEX ELT åŠ å·¥ç®¡ç·š v1.0
    """
    import os
    from prometheus.core.db.data_warehouse import AnalyticsDataWarehouse, RawDataWarehouse
    from prometheus.core.db.schema_registry import SchemaRegistry

    # Ensure parent directories for database files exist
    os.makedirs(os.path.dirname(raw_db_path), exist_ok=True)
    os.makedirs(os.path.dirname(schema_db_path), exist_ok=True)
    os.makedirs(os.path.dirname(analytics_db_path), exist_ok=True)

    run_loader(input_dir, raw_db_path, schema_db_path)
    run_transformer(raw_db_path, schema_db_path, analytics_db_path)


def run_loader(input_dir, raw_db_path, schema_db_path):
    import os
    from prometheus.core.db.data_warehouse import RawDataWarehouse
    from prometheus.core.db.schema_registry import SchemaRegistry
    from prometheus.core.utils.helpers import (
        prospect_file_content,
        read_file_content,
    )

    logger.info("--- [éšæ®µ 2] åŸ·è¡Œ Loader ---")
    raw_wh = RawDataWarehouse(raw_db_path)
    schema_registry = SchemaRegistry(schema_db_path)

    known_fingerprints = schema_registry.get_known_fingerprints()
    if not known_fingerprints:
        logger.info("Loader: No known fingerprints loaded from schema registry. Only files matching these will be processed.")

    files_loaded = 0
    if not os.path.exists(input_dir):
        logger.warning(f"Loader input directory {input_dir} does not exist. Skipping loading.")
        raw_wh.close()
        schema_registry.close()
        return

    for filename in os.listdir(input_dir):
        file_path = os.path.join(input_dir, filename)

        if not os.path.isfile(file_path):
            continue

        if raw_wh.is_file_processed(file_path):
            logger.debug(f"Loader: File {filename} already in raw_import_log. Skipping.")
            continue

        try:
            file_bytes_content = read_file_content(file_path)
            if file_bytes_content is None:
                continue

            result = prospect_file_content(file_bytes_content)
            if result["status"] == "success":
                fingerprint = get_header_fingerprint(result["header"])
                if fingerprint in known_fingerprints:
                    raw_wh.log_processed_file(file_path, file_bytes_content, fingerprint)
                    files_loaded += 1
                    logger.info(f"Loader: Loaded {filename} (fingerprint: {fingerprint[:8]}...) as it's a known schema.")
                else:
                    logger.info(f"Loader: Skipped {filename} (fingerprint: {fingerprint[:8]}...) as its schema is not in the registry.")
            else:
                logger.warning(f"Loader: Skipped file {filename} due to content prospecting failure: {result.get('error', 'Unknown error')}")

        except Exception as e:
            logger.error(f"Loader è™•ç† {filename} å¤±æ•—: {e}", exc_info=True)

    raw_wh.close()
    schema_registry.close()
    logger.info(f"Loader å®Œæˆï¼Œæ–°è¼‰å…¥ {files_loaded} å€‹æª”æ¡ˆã€‚")


def run_transformer(raw_db_path, schema_db_path, analytics_db_path):
    import io
    import pandas as pd
    from prometheus.core.db.data_warehouse import AnalyticsDataWarehouse, RawDataWarehouse
    from prometheus.core.db.schema_registry import SchemaRegistry

    logger.info("--- [éšæ®µ 3] åŸ·è¡Œ Transformer ---")
    schema_registry = SchemaRegistry(schema_db_path)
    raw_wh = RawDataWarehouse(raw_db_path)
    analytics_wh = AnalyticsDataWarehouse(analytics_db_path)

    schema_map = schema_registry.get_all_schemas()
    if not schema_map:
        logger.warning("Transformer: æ ¼å¼è¨»å†Šè¡¨ç‚ºç©ºæˆ–è®€å–å¤±æ•—ï¼ŒTransformer ç„¡æ³•åŸ·è¡Œæœ‰æ•ˆè½‰æ›ã€‚")
        schema_registry.close()
        raw_wh.close()
        analytics_wh.close()
        return

    daily_futures_header_str = "äº¤æ˜“æ—¥æœŸ,å¥‘ç´„ä»£ç¢¼,åˆ°æœŸæœˆä»½(é€±åˆ¥),é–‹ç›¤åƒ¹,æœ€é«˜åƒ¹,æœ€ä½åƒ¹,æ”¶ç›¤åƒ¹,æˆäº¤é‡"
    target_daily_futures_fingerprint = get_header_fingerprint(daily_futures_header_str)

    if target_daily_futures_fingerprint not in schema_map:
        logger.warning(f"Transformer: Did not find fingerprint for daily_futures_header '{daily_futures_header_str}' in schema_map. Cannot process daily_futures.")
    else:
        logger.info(f"Transformer: Target fingerprint for daily_futures is {target_daily_futures_fingerprint[:8]}...")

    analytics_wh.create_daily_futures_table()

    records = raw_wh.execute_query("SELECT content_blob, format_fingerprint FROM raw_import_log").fetchall()
    transformed_count = 0
    for blob, fingerprint in records:
        if fingerprint != target_daily_futures_fingerprint:
            continue

        if fingerprint not in schema_map:
            continue

        header_str_from_registry, encoding = schema_map[fingerprint]
        try:
            df = pd.read_csv(io.BytesIO(blob), encoding=encoding, thousands=",", header=0, on_bad_lines="skip")
            df.columns = [str(col).strip().replace('"', "") for col in df.columns]

            target_columns_canonical = [
                "äº¤æ˜“æ—¥æœŸ", "å¥‘ç´„ä»£ç¢¼", "åˆ°æœŸæœˆä»½(é€±åˆ¥)", "é–‹ç›¤åƒ¹",
                "æœ€é«˜åƒ¹", "æœ€ä½åƒ¹", "æ”¶ç›¤åƒ¹", "æˆäº¤é‡",
            ]

            df_to_load = pd.DataFrame()
            for canonical_col_name in target_columns_canonical:
                if canonical_col_name in df.columns:
                    df_to_load[canonical_col_name] = df[canonical_col_name]
                else:
                    df_to_load[canonical_col_name] = None

            if not df_to_load.empty:
                analytics_wh.insert_daily_futures(df_to_load)
                transformed_count += 1

        except pd.errors.EmptyDataError:
            logger.warning(f"Transformer: No data or columns found in CSV for fingerprint {fingerprint[:8]}...")
        except Exception as e:
            logger.error(f"Transformer è™•ç†æŒ‡ç´‹ {fingerprint[:8]}... çš„è³‡æ–™æ™‚å¤±æ•—: {e}", exc_info=True)

    raw_wh.close()
    analytics_wh.close()
    schema_registry.close()
    logger.info(f"Transformer å®Œæˆï¼ŒæˆåŠŸè½‰æ› {transformed_count} ç­†è¨˜éŒ„ã€‚")


@pipelines_app.command("run-stock-factors")
def run_stock_factors():
    """
    åŸ·è¡Œç¬¬å››è™Ÿç”Ÿç”¢ç·šï¼šè‚¡ç¥¨å› å­ç”Ÿæˆã€‚
    """
    from prometheus.pipelines.p4_stock_factor_generation import main as p4_main
    logger.info("--- å•Ÿå‹• P4ï¼šè‚¡ç¥¨å› å­ç”Ÿæˆç®¡ç·š ---")
    p4_main()
    logger.info("--- P4ï¼šè‚¡ç¥¨å› å­ç”Ÿæˆç®¡ç·šåŸ·è¡Œå®Œç•¢ ---")


@pipelines_app.command("run-crypto-factors")
def run_crypto_factors():
    """
    åŸ·è¡Œç¬¬äº”è™Ÿç”Ÿç”¢ç·šï¼šåŠ å¯†è²¨å¹£å› å­ç”Ÿæˆã€‚
    """
    from prometheus.pipelines.p5_crypto_factor_generation import main as p5_main
    logger.info("--- å•Ÿå‹• P5ï¼šåŠ å¯†è²¨å¹£å› å­ç”Ÿæˆç®¡ç·š ---")
    p5_main()
    logger.info("--- P5ï¼šåŠ å¯†è²¨å¹£å› å­ç”Ÿæˆç®¡ç·šåŸ·è¡Œå®Œç•¢ ---")


@app.command(name="build-feature-store")
def build_feature_store():
    """
    ã€ä½œæˆ°æŒ‡ä»¤ã€‘çµ±ä¸€æ•¸æ“šå€‰å„²é‡æ§‹ï¼šå»ºé€ ç‰¹å¾µå€‰å„²ã€‚
    """
    import asyncio
    from prometheus.core.db.db_manager import DBManager
    # from prometheus.pipelines.p1_factor_generation import p1_factor_generation_pipeline
    # from prometheus.pipelines.p2_index_factor_generation import p2_index_factor_pipeline
    # from prometheus.pipelines.p3_bond_factor_generation import p3_bond_factor_pipeline
    from prometheus.pipelines.p4_stock_factor_generation import main as p4_main
    from prometheus.pipelines.p5_crypto_factor_generation import main as p5_main

    logger.info("--- å•Ÿå‹•çµ±ä¸€æ•¸æ“šå€‰å„²å»ºæ§‹æµç¨‹ ---")
    db_manager = DBManager()

    # --- P1, P2, P3 (å·²åœç”¨) ---
    # æ ¹æ“šç›®å‰çš„æª”æ¡ˆçµæ§‹ï¼Œé€™äº›ç®¡ç·šä¸å­˜åœ¨ï¼Œæš«æ™‚è¨»è§£ä»¥ç¢ºä¿å‘½ä»¤å¯åŸ·è¡Œ
    # logger.info("åŸ·è¡Œ P1 é€šç”¨å› å­ç”Ÿæˆ...")
    # p1_df = asyncio.run(p1_factor_generation_pipeline.run())
    # db_manager.save_data(p1_df, 'factors')
    # logger.info("P1 é€šç”¨å› å­æ•¸æ“šå·²åˆä½µã€‚")

    # logger.info("åŸ·è¡Œ P2 æŒ‡æ•¸å› å­ç”Ÿæˆ...")
    # p2_df = asyncio.run(p2_index_factor_pipeline.run())
    # db_manager.save_data(p2_df, 'factors')
    # logger.info("P2 æŒ‡æ•¸å› å­æ•¸æ“šå·²åˆä½µã€‚")

    # logger.info("åŸ·è¡Œ P3 å‚µåˆ¸å› å­ç”Ÿæˆ...")
    # p3_df = asyncio.run(p3_bond_factor_pipeline.run())
    # db_manager.save_data(p3_df, 'factors')
    # logger.info("P3 å‚µåˆ¸å› å­æ•¸æ“šå·²åˆä½µã€‚")

    # --- P4, P5 ---
    # é€™äº›ç®¡ç·šçš„ main å‡½æ•¸å…§éƒ¨ç›´æ¥èª¿ç”¨ DBManagerï¼Œæˆ‘å€‘æš«æ™‚ä¿æŒé€™ç¨®æ–¹å¼
    # æœªä¾†å¯ä»¥é€²ä¸€æ­¥é‡æ§‹ï¼Œä½†ç›®å‰è¶³ä»¥æ»¿è¶³ä½œæˆ°ç›®æ¨™
    logger.info("åŸ·è¡Œ P4 è‚¡ç¥¨å› å­ç”Ÿæˆ...")
    p4_main()
    logger.info("P4 è‚¡ç¥¨å› å­æ•¸æ“šå·²åˆä½µã€‚")

    logger.info("åŸ·è¡Œ P5 åŠ å¯†è²¨å¹£å› å­ç”Ÿæˆ...")
    p5_main()
    logger.info("P5 åŠ å¯†è²¨å¹£å› å­æ•¸æ“šå·²åˆä½µã€‚")

    logger.info("--- çµ±ä¸€æ•¸æ“šå€‰å„²å»ºæ§‹æµç¨‹å®Œç•¢ ---")


@pipelines_app.command("run-simulation-training")
def run_simulation_training(
    target_factor: str = typer.Option(..., help="è¦æ¨¡æ“¬çš„ç›®æ¨™å› å­åç¨±"),
):
    """
    åŸ·è¡Œç¬¬å…­è™Ÿç”Ÿç”¢ç·šï¼šå› å­ä»£ç†æ¨¡æ“¬æ¨¡å‹è¨“ç·´ã€‚
    """
    from prometheus.pipelines.p6_simulation_training import run_main as p6_run_main
    logger.info(f"--- å•Ÿå‹• P6ï¼šå› å­ä»£ç†æ¨¡æ“¬æ¨¡å‹è¨“ç·´ç®¡ç·šï¼Œç›®æ¨™ç‚º {target_factor} ---")
    p6_run_main(target_factor=target_factor)
    logger.info(f"--- P6ï¼šå› å­ä»£ç†æ¨¡æ“¬æ¨¡å‹è¨“ç·´ç®¡ç·šåŸ·è¡Œå®Œç•¢ ---")


@pipelines_app.command("run")
def run_pipeline(
    name: str = typer.Option(..., help="è¦åŸ·è¡Œçš„ç®¡ç·šåç¨±"),
    ticker: str = typer.Option(None, "--ticker", "-t", help="è¦è™•ç†çš„è³‡ç”¢ä»£è™Ÿ")
):
    """
    åŸ·è¡ŒæŒ‡å®šçš„æ•¸æ“šç®¡ç·šã€‚
    """
    import asyncio
    pipeline_context = {"ticker": ticker} if ticker else {}
    logger.info(f"--- å•Ÿå‹• {name} ç®¡ç·šï¼Œä¸Šä¸‹æ–‡: {pipeline_context} ---")

    if name == "p1_factor_generation":
        from prometheus.pipelines.p1_factor_generation import p1_factor_generation_pipeline
        asyncio.run(p1_factor_generation_pipeline.run(context=pipeline_context))
    elif name == "p2_index_factor_generation":
        from prometheus.pipelines.p2_index_factor_generation import p2_index_factor_pipeline
        asyncio.run(p2_index_factor_pipeline.run(context=pipeline_context))
    elif name == "p3_bond_factor_generation":
        from prometheus.pipelines.p3_bond_factor_generation import p3_bond_factor_pipeline
        asyncio.run(p3_bond_factor_pipeline.run(context=pipeline_context))
    else:
        logger.error(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°åç‚º '{name}' çš„ç®¡ç·šã€‚")
        raise typer.Exit(code=1)

    logger.info(f"--- {name} ç®¡ç·šåŸ·è¡Œå®Œç•¢ ---")


@pipelines_app.command("run-backfill")
def run_backfill_cli(
    start_date: str = typer.Option(..., help="å›å¡«é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)"),
    end_date: str = typer.Option(..., help="å›å¡«çµæŸæ—¥æœŸ (YYYY-MM-DD)"),
):
    """
    åŸ·è¡Œæ­·å²æ•¸æ“šå›å¡«ç®¡ç·šã€‚
    """
    import pandas as pd
    from prometheus.core.analysis.data_engine import DataEngine
    from prometheus.core.clients.client_factory import ClientFactory

    logger.info(f"--- é–‹å§‹åŸ·è¡Œæ•¸æ“šå›å¡«ä½œæ¥­ï¼šå¾ {start_date} åˆ° {end_date} ---")

    data_engine = DataEngine()
    hourly_timestamps = pd.date_range(start=start_date, end=end_date, freq="H")
    total_tasks = len(hourly_timestamps)

    for i, ts in enumerate(hourly_timestamps):
        logger.debug(f"--- æ­£åœ¨è™•ç† ({i + 1}/{total_tasks}): {ts} ---")
        try:
            data_engine.generate_snapshot(ts)
        except Exception as e:
            logger.error(f"âŒ è™•ç† {ts} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)

    data_engine.close()
    ClientFactory.close_all()
    logger.info("--- æ•¸æ“šå›å¡«ä½œæ¥­å®Œæˆ ---")


from prometheus.models.strategy_models import Strategy
from prometheus.services.backtesting_service import BacktestingService
from prometheus.services.evolution_chamber import EvolutionChamber
from prometheus.services.strategy_reporter import StrategyReporter
from prometheus.core.db.db_manager import DBManager

@app.command()
def run_evolution_cycle():
    """
    ğŸš€ [ç«¯åˆ°ç«¯] åŸ·è¡Œä¸€æ¬¡å®Œæ•´çš„æ¼”åŒ–é€±æœŸï¼šæ¼”åŒ– -> å›æ¸¬ -> å ±å‘Šã€‚
    """
    print("--- å•Ÿå‹•ã€æ¼”åŒ–å®¤è¡Œå‹•ã€‘å®Œæ•´ä½œæˆ°é€±æœŸ ---")

    # 1. åˆå§‹åŒ–æ ¸å¿ƒæœå‹™
    db_manager = DBManager()
    backtester = BacktestingService(db_manager)

    # 2. æº–å‚™æ¼”åŒ–æ‰€éœ€æ•¸æ“š
    # å‡è¨­å› å­æ•¸æ“šå·²å­˜åœ¨
    all_factors_df = db_manager.fetch_table('factors')
    # æ’é™¤éå› å­æ¬„ä½
    available_factors = [col for col in all_factors_df.columns if col not in ['date', 'symbol', 'close']]

    if not available_factors:
        print("éŒ¯èª¤ï¼šæ•¸æ“šåº«ä¸­æ‰¾ä¸åˆ°å¯ç”¨çš„å› å­ã€‚è«‹å…ˆåŸ·è¡Œ build-feature-storeã€‚")
        return

    target_asset_for_evolution = 'AAPL' # é¸æ“‡ä¸€å€‹æ•¸æ“šåº«ä¸­å­˜åœ¨çš„è³‡ç”¢
    print(f"INFO: å°‡ä½¿ç”¨ '{target_asset_for_evolution}' ä½œç‚ºæœ¬æ¬¡æ¼”åŒ–çš„ç›®æ¨™è³‡ç”¢ã€‚")
    chamber = EvolutionChamber(backtester, available_factors, target_asset=target_asset_for_evolution)

    # 3. åŸ·è¡Œæ¼”åŒ–
    # ç‚ºäº†å¿«é€Ÿæ¼”ç¤ºï¼Œä½¿ç”¨è¼ƒå°çš„åƒæ•¸
    hof = chamber.run_evolution(n_pop=20, n_gen=5)

    if not hof:
        print("éŒ¯èª¤ï¼šæ¼”åŒ–æœªèƒ½ç”¢ç”Ÿæœ‰æ•ˆçµæœã€‚")
        return

    # 4. å°æœ€å„ªç­–ç•¥é€²è¡Œæœ€çµ‚å›æ¸¬ä»¥ç²å¾—å®Œæ•´å ±å‘Š
    best_individual = hof[0]
    best_factors = [available_factors[i] for i in best_individual]
    final_strategy = Strategy(
        factors=best_factors,
        weights={factor: 1.0 / len(best_factors) for factor in best_factors},
        target_asset='AAPL' # ä¿®æ­£ï¼šæ˜ç¢ºæŒ‡å®šä¸€å€‹å­˜åœ¨çš„è³‡ç”¢
    )
    final_report = backtester.run(final_strategy)

    # 5. ç”Ÿæˆå ±å‘Š
    reporter = StrategyReporter()
    reporter.generate_report(hof, final_report, available_factors)

    print("--- ã€æ¼”åŒ–å®¤è¡Œå‹•ã€‘ä½œæˆ°é€±æœŸçµæŸ ---")

if __name__ == "__main__":
    app()
